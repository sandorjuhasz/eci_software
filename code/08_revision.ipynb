{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from itertools import product\n",
    "from ecomplexity import ecomplexity\n",
    "from ecomplexity import proximity\n",
    "from ecomplexity import calc_density\n",
    "import country_converter as coco\n",
    "import itertools\n",
    "\n",
    "# spatial autocorrelation\n",
    "import geopandas as gpd\n",
    "from pysal.lib import weights\n",
    "from libpysal.io import open as psopen\n",
    "from splot.esda import (\n",
    "    moran_scatterplot, lisa_cluster, plot_local_autocorrelation, plot_moran\n",
    ")\n",
    "from splot.libpysal import plot_spatial_weights\n",
    "import esda\n",
    "\n",
    "# stats\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import linregress\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from data_prep_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**different RCA thresholds -- WIP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved in data prep\n",
    "eci_software = pd.read_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_5603/1169018816.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eci_df[\"mcp075\"] = np.where(eci_df[\"rca\"]>=0.75, 1, 0)\n",
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_5603/1169018816.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eci_df[\"mcp125\"] = np.where(eci_df[\"rca\"]>=1.25, 1, 0)\n"
     ]
    }
   ],
   "source": [
    "# year focus\n",
    "eci_df = eci_software[eci_software[\"year\"]==2020]\n",
    "eci_df[\"mcp075\"] = np.where(eci_df[\"rca\"]>=0.75, 1, 0)\n",
    "eci_df[\"mcp125\"] = np.where(eci_df[\"rca\"]>=1.25, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcp_crosstable(df, columns):\n",
    "    mcp_crosstable = df[columns].value_counts().reset_index().sort_values(by=columns).rename(columns={\"count\":\"obs\"})\n",
    "    mcp_crosstable[\"obs_share\"] = round(mcp_crosstable[\"obs\"] / mcp_crosstable[\"obs\"].sum(), 2)\n",
    "    return mcp_crosstable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mcp</th>\n",
       "      <th>mcp075</th>\n",
       "      <th>obs</th>\n",
       "      <th>obs_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15766</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1074</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3593</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mcp  mcp075    obs  obs_share\n",
       "0    0       0  15766       0.77\n",
       "2    0       1   1074       0.05\n",
       "1    1       1   3593       0.18"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcp_crosstable(eci_df, columns=[\"mcp\", \"mcp075\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mcp</th>\n",
       "      <th>mcp125</th>\n",
       "      <th>obs</th>\n",
       "      <th>obs_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16840</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1191</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2402</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mcp  mcp125    obs  obs_share\n",
       "0    0       0  16840       0.82\n",
       "2    1       0   1191       0.06\n",
       "1    1       1   2402       0.12"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcp_crosstable(eci_df, columns=[\"mcp\", \"mcp125\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n"
     ]
    }
   ],
   "source": [
    "# data IN -- for threshold 0.75 AND 1.25\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "\n",
    "# parameter to choose year / semester / quarter to construct period IDs\n",
    "selected_period = \"year\"\n",
    "\n",
    "# for ecomplexity calculcation\n",
    "key_cols = {\n",
    "    \"time\": \"period\",\n",
    "    \"loc\": \"iso2_code\",\n",
    "    \"prod\": \"language\",\n",
    "    \"val\": \"num_pushers\",\n",
    "}\n",
    "\n",
    "# use data_prep_functions to clean the dataframe of ECI_software calculation\n",
    "data = data[data[\"year\"].isin([2020, 2021, 2022, 2023])]\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Percentage of pairs compared that meet log-supermodularity condition: 4.90%\n",
      "1\n",
      "2020  DONE\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 1: Log-supermodularity condition is not fully satisfied (4.90% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 4.28%\n",
      "1\n",
      "2021  DONE\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 1: Log-supermodularity condition is not fully satisfied (4.28% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 4.40%\n",
      "1\n",
      "2022  DONE\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 1: Log-supermodularity condition is not fully satisfied (4.40% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 5.11%\n",
      "1\n",
      "2023  DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 1: Log-supermodularity condition is not fully satisfied (5.11% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# software complexity calculation -- period IDs -- 1 means 2020 on yearly basis\n",
    "ccdf = []\n",
    "ppdf = []\n",
    "year_dict = {1 : 2020, 2 : 2021, 3 : 2022, 4 : 2023}\n",
    "for k in year_dict.keys():\n",
    "    dfb = bundle_data(df, periods=[k])\n",
    "    #cdf = ecomplexity(dfb, key_cols, rca_mcp_threshold=0.75)\n",
    "    cdf = ecomplexity(dfb, key_cols, rca_mcp_threshold=1.25)\n",
    "    cdf[\"year\"] = year_dict[k]\n",
    "\n",
    "    #pdf = proximity(dfb, key_cols, rca_mcp_threshold=0.75)\n",
    "    pdf = proximity(dfb, key_cols, rca_mcp_threshold=1.25)\n",
    "    pdf[\"year\"] = year_dict[k]\n",
    "\n",
    "    # combine yearly dataframes\n",
    "    ccdf.append(cdf)\n",
    "    ppdf.append(pdf)\n",
    "    print(year_dict[k], \" DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine and save -- complexity\n",
    "cdf = pd.concat(ccdf, axis=0, ignore_index=True)\n",
    "#cdf.to_csv(\"../outputs/eci_software_2020_2023_threshold_075.csv\", sep=\";\", index=False)\n",
    "cdf.to_csv(\"../outputs/eci_software_2020_2023_threshold_125.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine and save -- language proximity\n",
    "prox_df = pd.concat(ppdf, axis=0, ignore_index=True)\n",
    "#prox_df.to_csv(\"../outputs/proximity_2020_2023_threshold_075.csv\", sep=\";\", index=False)\n",
    "prox_df.to_csv(\"../outputs/proximity_2020_2023_threshold_125.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "cdf100 = pd.read_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\")\n",
    "cdf100 = cdf100[cdf100[\"year\"]==2020]\n",
    "cdf075 = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_075.csv\", sep=\";\")\n",
    "cdf075 = cdf075[cdf075[\"year\"]==2020]\n",
    "cdf125 = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_125.csv\", sep=\";\")\n",
    "cdf125 = cdf125[cdf125[\"year\"]==2020].rename(columns={\"eci\":\"eci125\"})\n",
    "\n",
    "full_cdf = pd.merge(\n",
    "    cdf100[[\"iso2_code\", \"eci\"]].drop_duplicates(),\n",
    "    cdf075[[\"iso2_code\", \"eci\"]].drop_duplicates(),\n",
    "    on=[\"iso2_code\"],\n",
    "    how=\"left\",\n",
    "    suffixes=[\"100\", \"075\"]\n",
    ")\n",
    "full_cdf = pd.merge(\n",
    "    full_cdf,\n",
    "    cdf125,\n",
    "    on=[\"iso2_code\"],\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "### ENTRY -- 0.75 threshold\n",
    "\n",
    "# relatedness density -- as in Hidalgo et al. (2007) Science\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_075.csv\", sep=\";\")\n",
    "rel_dens = cdf[cdf[\"year\"] == 2020][[\"iso2_code\", \"language\", \"density\"]].drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "selected_period = \"year\"\n",
    "\n",
    "# steps to prep dataframe of ecomplexity\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "\n",
    "# threshold for RCA : 0.75\n",
    "ps = [1, 2, 3, 4]\n",
    "rca_tables = []\n",
    "for p in ps:\n",
    "    print(p)\n",
    "    temp = bundle_data(df, periods=[p])\n",
    "    temp[\"period\"] = p\n",
    "    rca_tables.append(rca_calculation(temp, c_column=\"iso2_code\", p_column=\"language\", value_column=\"num_pushers\", threshold=0.75))\n",
    "\n",
    "#    dfbs.append(temp)\n",
    "rca_tables = pd.concat(rca_tables)\n",
    "\n",
    "\n",
    "\n",
    "# identify entry following the given patterns\n",
    "entry_pattern = [0,0,1,1]\n",
    "consider_pattern = [0,0,0,0]\n",
    "ent = rca_tables.sort_values([\"period\"], ascending=True).groupby([\"iso2_code\",\"language\"])[\"rca01\"].agg(list).reset_index()\n",
    "ent[\"entry01\"] = ent[\"rca01\"].apply(lambda x: x == entry_pattern).astype(int)\n",
    "ent[\"consider00\"] = ent[\"rca01\"].apply(lambda x: x == consider_pattern).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# full combination\n",
    "all_countries = ent[\"iso2_code\"].unique()\n",
    "all_languages = ent[\"language\"].unique()\n",
    "\n",
    "all_combinations = list(product(all_countries, all_languages))\n",
    "full_df = pd.DataFrame(all_combinations, columns=[\"iso2_code\", \"language\"])\\\n",
    "    .sort_values([\"iso2_code\", \"language\"])\n",
    "\n",
    "# join entries\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    ent[[\"iso2_code\", \"language\", \"entry01\", \"consider00\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# join complexity\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_075.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    cdf[[\"iso2_code\", \"language\", \"pci\", \"ubiquity\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# join RCA from the baseline period\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rca_tables[rca_tables[\"period\"]==3].loc[:,[\"iso2_code\", \"language\", \"rca01\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_df[\"rca01\"] = full_df[\"rca01\"].fillna(0)\n",
    "\n",
    "# drop languages with no complexity value\n",
    "full_df.dropna(subset=[\"pci\"], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# join to full_df with entries and PCI\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rel_dens,\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# export for entry models -- only consider 00, 01 patterns\n",
    "full_df[\"entry01\"] = full_df[\"entry01\"].astype(int)\n",
    "full_df[\"consider00\"] = full_df[\"consider00\"].astype(int)\n",
    "export_df = full_df[(full_df[\"entry01\"]==1) | (full_df[\"consider00\"]==1)]\n",
    "export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold_075.csv\", index=False, sep=\";\")\n",
    "#export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold05.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "### ENTRY -- 1.25 threshold\n",
    "\n",
    "# relatedness density -- as in Hidalgo et al. (2007) Science\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_125.csv\", sep=\";\")\n",
    "rel_dens = cdf[cdf[\"year\"] == 2020][[\"iso2_code\", \"language\", \"density\"]].drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "selected_period = \"year\"\n",
    "\n",
    "# steps to prep dataframe of ecomplexity\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "\n",
    "# threshold for RCA : 1.25\n",
    "ps = [1, 2, 3, 4]\n",
    "rca_tables = []\n",
    "for p in ps:\n",
    "    print(p)\n",
    "    temp = bundle_data(df, periods=[p])\n",
    "    temp[\"period\"] = p\n",
    "    rca_tables.append(rca_calculation(temp, c_column=\"iso2_code\", p_column=\"language\", value_column=\"num_pushers\", threshold=1.25))\n",
    "\n",
    "#    dfbs.append(temp)\n",
    "rca_tables = pd.concat(rca_tables)\n",
    "\n",
    "\n",
    "\n",
    "# identify entry following the given patterns\n",
    "entry_pattern = [0,0,1,1]\n",
    "consider_pattern = [0,0,0,0]\n",
    "ent = rca_tables.sort_values([\"period\"], ascending=True).groupby([\"iso2_code\",\"language\"])[\"rca01\"].agg(list).reset_index()\n",
    "ent[\"entry01\"] = ent[\"rca01\"].apply(lambda x: x == entry_pattern).astype(int)\n",
    "ent[\"consider00\"] = ent[\"rca01\"].apply(lambda x: x == consider_pattern).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# full combination\n",
    "all_countries = ent[\"iso2_code\"].unique()\n",
    "all_languages = ent[\"language\"].unique()\n",
    "\n",
    "all_combinations = list(product(all_countries, all_languages))\n",
    "full_df = pd.DataFrame(all_combinations, columns=[\"iso2_code\", \"language\"])\\\n",
    "    .sort_values([\"iso2_code\", \"language\"])\n",
    "\n",
    "# join entries\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    ent[[\"iso2_code\", \"language\", \"entry01\", \"consider00\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# join complexity\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_125.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    cdf[[\"iso2_code\", \"language\", \"pci\", \"ubiquity\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# join RCA from the baseline period\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rca_tables[rca_tables[\"period\"]==3].loc[:,[\"iso2_code\", \"language\", \"rca01\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_df[\"rca01\"] = full_df[\"rca01\"].fillna(0)\n",
    "\n",
    "# drop languages with no complexity value\n",
    "full_df.dropna(subset=[\"pci\"], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# join to full_df with entries and PCI\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rel_dens,\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# export for entry models -- only consider 00, 01 patterns\n",
    "full_df[\"entry01\"] = full_df[\"entry01\"].astype(int)\n",
    "full_df[\"consider00\"] = full_df[\"consider00\"].astype(int)\n",
    "export_df = full_df[(full_df[\"entry01\"]==1) | (full_df[\"consider00\"]==1)]\n",
    "export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold_125.csv\", index=False, sep=\";\")\n",
    "#export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold05.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "### EXIT -- 0.75 threshold\n",
    "\n",
    "# relatedness density -- as in Hidalgo et al. (2007) Science\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_075.csv\", sep=\";\")\n",
    "rel_dens = cdf[cdf[\"year\"] == 2020][[\"iso2_code\", \"language\", \"density\"]].drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "selected_period = \"year\"\n",
    "\n",
    "# steps to prep dataframe of ecomplexity\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "\n",
    "# threshold for RCA : 0.75\n",
    "ps = [1, 2, 3, 4]\n",
    "rca_tables = []\n",
    "for p in ps:\n",
    "    print(p)\n",
    "    temp = bundle_data(df, periods=[p])\n",
    "    temp[\"period\"] = p\n",
    "    rca_tables.append(rca_calculation(temp, c_column=\"iso2_code\", p_column=\"language\", value_column=\"num_pushers\", threshold=0.75))\n",
    "\n",
    "#    dfbs.append(temp)\n",
    "rca_tables = pd.concat(rca_tables)\n",
    "\n",
    "\n",
    "\n",
    "# identify entry following the given patterns\n",
    "exit_pattern = [1,1,0,0]\n",
    "consider_pattern = [1,1,1,1]\n",
    "ext = rca_tables.sort_values([\"period\"], ascending=True).groupby([\"iso2_code\",\"language\"])[\"rca01\"].agg(list).reset_index()\n",
    "ext[\"entry01\"] = ext[\"rca01\"].apply(lambda x: x == exit_pattern).astype(int)\n",
    "ext[\"consider00\"] = ext[\"rca01\"].apply(lambda x: x == consider_pattern).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# full combination\n",
    "all_countries = ext[\"iso2_code\"].unique()\n",
    "all_languages = ext[\"language\"].unique()\n",
    "\n",
    "all_combinations = list(product(all_countries, all_languages))\n",
    "full_df = pd.DataFrame(all_combinations, columns=[\"iso2_code\", \"language\"])\\\n",
    "    .sort_values([\"iso2_code\", \"language\"])\n",
    "\n",
    "# join entries\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    ext[[\"iso2_code\", \"language\", \"entry01\", \"consider00\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# join complexity\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_075.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    cdf[[\"iso2_code\", \"language\", \"pci\", \"ubiquity\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# join RCA from the baseline period\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rca_tables[rca_tables[\"period\"]==3].loc[:,[\"iso2_code\", \"language\", \"rca01\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_df[\"rca01\"] = full_df[\"rca01\"].fillna(0)\n",
    "\n",
    "# drop languages with no complexity value\n",
    "full_df.dropna(subset=[\"pci\"], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# join to full_df with entries and PCI\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rel_dens,\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# export for entry models -- only consider 00, 01 patterns\n",
    "full_df[\"entry01\"] = full_df[\"entry01\"].astype(int)\n",
    "full_df.rename(columns={\"entry01\":\"exit01\"}, inplace=True)\n",
    "full_df[\"consider00\"] = full_df[\"consider00\"].astype(int)\n",
    "export_df = full_df[(full_df[\"exit01\"]==1) | (full_df[\"consider00\"]==1)]\n",
    "export_df.to_csv(\"../outputs/data_exit_regressions_1100_threshold_075.csv\", index=False, sep=\";\")\n",
    "#export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold05.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "### EXIT -- 1.25 threshold\n",
    "\n",
    "# relatedness density -- as in Hidalgo et al. (2007) Science\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_125.csv\", sep=\";\")\n",
    "rel_dens = cdf[cdf[\"year\"] == 2020][[\"iso2_code\", \"language\", \"density\"]].drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "selected_period = \"year\"\n",
    "\n",
    "# steps to prep dataframe of ecomplexity\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "\n",
    "# threshold for RCA : 1.25\n",
    "ps = [1, 2, 3, 4]\n",
    "rca_tables = []\n",
    "for p in ps:\n",
    "    print(p)\n",
    "    temp = bundle_data(df, periods=[p])\n",
    "    temp[\"period\"] = p\n",
    "    rca_tables.append(rca_calculation(temp, c_column=\"iso2_code\", p_column=\"language\", value_column=\"num_pushers\", threshold=1.25))\n",
    "\n",
    "#    dfbs.append(temp)\n",
    "rca_tables = pd.concat(rca_tables)\n",
    "\n",
    "\n",
    "\n",
    "# identify entry following the given patterns\n",
    "exit_pattern = [1,1,0,0]\n",
    "consider_pattern = [1,1,1,1]\n",
    "ext = rca_tables.sort_values([\"period\"], ascending=True).groupby([\"iso2_code\",\"language\"])[\"rca01\"].agg(list).reset_index()\n",
    "ext[\"entry01\"] = ext[\"rca01\"].apply(lambda x: x == exit_pattern).astype(int)\n",
    "ext[\"consider00\"] = ext[\"rca01\"].apply(lambda x: x == consider_pattern).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# full combination\n",
    "all_countries = ext[\"iso2_code\"].unique()\n",
    "all_languages = ext[\"language\"].unique()\n",
    "\n",
    "all_combinations = list(product(all_countries, all_languages))\n",
    "full_df = pd.DataFrame(all_combinations, columns=[\"iso2_code\", \"language\"])\\\n",
    "    .sort_values([\"iso2_code\", \"language\"])\n",
    "\n",
    "# join entries\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    ext[[\"iso2_code\", \"language\", \"entry01\", \"consider00\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# join complexity\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_125.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    cdf[[\"iso2_code\", \"language\", \"pci\", \"ubiquity\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# join RCA from the baseline period\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rca_tables[rca_tables[\"period\"]==3].loc[:,[\"iso2_code\", \"language\", \"rca01\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_df[\"rca01\"] = full_df[\"rca01\"].fillna(0)\n",
    "\n",
    "# drop languages with no complexity value\n",
    "full_df.dropna(subset=[\"pci\"], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# join to full_df with entries and PCI\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rel_dens,\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# export for entry models -- only consider 00, 01 patterns\n",
    "full_df[\"entry01\"] = full_df[\"entry01\"].astype(int)\n",
    "full_df.rename(columns={\"entry01\":\"exit01\"}, inplace=True)\n",
    "full_df[\"consider00\"] = full_df[\"consider00\"].astype(int)\n",
    "export_df = full_df[(full_df[\"exit01\"]==1) | (full_df[\"consider00\"]==1)]\n",
    "export_df.to_csv(\"../outputs/data_exit_regressions_1100_threshold_125.csv\", index=False, sep=\";\")\n",
    "#export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold05.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**new data from World Bank for growth models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add ECI_software value\n",
    "eci_software = pd.read_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\")\n",
    "\n",
    "# country code switcher\n",
    "iso2_codes = eci_software[\"iso2_code\"]\n",
    "iso3_codes = coco.convert(names=iso2_codes, to=\"ISO3\")\n",
    "codes = pd.DataFrame(iso3_codes, iso2_codes).reset_index()\n",
    "codes.columns = [\"iso2_code\", \"iso3_code\"]\n",
    "codes = dict(zip(codes[\"iso2_code\"], codes[\"iso3_code\"]))\n",
    "\n",
    "# clean table for 2020\n",
    "eci_software[\"iso3_code\"] = eci_software[\"iso2_code\"].map(codes)\n",
    "eci_software = eci_software[[\"iso2_code\", \"iso3_code\", \"eci\", \"year\"]].drop_duplicates()\n",
    "eci_software.rename(columns={\"eci\":\"eci_software\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in 3 other ECI measures\n",
    "eci_trade = pd.read_csv(\"../data/Data-ECI-Trade.csv\")\n",
    "eci_trade = eci_trade[[\"Country\", \"2020\", \"2021\", \"2022\"]]\n",
    "eci_trade = pd.melt(\n",
    "        eci_trade,\n",
    "        id_vars=[\"Country\"],\n",
    "        value_vars=[\"2020\", \"2021\", \"2022\"],\n",
    "        var_name=\"year\",\n",
    "        value_name=\"eci_trade\",\n",
    "    ).rename(columns={\"Country\":\"country_name\"})\n",
    "eci_tech = pd.read_csv(\"../data/Data-ECI-Technology.csv\")\n",
    "eci_tech = eci_tech[[\"Country\", \"2020\", \"2021\", \"2022\"]]\n",
    "eci_tech = pd.melt(\n",
    "        eci_tech,\n",
    "        id_vars=[\"Country\"],\n",
    "        value_vars=[\"2020\", \"2021\", \"2022\"],\n",
    "        var_name=\"year\",\n",
    "        value_name=\"eci_tech\",\n",
    "    ).rename(columns={\"Country\":\"country_name\"})\n",
    "eci_research = pd.read_csv(\"../data/Data-ECI-Research.csv\")\n",
    "eci_research = eci_research[[\"Country\", \"2020\", \"2021\", \"2022\"]]\n",
    "eci_research = pd.melt(\n",
    "        eci_research,\n",
    "        id_vars=[\"Country\"],\n",
    "        value_vars=[\"2020\", \"2021\", \"2022\"],\n",
    "        var_name=\"year\",\n",
    "        value_name=\"eci_research\",\n",
    "    ).rename(columns={\"Country\":\"country_name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map country names to iso2_codes - using the country_converter package\n",
    "c_to_iso = dict(\n",
    "    zip(eci_trade.country_name.unique(), coco.convert(names=eci_trade.country_name.unique(), to=\"ISO2\")))\n",
    "eci_trade[\"iso2_code\"] = eci_trade[\"country_name\"].map(c_to_iso)\n",
    "c_to_iso = dict(\n",
    "    zip(eci_tech.country_name.unique(), coco.convert(names=eci_tech.country_name.unique(), to=\"ISO2\")))\n",
    "eci_tech[\"iso2_code\"] = eci_tech[\"country_name\"].map(c_to_iso)\n",
    "c_to_iso = dict(\n",
    "    zip(eci_research.country_name.unique(), coco.convert(names=eci_research.country_name.unique(), to=\"ISO2\")))\n",
    "eci_research[\"iso2_code\"] = eci_research[\"country_name\"].map(c_to_iso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "eci_software[\"year\"] = eci_software[\"year\"].astype(int)\n",
    "eci_trade[\"year\"] = eci_trade[\"year\"].astype(int)\n",
    "eci_tech[\"year\"] = eci_tech[\"year\"].astype(int)\n",
    "eci_research[\"year\"] = eci_research[\"year\"].astype(int)\n",
    "\n",
    "eci_df = pd.merge(\n",
    "    eci_software,\n",
    "    eci_trade,\n",
    "    on=[\"iso2_code\", \"year\"]\n",
    ")\n",
    "eci_df = pd.merge(\n",
    "    eci_df,\n",
    "    eci_tech,\n",
    "    on=[\"iso2_code\", \"year\"]\n",
    ")\n",
    "eci_df = pd.merge(\n",
    "    eci_df,\n",
    "    eci_research,\n",
    "    on=[\"iso2_code\", \"year\"]\n",
    ").drop(columns=[\"country_name_x\", \"country_name_y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def world_bank_data_cleaner(df, names_list, new_names):\n",
    "    filtered_wdf = df[df[\"variable\"].isin(names_list)]\n",
    "\n",
    "    # melt the dataframe for all variables\n",
    "    long_df = pd.melt(\n",
    "        filtered_wdf,\n",
    "        id_vars=[\"variable\", \"country_name\", \"iso3_code\"],\n",
    "        value_vars=[col for col in filtered_wdf.columns if \"YR\" in col],\n",
    "        var_name=\"year\",\n",
    "        value_name=\"value\",\n",
    "    )\n",
    "\n",
    "    # extract the year\n",
    "    long_df[\"year\"] = long_df[\"year\"].str.extract(r\"(\\d{4})\")\n",
    "\n",
    "    # pivot to create separate columns for each variable (optional)\n",
    "    country_df = long_df.pivot_table(\n",
    "        index=[\"country_name\", \"iso3_code\", \"year\"],\n",
    "        columns=\"variable\",\n",
    "        values=\"value\",\n",
    "        aggfunc=\"first\"\n",
    "    ).reset_index()\n",
    "\n",
    "    # rename columns\n",
    "    rename_dict = dict(zip(names_list, new_names))\n",
    "    country_df.rename(columns=rename_dict, inplace=True)\n",
    "    country_df[\"year\"] = country_df[\"year\"].astype(int)\n",
    "    return country_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# World Bank -- general indicators\n",
    "wdf = pd.read_csv(\"../data/worldbank_general_indicators_2018_2023.csv\")\\\n",
    "    .rename(columns={\"Series Name\":\"variable\", \"Country Name\":\"country_name\", \"Country Code\":\"iso3_code\"})\n",
    "\n",
    "# variable names\n",
    "names_list = [\"Population, total\", \"GDP per capita (current US$)\", \"GDP growth (annual %)\", \"GDP (current US$)\"]\n",
    "new_names = [\"population\", \"gdp_per_capita\", \"gdp_growth\", \"gdp_current_USD\"]\n",
    "\n",
    "country_df1 = world_bank_data_cleaner(wdf, names_list=names_list, new_names=new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# World Bank data on natural resources\n",
    "wdf_nat = pd.read_csv(\"../data/worldbank_total_natural_resources_rents_GPD_perc_2014_2021.csv\")\\\n",
    "    .rename(columns={\"Series Name\":\"variable\", \"Country Name\":\"country_name\", \"Country Code\":\"iso3_code\"})\n",
    "\n",
    "# variable names\n",
    "names_list = [\"Total natural resources rents (% of GDP)\"]\n",
    "new_names = [\"natural_resources\"]\n",
    "\n",
    "country_df2 = world_bank_data_cleaner(wdf_nat, names_list=names_list, new_names=new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# World Bank data on human capital\n",
    "wdf_hum = pd.read_csv(\"../data/worldbank_human_capital_indicators_2018_2020.csv\")\\\n",
    "    .rename(columns={\"Series Name\":\"variable\", \"Country Name\":\"country_name\", \"Country Code\":\"iso3_code\"})\n",
    "\n",
    "# variable names\n",
    "names_list = [\"Human Capital Index (HCI) (scale 0-1)\"]\n",
    "new_names = [\"human_capital_index\"]\n",
    "\n",
    "country_df3 = world_bank_data_cleaner(wdf_hum, names_list=names_list, new_names=new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# World Bank data on Gini -- https://data.worldbank.org/indicator/SI.POV.GINI\n",
    "wdf_gini = pd.read_excel(\"../data/worldbank_gini.xls\")\\\n",
    "    .rename(columns={\"Indicator Name\":\"variable\", \"Country Name\":\"country_name\", \"Country Code\":\"iso3_code\"})\n",
    "\n",
    "# variable names\n",
    "gini_df = pd.melt(\n",
    "        wdf_gini,\n",
    "        id_vars=[\"variable\", \"country_name\", \"iso3_code\"],\n",
    "        value_vars=[str(year) for year in range(2010, 2020)],\n",
    "        var_name=\"year\",\n",
    "        value_name=\"gini\",\n",
    "    ).drop(columns=[\"variable\"])\n",
    "\n",
    "# mean Gini -- too many missing data points\n",
    "gini_df[\"gini_mean\"] = gini_df.groupby([\"country_name\", \"iso3_code\"])[\"gini\"].transform(\"mean\")\n",
    "gini_df[\"year\"] = gini_df[\"year\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# World Bank data on emissions\n",
    "wdf_emission = pd.read_excel(\"../data/worldbank_emission_1960_2022.xlsx\")\n",
    "wdf_emission = wdf_emission[wdf_emission[\"Indicator\"].str.contains(\"Total greenhouse gas\", na=False)]\\\n",
    "    .rename(columns={\"Indicator\":\"variable\", \"Economy Name\":\"country_name\", \"Economy ISO3\":\"iso3_code\"})\n",
    "\n",
    "# variable names\n",
    "emission_df = pd.melt(\n",
    "        wdf_emission,\n",
    "        id_vars=[\"variable\", \"country_name\", \"iso3_code\"],\n",
    "        value_vars=[str(year) for year in range(2010, 2022)],\n",
    "        var_name=\"year\",\n",
    "        value_name=\"total_ghg_emissions\",\n",
    "    ).drop(columns=[\"variable\"])\n",
    "emission_df[\"year\"] = emission_df[\"year\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join emission data from Viktor\n",
    "emdf = pd.read_csv(\"../data/regressions_emissions_data.csv\")\n",
    "emdf = emdf[[\"country\", \"year\", \"emissions\"]].drop_duplicates().rename(columns={\"country\":\"iso3_code\", \"emissions\":\"emission_viktor\"})\n",
    "emdf[\"year\"] = emdf[\"year\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join World Bank tables\n",
    "rdf = pd.merge(\n",
    "    country_df1,\n",
    "    country_df2,\n",
    "    on=[\"country_name\", \"iso3_code\", \"year\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "rdf = pd.merge(\n",
    "    rdf,\n",
    "    country_df3,\n",
    "    on=[\"country_name\", \"iso3_code\", \"year\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "rdf = pd.merge(\n",
    "    rdf,\n",
    "    gini_df,\n",
    "    on=[\"country_name\", \"iso3_code\", \"year\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# mean Gini correction\n",
    "rdf[\"gini_mean\"] = rdf.groupby([\"country_name\", \"iso3_code\"])[\"gini_mean\"].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# add emissions\n",
    "rdf[\"year\"] = rdf[\"year\"].astype(int)\n",
    "rdf = pd.merge(\n",
    "    rdf,\n",
    "    emission_df.drop(columns=\"country_name\"),\n",
    "    on=[\"iso3_code\", \"year\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "rdf = pd.merge(\n",
    "    rdf,\n",
    "    emdf,\n",
    "    on=[\"iso3_code\", \"year\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# replace .. w/ NA\n",
    "rdf.replace([\"..\"], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join ECI and country information\n",
    "reg_df = pd.merge(\n",
    "    eci_df,\n",
    "    rdf,\n",
    "    on=[\"iso3_code\", \"year\"],\n",
    "    how=\"outer\",\n",
    "    suffixes=[\"\", \"2\"]\n",
    ").drop(columns=\"country_name2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for regressions in R\n",
    "reg_df.to_csv(\"../outputs/eci_regression_table.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**spatial autocorrelation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data -- ECI_software\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "cdf = cdf[[\"iso2_code\", \"eci\"]].drop_duplicates()\n",
    "\n",
    "# data -- world map\n",
    "cmap = gpd.read_file(\"../data/world-administrative-boundaries.geojson\")\n",
    "cmap = cmap[[\"iso3\", \"iso_3166_1_alpha_2_codes\", \"name\", \"geometry\"]].rename(columns={\"iso_3166_1_alpha_2_codes\" : \"iso2\"})\n",
    "\n",
    "cmap = pd.merge(\n",
    "    cmap,\n",
    "    cdf,\n",
    "    left_on=\"iso2\",\n",
    "    right_on=\"iso2_code\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# clean up\n",
    "cmap = cmap.drop_duplicates(subset=[\"iso2\"])\n",
    "#cmap.dropna(subset=\"iso2\", inplace=True)\n",
    "cmap.dropna(subset=\"eci\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_8256/214813710.py:7: FutureWarning: `idVariable` is deprecated and will be removed in future. Use `ids` instead.\n",
      "  w = weights.Queen.from_dataframe(cmap, idVariable=\"iso2\")\n",
      "/opt/homebrew/lib/python3.12/site-packages/libpysal/weights/contiguity.py:347: UserWarning: The weights matrix is not fully connected: \n",
      " There are 27 disconnected components.\n",
      " There are 22 islands with ids: CY, JM, LK, AU, PH, MG, KR, MU, JP, MT, SG, BH, BB, PR, SN, MV, IS, CU, RE, TW, NZ, TT.\n",
      "  W.__init__(self, neighbors, ids=ids, **kw)\n",
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_8256/214813710.py:13: FutureWarning: `idVariable` is deprecated and will be removed in future. Use `ids` instead.\n",
      "  w = weights.Queen.from_dataframe(cmap, idVariable=\"iso2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5849299169848287 Moran's I\n",
      "0.001 significance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/libpysal/weights/contiguity.py:347: UserWarning: The weights matrix is not fully connected: \n",
      " There are 5 disconnected components.\n",
      "  W.__init__(self, neighbors, ids=ids, **kw)\n"
     ]
    }
   ],
   "source": [
    "### spatial autocorrelation\n",
    "\n",
    "# index setting\n",
    "cmap = cmap.set_index(\"iso2\", drop=False)\n",
    "\n",
    "# create the spatial weights matrix\n",
    "w = weights.Queen.from_dataframe(cmap, idVariable=\"iso2\")\n",
    "\n",
    "# drop island\n",
    "cmap = cmap.drop(w.islands)\n",
    "\n",
    "# create the spatial weights matrix\n",
    "w = weights.Queen.from_dataframe(cmap, idVariable=\"iso2\")\n",
    "\n",
    "# row standardize the matrix\n",
    "w.transform = \"R\"\n",
    "\n",
    "# spatial lag\n",
    "cmap[\"w_eci\"] = weights.lag_spatial(w, cmap[\"eci\"])\n",
    "\n",
    "# z score\n",
    "cmap[\"eci_std\"] = (cmap[\"eci\"] - cmap[\"eci\"].mean()) / cmap[\"eci\"].std()\n",
    "cmap[\"w_eci_std\"] = weights.lag_spatial(w, cmap[\"eci_std\"])\n",
    "\n",
    "# Moran I\n",
    "mi = esda.Moran(cmap[\"eci\"], w)\n",
    "print(mi.I, \"Moran's I\")\n",
    "print(mi.p_sim, \"significance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**clustered languages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n"
     ]
    }
   ],
   "source": [
    "# parameter to choose year / semester / quarter to construct period IDs\n",
    "selected_period = \"year\"\n",
    "\n",
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "\n",
    "# use data_prep_functions to clean the dataframe of ECI_software calculation\n",
    "data = data[data[\"year\"].isin([2020, 2021, 2022, 2023])]\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3458"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clusters of languages\n",
    "cl_df = pd.read_csv(\"../data/language_to_cluster_mapping.csv\")\\\n",
    "    .rename(columns={\"Language\":\"language\", \"Cluster\":\"cluster_id\", \"Cluster Name\":\"cluster_name\"})\n",
    "\n",
    "# combine\n",
    "cl_df = pd.merge(\n",
    "    df,\n",
    "    cl_df,\n",
    "    on=\"language\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# aggregate by clusters\n",
    "#cl_df = cl_df.groupby([\"iso2_code\", \"period\", \"cluster_id\", \"cluster_name\"])[\"num_pushers\"].agg(\"sum\").reset_index()\n",
    "cl_df[\"cluster_name\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Percentage of pairs compared that meet log-supermodularity condition: 9.78%\n",
      "1\n",
      "2020  DONE\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 1: Log-supermodularity condition is not fully satisfied (9.78% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 10.50%\n",
      "2\n",
      "2021  DONE\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 2: Log-supermodularity condition is not fully satisfied (10.50% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 11.00%\n",
      "3\n",
      "2022  DONE\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 3: Log-supermodularity condition is not fully satisfied (11.00% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 11.77%\n",
      "4\n",
      "2023  DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 4: Log-supermodularity condition is not fully satisfied (11.77% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# clusters of languages\n",
    "cl_df = pd.read_csv(\"../data/language_to_cluster_mapping.csv\")\\\n",
    "    .rename(columns={\"Language\":\"language\", \"Cluster\":\"cluster_id\", \"Cluster Name\":\"cluster_name\"})\n",
    "\n",
    "# combine\n",
    "cl_df = pd.merge(\n",
    "    df,\n",
    "    cl_df,\n",
    "    on=\"language\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# drop NAs... -- not so great\n",
    "cl_df.dropna(subset=[\"cluster_id\", \"cluster_name\"], inplace=True)\n",
    "\n",
    "# aggregate by clusters\n",
    "cl_df = cl_df.groupby([\"iso2_code\", \"period\", \"cluster_name\"])[\"num_pushers\"].agg(\"mean\").reset_index()\n",
    "\n",
    "# for ecomplexity calculcation\n",
    "key_cols = {\n",
    "    \"time\": \"period\",\n",
    "    \"loc\": \"iso2_code\",\n",
    "    \"prod\": \"cluster_name\",\n",
    "    \"val\": \"num_pushers\",\n",
    "}\n",
    "\n",
    "# software complexity calculation -- period IDs -- 1 means 2020 on yearly basis\n",
    "ccdf = []\n",
    "ppdf = []\n",
    "year_dict = {1 : 2020, 2 : 2021, 3 : 2022, 4 : 2023}\n",
    "for k in year_dict.keys():\n",
    "    dfb = cl_df[cl_df[\"period\"]==k]\n",
    "    cdf = ecomplexity(dfb, key_cols)\n",
    "    cdf[\"year\"] = year_dict[k]\n",
    "\n",
    "    pdf = proximity(dfb, key_cols)\n",
    "    pdf[\"year\"] = year_dict[k]\n",
    "\n",
    "    # combine yearly dataframes\n",
    "    ccdf.append(cdf)\n",
    "    ppdf.append(pdf)\n",
    "    print(year_dict[k], \" DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine and save -- complexity\n",
    "cluster_cdf = pd.concat(ccdf, axis=0, ignore_index=True)\n",
    "cluster_cdf.to_csv(\"../outputs/eci_clusters_2020_2023.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eci_software</th>\n",
       "      <th>eci_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eci_software</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eci_cluster</th>\n",
       "      <td>0.983028</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              eci_software  eci_cluster\n",
       "eci_software      1.000000     0.983028\n",
       "eci_cluster       0.983028     1.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparison\n",
    "cluster_cdf = pd.read_csv(\"../outputs/eci_clusters_2020_2023.csv\", sep=\";\")\n",
    "eci_software = pd.read_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\")\n",
    "\n",
    "cc_df = pd.merge(\n",
    "    eci_software[eci_software[\"year\"]==2020][[\"iso2_code\", \"eci\"]].drop_duplicates(),\n",
    "    cluster_cdf[cluster_cdf[\"year\"]==2020][[\"iso2_code\", \"eci\"]].drop_duplicates(),\n",
    "    on=[\"iso2_code\"],\n",
    "    how=\"left\",\n",
    "    suffixes=[\"_software\", \"_cluster\"]\n",
    ")\n",
    "\n",
    "cc_df[[\"eci_software\", \"eci_cluster\"]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 unique clusters\n",
      "150 unique clusters\n"
     ]
    }
   ],
   "source": [
    "print(cluster_cdf[\"cluster_name\"].nunique(), \"unique clusters\")\n",
    "print(eci_software[\"language\"].nunique(), \"unique clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iso2_code</th>\n",
       "      <th>cluster_name</th>\n",
       "      <th>num_pushers</th>\n",
       "      <th>period</th>\n",
       "      <th>diversity</th>\n",
       "      <th>ubiquity</th>\n",
       "      <th>mcp</th>\n",
       "      <th>eci</th>\n",
       "      <th>pci</th>\n",
       "      <th>density</th>\n",
       "      <th>coi</th>\n",
       "      <th>cog</th>\n",
       "      <th>rca</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AE</td>\n",
       "      <td>Batch/High-Level Tools</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.156504</td>\n",
       "      <td>2.063356</td>\n",
       "      <td>0.082443</td>\n",
       "      <td>-0.037114</td>\n",
       "      <td>1.144809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AE</td>\n",
       "      <td>Concurrency-Oriented</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.156504</td>\n",
       "      <td>2.024662</td>\n",
       "      <td>0.079966</td>\n",
       "      <td>-0.037114</td>\n",
       "      <td>1.106217</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AE</td>\n",
       "      <td>Cross-Platform Build Systems</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.156504</td>\n",
       "      <td>1.675674</td>\n",
       "      <td>0.106298</td>\n",
       "      <td>-0.037114</td>\n",
       "      <td>0.908092</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AE</td>\n",
       "      <td>Data/Document DSLs</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.156504</td>\n",
       "      <td>1.857410</td>\n",
       "      <td>0.086427</td>\n",
       "      <td>-0.037114</td>\n",
       "      <td>1.011981</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AE</td>\n",
       "      <td>Database Scripting</td>\n",
       "      <td>143.00</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.156504</td>\n",
       "      <td>-0.420149</td>\n",
       "      <td>0.311666</td>\n",
       "      <td>-0.037114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.021919</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23555</th>\n",
       "      <td>ZW</td>\n",
       "      <td>Text Parsing DSLs</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.462247</td>\n",
       "      <td>2.438673</td>\n",
       "      <td>0.048620</td>\n",
       "      <td>-0.333527</td>\n",
       "      <td>1.372230</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23556</th>\n",
       "      <td>ZW</td>\n",
       "      <td>Typed Scripting</td>\n",
       "      <td>181.00</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.462247</td>\n",
       "      <td>1.454026</td>\n",
       "      <td>0.068067</td>\n",
       "      <td>-0.333527</td>\n",
       "      <td>0.856199</td>\n",
       "      <td>0.557868</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23557</th>\n",
       "      <td>ZW</td>\n",
       "      <td>Unix Build Tools</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.462247</td>\n",
       "      <td>1.102043</td>\n",
       "      <td>0.129158</td>\n",
       "      <td>-0.333527</td>\n",
       "      <td>0.725991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23558</th>\n",
       "      <td>ZW</td>\n",
       "      <td>Unix-Like Scripting</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.462247</td>\n",
       "      <td>2.432743</td>\n",
       "      <td>0.045595</td>\n",
       "      <td>-0.333527</td>\n",
       "      <td>1.384479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23559</th>\n",
       "      <td>ZW</td>\n",
       "      <td>Web Markup/Styling</td>\n",
       "      <td>604.25</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.462247</td>\n",
       "      <td>-1.357762</td>\n",
       "      <td>0.496984</td>\n",
       "      <td>-0.333527</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.421369</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23560 rows  14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      iso2_code                  cluster_name  num_pushers  period  diversity  \\\n",
       "0            AE        Batch/High-Level Tools         0.00       1          7   \n",
       "1            AE          Concurrency-Oriented         0.00       1          7   \n",
       "2            AE  Cross-Platform Build Systems         0.00       1          7   \n",
       "3            AE            Data/Document DSLs         0.00       1          7   \n",
       "4            AE            Database Scripting       143.00       1          7   \n",
       "...         ...                           ...          ...     ...        ...   \n",
       "23555        ZW             Text Parsing DSLs         0.00       4          6   \n",
       "23556        ZW               Typed Scripting       181.00       4          6   \n",
       "23557        ZW              Unix Build Tools         0.00       4          6   \n",
       "23558        ZW           Unix-Like Scripting         0.00       4          6   \n",
       "23559        ZW            Web Markup/Styling       604.25       4          6   \n",
       "\n",
       "       ubiquity  mcp       eci       pci   density       coi       cog  \\\n",
       "0            29    0 -0.156504  2.063356  0.082443 -0.037114  1.144809   \n",
       "1            20    0 -0.156504  2.024662  0.079966 -0.037114  1.106217   \n",
       "2            35    0 -0.156504  1.675674  0.106298 -0.037114  0.908092   \n",
       "3            32    0 -0.156504  1.857410  0.086427 -0.037114  1.011981   \n",
       "4            48    1 -0.156504 -0.420149  0.311666 -0.037114  0.000000   \n",
       "...         ...  ...       ...       ...       ...       ...       ...   \n",
       "23555        29    0 -0.462247  2.438673  0.048620 -0.333527  1.372230   \n",
       "23556        34    0 -0.462247  1.454026  0.068067 -0.333527  0.856199   \n",
       "23557        56    0 -0.462247  1.102043  0.129158 -0.333527  0.725991   \n",
       "23558        28    0 -0.462247  2.432743  0.045595 -0.333527  1.384479   \n",
       "23559       121    1 -0.462247 -1.357762  0.496984 -0.333527 -0.000000   \n",
       "\n",
       "            rca  year  \n",
       "0      0.000000  2020  \n",
       "1      0.000000  2020  \n",
       "2      0.000000  2020  \n",
       "3      0.000000  2020  \n",
       "4      1.021919  2020  \n",
       "...         ...   ...  \n",
       "23555  0.000000  2023  \n",
       "23556  0.557868  2023  \n",
       "23557  0.000000  2023  \n",
       "23558  0.000000  2023  \n",
       "23559  1.421369  2023  \n",
       "\n",
       "[23560 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "### ENTRY -- based on clusters\n",
    "\n",
    "# relatedness density -- as in Hidalgo et al. (2007) Science\n",
    "cdf = pd.read_csv(\"../outputs/eci_clusters_2020_2023.csv\", sep=\";\")\n",
    "rel_dens = cdf[cdf[\"year\"] == 2020][[\"iso2_code\", \"cluster_name\", \"density\"]].drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "selected_period = \"year\"\n",
    "\n",
    "# steps to prep dataframe of ecomplexity\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)\n",
    "\n",
    "# clusters of languages\n",
    "cl_df = pd.read_csv(\"../data/language_to_cluster_mapping.csv\")\\\n",
    "    .rename(columns={\"Language\":\"language\", \"Cluster\":\"cluster_id\", \"Cluster Name\":\"cluster_name\"})\n",
    "\n",
    "# combine\n",
    "df = pd.merge(\n",
    "    df,\n",
    "    cl_df,\n",
    "    on=\"language\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# drop NAs... -- not so great\n",
    "df.dropna(subset=[\"cluster_id\", \"cluster_name\"], inplace=True)\n",
    "\n",
    "# aggregate by clusters\n",
    "df = df.groupby([\"iso2_code\", \"period\", \"cluster_name\"])[\"num_pushers\"].agg(\"mean\").reset_index()\n",
    "\n",
    "\n",
    "def bundle_data_clusters(data, periods):\n",
    "    \"\"\"aggreagte data for period by taking the mean number active developers\"\"\"\n",
    "    data = (\n",
    "        data[data[\"period\"].isin(periods)]\n",
    "        .groupby([\"iso2_code\", \"cluster_name\"])[\"num_pushers\"]\n",
    "        .agg(\"mean\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    data[\"period\"] = 1\n",
    "    data[\"num_pushers\"] = data[\"num_pushers\"].astype(int)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# threshold for RCA : 1.00\n",
    "ps = [1, 2, 3, 4]\n",
    "rca_tables = []\n",
    "for p in ps:\n",
    "    print(p)\n",
    "    temp = bundle_data_clusters(df, periods=[p])\n",
    "    temp[\"period\"] = p\n",
    "    rca_tables.append(rca_calculation(temp, c_column=\"iso2_code\", p_column=\"cluster_name\", value_column=\"num_pushers\", threshold=1))\n",
    "\n",
    "#    dfbs.append(temp)\n",
    "rca_tables = pd.concat(rca_tables)\n",
    "\n",
    "\n",
    "# identify entry following the given patterns\n",
    "entry_pattern = [0,0,1,1]\n",
    "consider_pattern = [0,0,0,0]\n",
    "ent = rca_tables.sort_values([\"period\"], ascending=True).groupby([\"iso2_code\",\"cluster_name\"])[\"rca01\"].agg(list).reset_index()\n",
    "ent[\"entry01\"] = ent[\"rca01\"].apply(lambda x: x == entry_pattern).astype(int)\n",
    "ent[\"consider00\"] = ent[\"rca01\"].apply(lambda x: x == consider_pattern).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# full combination\n",
    "all_countries = ent[\"iso2_code\"].unique()\n",
    "all_languages = ent[\"cluster_name\"].unique()\n",
    "\n",
    "all_combinations = list(product(all_countries, all_languages))\n",
    "full_df = pd.DataFrame(all_combinations, columns=[\"iso2_code\", \"cluster_name\"])\\\n",
    "    .sort_values([\"iso2_code\", \"cluster_name\"])\n",
    "\n",
    "# join entries\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    ent[[\"iso2_code\", \"cluster_name\", \"entry01\", \"consider00\"]],\n",
    "    on=[\"iso2_code\", \"cluster_name\"],\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# join complexity\n",
    "cdf = pd.read_csv(\"../outputs/eci_clusters_2020_2023.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    cdf[[\"iso2_code\", \"cluster_name\", \"pci\", \"ubiquity\"]],\n",
    "    on=[\"iso2_code\", \"cluster_name\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# join RCA from the baseline period\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rca_tables[rca_tables[\"period\"]==3].loc[:,[\"iso2_code\", \"cluster_name\", \"rca01\"]],\n",
    "    on=[\"iso2_code\", \"cluster_name\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_df[\"rca01\"] = full_df[\"rca01\"].fillna(0)\n",
    "\n",
    "# drop languages with no complexity value\n",
    "full_df.dropna(subset=[\"pci\"], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# join to full_df with entries and PCI\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rel_dens,\n",
    "    on=[\"iso2_code\", \"cluster_name\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# export for entry models -- only consider 00, 01 patterns\n",
    "full_df[\"entry01\"] = full_df[\"entry01\"].astype(int)\n",
    "full_df[\"consider00\"] = full_df[\"consider00\"].astype(int)\n",
    "export_df = full_df[(full_df[\"entry01\"]==1) | (full_df[\"consider00\"]==1)]\n",
    "export_df.to_csv(\"../outputs/data_entry_regressions_0011_clusters.csv\", index=False, sep=\";\")\n",
    "#export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold05.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "### EXIT -- 1.00 threshold\n",
    "\n",
    "# relatedness density -- as in Hidalgo et al. (2007) Science\n",
    "cdf = pd.read_csv(\"../outputs/eci_clusters_2020_2023.csv\", sep=\";\")\n",
    "rel_dens = cdf[cdf[\"year\"] == 2020][[\"iso2_code\", \"cluster_name\", \"density\"]].drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "selected_period = \"year\"\n",
    "\n",
    "# steps to prep dataframe of ecomplexity\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)\n",
    "\n",
    "# clusters of languages\n",
    "cl_df = pd.read_csv(\"../data/language_to_cluster_mapping.csv\")\\\n",
    "    .rename(columns={\"Language\":\"language\", \"Cluster\":\"cluster_id\", \"Cluster Name\":\"cluster_name\"})\n",
    "\n",
    "# combine\n",
    "df = pd.merge(\n",
    "    df,\n",
    "    cl_df,\n",
    "    on=\"language\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# drop NAs... -- not so great\n",
    "df.dropna(subset=[\"cluster_id\", \"cluster_name\"], inplace=True)\n",
    "\n",
    "# aggregate by clusters\n",
    "df = df.groupby([\"iso2_code\", \"period\", \"cluster_name\"])[\"num_pushers\"].agg(\"mean\").reset_index()\n",
    "\n",
    "\n",
    "def bundle_data_clusters(data, periods):\n",
    "    \"\"\"aggreagte data for period by taking the mean number active developers\"\"\"\n",
    "    data = (\n",
    "        data[data[\"period\"].isin(periods)]\n",
    "        .groupby([\"iso2_code\", \"cluster_name\"])[\"num_pushers\"]\n",
    "        .agg(\"mean\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    data[\"period\"] = 1\n",
    "    data[\"num_pushers\"] = data[\"num_pushers\"].astype(int)\n",
    "    return data\n",
    "\n",
    "\n",
    "# threshold for RCA : 1.00\n",
    "ps = [1, 2, 3, 4]\n",
    "rca_tables = []\n",
    "for p in ps:\n",
    "    print(p)\n",
    "    temp = bundle_data_clusters(df, periods=[p])\n",
    "    temp[\"period\"] = p\n",
    "    rca_tables.append(rca_calculation(temp, c_column=\"iso2_code\", p_column=\"cluster_name\", value_column=\"num_pushers\", threshold=1))\n",
    "\n",
    "#    dfbs.append(temp)\n",
    "rca_tables = pd.concat(rca_tables)\n",
    "\n",
    "\n",
    "\n",
    "# identify entry following the given patterns\n",
    "exit_pattern = [1,1,0,0]\n",
    "consider_pattern = [1,1,1,1]\n",
    "ext = rca_tables.sort_values([\"period\"], ascending=True).groupby([\"iso2_code\",\"cluster_name\"])[\"rca01\"].agg(list).reset_index()\n",
    "ext[\"entry01\"] = ext[\"rca01\"].apply(lambda x: x == exit_pattern).astype(int)\n",
    "ext[\"consider00\"] = ext[\"rca01\"].apply(lambda x: x == consider_pattern).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# full combination\n",
    "all_countries = ext[\"iso2_code\"].unique()\n",
    "all_languages = ext[\"cluster_name\"].unique()\n",
    "\n",
    "all_combinations = list(product(all_countries, all_languages))\n",
    "full_df = pd.DataFrame(all_combinations, columns=[\"iso2_code\", \"cluster_name\"])\\\n",
    "    .sort_values([\"iso2_code\", \"cluster_name\"])\n",
    "\n",
    "# join entries\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    ext[[\"iso2_code\", \"cluster_name\", \"entry01\", \"consider00\"]],\n",
    "    on=[\"iso2_code\", \"cluster_name\"],\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# join complexity\n",
    "cdf = pd.read_csv(\"../outputs/eci_clusters_2020_2023.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    cdf[[\"iso2_code\", \"cluster_name\", \"pci\", \"ubiquity\"]],\n",
    "    on=[\"iso2_code\", \"cluster_name\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# join RCA from the baseline period\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rca_tables[rca_tables[\"period\"]==3].loc[:,[\"iso2_code\", \"cluster_name\", \"rca01\"]],\n",
    "    on=[\"iso2_code\", \"cluster_name\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_df[\"rca01\"] = full_df[\"rca01\"].fillna(0)\n",
    "\n",
    "# drop languages with no complexity value\n",
    "full_df.dropna(subset=[\"pci\"], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# join to full_df with entries and PCI\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rel_dens,\n",
    "    on=[\"iso2_code\", \"cluster_name\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# export for entry models -- only consider 00, 01 patterns\n",
    "full_df[\"entry01\"] = full_df[\"entry01\"].astype(int)\n",
    "full_df.rename(columns={\"entry01\":\"exit01\"}, inplace=True)\n",
    "full_df[\"consider00\"] = full_df[\"consider00\"].astype(int)\n",
    "export_df = full_df[(full_df[\"exit01\"]==1) | (full_df[\"consider00\"]==1)]\n",
    "export_df.to_csv(\"../outputs/data_exit_regressions_1100_clusters.csv\", index=False, sep=\";\")\n",
    "#export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold05.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 1 note -- RCA R2s for 2021**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../outputs/eci_regression_table.csv\", sep=\";\")\n",
    "df = df[df[\"year\"]==2021]\n",
    "df = df[[\"iso2_code\", \"eci_software\", \"eci_trade\", \"eci_tech\", \"eci_research\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_column_to_range(table, column_name):\n",
    "    min_val = table[column_name].min()\n",
    "    max_val = table[column_name].max()\n",
    "    table[column_name] = 2 * (table[column_name] - min_val) / (max_val - min_val) - 1\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = normalize_column_to_range(df, \"eci_software\")\n",
    "df = normalize_column_to_range(df, \"eci_trade\")\n",
    "df = normalize_column_to_range(df, \"eci_tech\")\n",
    "df = normalize_column_to_range(df, \"eci_research\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eci_correlations(df, key_variables):\n",
    "    df.dropna(subset=key_variables, inplace=True)\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(df[key_variables[0]], df[key_variables[1]])\n",
    "    r_squared = r_value ** 2\n",
    "    print(\"R2\", round(r_squared, 3), \"p-value\", round(p_value, 3), \"   \", key_variables[0], \"  \", key_variables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 0.665 p-value 0.0     eci_software    eci_trade\n"
     ]
    }
   ],
   "source": [
    "eci_correlations(df, key_variables=[\"eci_software\", \"eci_trade\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 0.7 p-value 0.0     eci_software    eci_tech\n"
     ]
    }
   ],
   "source": [
    "eci_correlations(df, key_variables=[\"eci_software\", \"eci_tech\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 0.488 p-value 0.0     eci_software    eci_research\n"
     ]
    }
   ],
   "source": [
    "eci_correlations(df, key_variables=[\"eci_software\", \"eci_research\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
