{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from itertools import product\n",
    "from ecomplexity import ecomplexity\n",
    "from ecomplexity import proximity\n",
    "from ecomplexity import calc_density\n",
    "import country_converter as coco\n",
    "import itertools\n",
    "\n",
    "# spatial autocorrelation\n",
    "import geopandas as gpd\n",
    "from pysal.lib import weights\n",
    "from libpysal.io import open as psopen\n",
    "from splot.esda import (\n",
    "    moran_scatterplot, lisa_cluster, plot_local_autocorrelation, plot_moran\n",
    ")\n",
    "from splot.libpysal import plot_spatial_weights\n",
    "import esda\n",
    "\n",
    "# stats\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import linregress\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**different RCA thresholds -- WIP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved in data prep\n",
    "eci_software = pd.read_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_5603/1169018816.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eci_df[\"mcp075\"] = np.where(eci_df[\"rca\"]>=0.75, 1, 0)\n",
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_5603/1169018816.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eci_df[\"mcp125\"] = np.where(eci_df[\"rca\"]>=1.25, 1, 0)\n"
     ]
    }
   ],
   "source": [
    "# year focus\n",
    "eci_df = eci_software[eci_software[\"year\"]==2020]\n",
    "eci_df[\"mcp075\"] = np.where(eci_df[\"rca\"]>=0.75, 1, 0)\n",
    "eci_df[\"mcp125\"] = np.where(eci_df[\"rca\"]>=1.25, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcp_crosstable(df, columns):\n",
    "    mcp_crosstable = df[columns].value_counts().reset_index().sort_values(by=columns).rename(columns={\"count\":\"obs\"})\n",
    "    mcp_crosstable[\"obs_share\"] = round(mcp_crosstable[\"obs\"] / mcp_crosstable[\"obs\"].sum(), 2)\n",
    "    return mcp_crosstable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mcp</th>\n",
       "      <th>mcp075</th>\n",
       "      <th>obs</th>\n",
       "      <th>obs_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15766</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1074</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3593</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mcp  mcp075    obs  obs_share\n",
       "0    0       0  15766       0.77\n",
       "2    0       1   1074       0.05\n",
       "1    1       1   3593       0.18"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcp_crosstable(eci_df, columns=[\"mcp\", \"mcp075\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mcp</th>\n",
       "      <th>mcp125</th>\n",
       "      <th>obs</th>\n",
       "      <th>obs_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16840</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1191</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2402</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mcp  mcp125    obs  obs_share\n",
       "0    0       0  16840       0.82\n",
       "2    1       0   1191       0.06\n",
       "1    1       1   2402       0.12"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcp_crosstable(eci_df, columns=[\"mcp\", \"mcp125\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n"
     ]
    }
   ],
   "source": [
    "# data IN -- for threshold 0.75 AND 1.25\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "\n",
    "# parameter to choose year / semester / quarter to construct period IDs\n",
    "selected_period = \"year\"\n",
    "\n",
    "# for ecomplexity calculcation\n",
    "key_cols = {\n",
    "    \"time\": \"period\",\n",
    "    \"loc\": \"iso2_code\",\n",
    "    \"prod\": \"language\",\n",
    "    \"val\": \"num_pushers\",\n",
    "}\n",
    "\n",
    "# use data_prep_functions to clean the dataframe of ECI_software calculation\n",
    "data = data[data[\"year\"].isin([2020, 2021, 2022, 2023])]\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Percentage of pairs compared that meet log-supermodularity condition: 4.90%\n",
      "1\n",
      "2020  DONE\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 1: Log-supermodularity condition is not fully satisfied (4.90% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 4.28%\n",
      "1\n",
      "2021  DONE\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 1: Log-supermodularity condition is not fully satisfied (4.28% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 4.40%\n",
      "1\n",
      "2022  DONE\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 1: Log-supermodularity condition is not fully satisfied (4.40% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 5.11%\n",
      "1\n",
      "2023  DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 1: Log-supermodularity condition is not fully satisfied (5.11% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# software complexity calculation -- period IDs -- 1 means 2020 on yearly basis\n",
    "ccdf = []\n",
    "ppdf = []\n",
    "year_dict = {1 : 2020, 2 : 2021, 3 : 2022, 4 : 2023}\n",
    "for k in year_dict.keys():\n",
    "    dfb = bundle_data(df, periods=[k])\n",
    "    #cdf = ecomplexity(dfb, key_cols, rca_mcp_threshold=0.75)\n",
    "    cdf = ecomplexity(dfb, key_cols, rca_mcp_threshold=1.25)\n",
    "    cdf[\"year\"] = year_dict[k]\n",
    "\n",
    "    #pdf = proximity(dfb, key_cols, rca_mcp_threshold=0.75)\n",
    "    pdf = proximity(dfb, key_cols, rca_mcp_threshold=1.25)\n",
    "    pdf[\"year\"] = year_dict[k]\n",
    "\n",
    "    # combine yearly dataframes\n",
    "    ccdf.append(cdf)\n",
    "    ppdf.append(pdf)\n",
    "    print(year_dict[k], \" DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine and save -- complexity\n",
    "cdf = pd.concat(ccdf, axis=0, ignore_index=True)\n",
    "#cdf.to_csv(\"../outputs/eci_software_2020_2023_threshold_075.csv\", sep=\";\", index=False)\n",
    "cdf.to_csv(\"../outputs/eci_software_2020_2023_threshold_125.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine and save -- language proximity\n",
    "prox_df = pd.concat(ppdf, axis=0, ignore_index=True)\n",
    "#prox_df.to_csv(\"../outputs/proximity_2020_2023_threshold_075.csv\", sep=\";\", index=False)\n",
    "prox_df.to_csv(\"../outputs/proximity_2020_2023_threshold_125.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "cdf100 = pd.read_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\")\n",
    "cdf100 = cdf100[cdf100[\"year\"]==2020]\n",
    "cdf075 = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_075.csv\", sep=\";\")\n",
    "cdf075 = cdf075[cdf075[\"year\"]==2020]\n",
    "cdf125 = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_125.csv\", sep=\";\")\n",
    "cdf125 = cdf125[cdf125[\"year\"]==2020].rename(columns={\"eci\":\"eci125\"})\n",
    "\n",
    "full_cdf = pd.merge(\n",
    "    cdf100[[\"iso2_code\", \"eci\"]].drop_duplicates(),\n",
    "    cdf075[[\"iso2_code\", \"eci\"]].drop_duplicates(),\n",
    "    on=[\"iso2_code\"],\n",
    "    how=\"left\",\n",
    "    suffixes=[\"100\", \"075\"]\n",
    ")\n",
    "full_cdf = pd.merge(\n",
    "    full_cdf,\n",
    "    cdf125,\n",
    "    on=[\"iso2_code\"],\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "### ENTRY -- 0.75 threshold\n",
    "\n",
    "# relatedness density -- as in Hidalgo et al. (2007) Science\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_075.csv\", sep=\";\")\n",
    "rel_dens = cdf[cdf[\"year\"] == 2020][[\"iso2_code\", \"language\", \"density\"]].drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "selected_period = \"year\"\n",
    "\n",
    "# steps to prep dataframe of ecomplexity\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "\n",
    "# threshold for RCA : 0.75\n",
    "ps = [1, 2, 3, 4]\n",
    "rca_tables = []\n",
    "for p in ps:\n",
    "    print(p)\n",
    "    temp = bundle_data(df, periods=[p])\n",
    "    temp[\"period\"] = p\n",
    "    rca_tables.append(rca_calculation(temp, c_column=\"iso2_code\", p_column=\"language\", value_column=\"num_pushers\", threshold=0.75))\n",
    "\n",
    "#    dfbs.append(temp)\n",
    "rca_tables = pd.concat(rca_tables)\n",
    "\n",
    "\n",
    "\n",
    "# identify entry following the given patterns\n",
    "entry_pattern = [0,0,1,1]\n",
    "consider_pattern = [0,0,0,0]\n",
    "ent = rca_tables.sort_values([\"period\"], ascending=True).groupby([\"iso2_code\",\"language\"])[\"rca01\"].agg(list).reset_index()\n",
    "ent[\"entry01\"] = ent[\"rca01\"].apply(lambda x: x == entry_pattern).astype(int)\n",
    "ent[\"consider00\"] = ent[\"rca01\"].apply(lambda x: x == consider_pattern).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# full combination\n",
    "all_countries = ent[\"iso2_code\"].unique()\n",
    "all_languages = ent[\"language\"].unique()\n",
    "\n",
    "all_combinations = list(product(all_countries, all_languages))\n",
    "full_df = pd.DataFrame(all_combinations, columns=[\"iso2_code\", \"language\"])\\\n",
    "    .sort_values([\"iso2_code\", \"language\"])\n",
    "\n",
    "# join entries\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    ent[[\"iso2_code\", \"language\", \"entry01\", \"consider00\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# join complexity\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_075.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    cdf[[\"iso2_code\", \"language\", \"pci\", \"ubiquity\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# join RCA from the baseline period\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rca_tables[rca_tables[\"period\"]==3].loc[:,[\"iso2_code\", \"language\", \"rca01\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_df[\"rca01\"] = full_df[\"rca01\"].fillna(0)\n",
    "\n",
    "# drop languages with no complexity value\n",
    "full_df.dropna(subset=[\"pci\"], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# join to full_df with entries and PCI\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rel_dens,\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# export for entry models -- only consider 00, 01 patterns\n",
    "full_df[\"entry01\"] = full_df[\"entry01\"].astype(int)\n",
    "full_df[\"consider00\"] = full_df[\"consider00\"].astype(int)\n",
    "export_df = full_df[(full_df[\"entry01\"]==1) | (full_df[\"consider00\"]==1)]\n",
    "export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold_075.csv\", index=False, sep=\";\")\n",
    "#export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold05.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "### ENTRY -- 1.25 threshold\n",
    "\n",
    "# relatedness density -- as in Hidalgo et al. (2007) Science\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_125.csv\", sep=\";\")\n",
    "rel_dens = cdf[cdf[\"year\"] == 2020][[\"iso2_code\", \"language\", \"density\"]].drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "selected_period = \"year\"\n",
    "\n",
    "# steps to prep dataframe of ecomplexity\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "\n",
    "# threshold for RCA : 1.25\n",
    "ps = [1, 2, 3, 4]\n",
    "rca_tables = []\n",
    "for p in ps:\n",
    "    print(p)\n",
    "    temp = bundle_data(df, periods=[p])\n",
    "    temp[\"period\"] = p\n",
    "    rca_tables.append(rca_calculation(temp, c_column=\"iso2_code\", p_column=\"language\", value_column=\"num_pushers\", threshold=1.25))\n",
    "\n",
    "#    dfbs.append(temp)\n",
    "rca_tables = pd.concat(rca_tables)\n",
    "\n",
    "\n",
    "\n",
    "# identify entry following the given patterns\n",
    "entry_pattern = [0,0,1,1]\n",
    "consider_pattern = [0,0,0,0]\n",
    "ent = rca_tables.sort_values([\"period\"], ascending=True).groupby([\"iso2_code\",\"language\"])[\"rca01\"].agg(list).reset_index()\n",
    "ent[\"entry01\"] = ent[\"rca01\"].apply(lambda x: x == entry_pattern).astype(int)\n",
    "ent[\"consider00\"] = ent[\"rca01\"].apply(lambda x: x == consider_pattern).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# full combination\n",
    "all_countries = ent[\"iso2_code\"].unique()\n",
    "all_languages = ent[\"language\"].unique()\n",
    "\n",
    "all_combinations = list(product(all_countries, all_languages))\n",
    "full_df = pd.DataFrame(all_combinations, columns=[\"iso2_code\", \"language\"])\\\n",
    "    .sort_values([\"iso2_code\", \"language\"])\n",
    "\n",
    "# join entries\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    ent[[\"iso2_code\", \"language\", \"entry01\", \"consider00\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# join complexity\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_125.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    cdf[[\"iso2_code\", \"language\", \"pci\", \"ubiquity\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# join RCA from the baseline period\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rca_tables[rca_tables[\"period\"]==3].loc[:,[\"iso2_code\", \"language\", \"rca01\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_df[\"rca01\"] = full_df[\"rca01\"].fillna(0)\n",
    "\n",
    "# drop languages with no complexity value\n",
    "full_df.dropna(subset=[\"pci\"], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# join to full_df with entries and PCI\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rel_dens,\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# export for entry models -- only consider 00, 01 patterns\n",
    "full_df[\"entry01\"] = full_df[\"entry01\"].astype(int)\n",
    "full_df[\"consider00\"] = full_df[\"consider00\"].astype(int)\n",
    "export_df = full_df[(full_df[\"entry01\"]==1) | (full_df[\"consider00\"]==1)]\n",
    "export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold_125.csv\", index=False, sep=\";\")\n",
    "#export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold05.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "### EXIT -- 0.75 threshold\n",
    "\n",
    "# relatedness density -- as in Hidalgo et al. (2007) Science\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_075.csv\", sep=\";\")\n",
    "rel_dens = cdf[cdf[\"year\"] == 2020][[\"iso2_code\", \"language\", \"density\"]].drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "selected_period = \"year\"\n",
    "\n",
    "# steps to prep dataframe of ecomplexity\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "\n",
    "# threshold for RCA : 0.75\n",
    "ps = [1, 2, 3, 4]\n",
    "rca_tables = []\n",
    "for p in ps:\n",
    "    print(p)\n",
    "    temp = bundle_data(df, periods=[p])\n",
    "    temp[\"period\"] = p\n",
    "    rca_tables.append(rca_calculation(temp, c_column=\"iso2_code\", p_column=\"language\", value_column=\"num_pushers\", threshold=0.75))\n",
    "\n",
    "#    dfbs.append(temp)\n",
    "rca_tables = pd.concat(rca_tables)\n",
    "\n",
    "\n",
    "\n",
    "# identify entry following the given patterns\n",
    "exit_pattern = [1,1,0,0]\n",
    "consider_pattern = [1,1,1,1]\n",
    "ext = rca_tables.sort_values([\"period\"], ascending=True).groupby([\"iso2_code\",\"language\"])[\"rca01\"].agg(list).reset_index()\n",
    "ext[\"entry01\"] = ext[\"rca01\"].apply(lambda x: x == exit_pattern).astype(int)\n",
    "ext[\"consider00\"] = ext[\"rca01\"].apply(lambda x: x == consider_pattern).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# full combination\n",
    "all_countries = ext[\"iso2_code\"].unique()\n",
    "all_languages = ext[\"language\"].unique()\n",
    "\n",
    "all_combinations = list(product(all_countries, all_languages))\n",
    "full_df = pd.DataFrame(all_combinations, columns=[\"iso2_code\", \"language\"])\\\n",
    "    .sort_values([\"iso2_code\", \"language\"])\n",
    "\n",
    "# join entries\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    ext[[\"iso2_code\", \"language\", \"entry01\", \"consider00\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# join complexity\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_075.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    cdf[[\"iso2_code\", \"language\", \"pci\", \"ubiquity\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# join RCA from the baseline period\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rca_tables[rca_tables[\"period\"]==3].loc[:,[\"iso2_code\", \"language\", \"rca01\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_df[\"rca01\"] = full_df[\"rca01\"].fillna(0)\n",
    "\n",
    "# drop languages with no complexity value\n",
    "full_df.dropna(subset=[\"pci\"], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# join to full_df with entries and PCI\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rel_dens,\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# export for entry models -- only consider 00, 01 patterns\n",
    "full_df[\"entry01\"] = full_df[\"entry01\"].astype(int)\n",
    "full_df.rename(columns={\"entry01\":\"exit01\"}, inplace=True)\n",
    "full_df[\"consider00\"] = full_df[\"consider00\"].astype(int)\n",
    "export_df = full_df[(full_df[\"exit01\"]==1) | (full_df[\"consider00\"]==1)]\n",
    "export_df.to_csv(\"../outputs/data_exit_regressions_1100_threshold_075.csv\", index=False, sep=\";\")\n",
    "#export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold05.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "### EXIT -- 1.25 threshold\n",
    "\n",
    "# relatedness density -- as in Hidalgo et al. (2007) Science\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_125.csv\", sep=\";\")\n",
    "rel_dens = cdf[cdf[\"year\"] == 2020][[\"iso2_code\", \"language\", \"density\"]].drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "selected_period = \"year\"\n",
    "\n",
    "# steps to prep dataframe of ecomplexity\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "\n",
    "# threshold for RCA : 1.25\n",
    "ps = [1, 2, 3, 4]\n",
    "rca_tables = []\n",
    "for p in ps:\n",
    "    print(p)\n",
    "    temp = bundle_data(df, periods=[p])\n",
    "    temp[\"period\"] = p\n",
    "    rca_tables.append(rca_calculation(temp, c_column=\"iso2_code\", p_column=\"language\", value_column=\"num_pushers\", threshold=1.25))\n",
    "\n",
    "#    dfbs.append(temp)\n",
    "rca_tables = pd.concat(rca_tables)\n",
    "\n",
    "\n",
    "\n",
    "# identify entry following the given patterns\n",
    "exit_pattern = [1,1,0,0]\n",
    "consider_pattern = [1,1,1,1]\n",
    "ext = rca_tables.sort_values([\"period\"], ascending=True).groupby([\"iso2_code\",\"language\"])[\"rca01\"].agg(list).reset_index()\n",
    "ext[\"entry01\"] = ext[\"rca01\"].apply(lambda x: x == exit_pattern).astype(int)\n",
    "ext[\"consider00\"] = ext[\"rca01\"].apply(lambda x: x == consider_pattern).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# full combination\n",
    "all_countries = ext[\"iso2_code\"].unique()\n",
    "all_languages = ext[\"language\"].unique()\n",
    "\n",
    "all_combinations = list(product(all_countries, all_languages))\n",
    "full_df = pd.DataFrame(all_combinations, columns=[\"iso2_code\", \"language\"])\\\n",
    "    .sort_values([\"iso2_code\", \"language\"])\n",
    "\n",
    "# join entries\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    ext[[\"iso2_code\", \"language\", \"entry01\", \"consider00\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# join complexity\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_125.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    cdf[[\"iso2_code\", \"language\", \"pci\", \"ubiquity\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# join RCA from the baseline period\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rca_tables[rca_tables[\"period\"]==3].loc[:,[\"iso2_code\", \"language\", \"rca01\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_df[\"rca01\"] = full_df[\"rca01\"].fillna(0)\n",
    "\n",
    "# drop languages with no complexity value\n",
    "full_df.dropna(subset=[\"pci\"], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# join to full_df with entries and PCI\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rel_dens,\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# export for entry models -- only consider 00, 01 patterns\n",
    "full_df[\"entry01\"] = full_df[\"entry01\"].astype(int)\n",
    "full_df.rename(columns={\"entry01\":\"exit01\"}, inplace=True)\n",
    "full_df[\"consider00\"] = full_df[\"consider00\"].astype(int)\n",
    "export_df = full_df[(full_df[\"exit01\"]==1) | (full_df[\"consider00\"]==1)]\n",
    "export_df.to_csv(\"../outputs/data_exit_regressions_1100_threshold_125.csv\", index=False, sep=\";\")\n",
    "#export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold05.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**spatial autocorrelation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data -- ECI_software\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "cdf = cdf[[\"iso2_code\", \"eci\"]].drop_duplicates()\n",
    "\n",
    "# data -- world map\n",
    "cmap = gpd.read_file(\"../data/world-administrative-boundaries.geojson\")\n",
    "cmap = cmap[[\"iso3\", \"iso_3166_1_alpha_2_codes\", \"name\", \"geometry\"]].rename(columns={\"iso_3166_1_alpha_2_codes\" : \"iso2\"})\n",
    "\n",
    "cmap = pd.merge(\n",
    "    cmap,\n",
    "    cdf,\n",
    "    left_on=\"iso2\",\n",
    "    right_on=\"iso2_code\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# clean up\n",
    "cmap = cmap.drop_duplicates(subset=[\"iso2\"])\n",
    "#cmap.dropna(subset=\"iso2\", inplace=True)\n",
    "cmap.dropna(subset=\"eci\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_8256/214813710.py:7: FutureWarning: `idVariable` is deprecated and will be removed in future. Use `ids` instead.\n",
      "  w = weights.Queen.from_dataframe(cmap, idVariable=\"iso2\")\n",
      "/opt/homebrew/lib/python3.12/site-packages/libpysal/weights/contiguity.py:347: UserWarning: The weights matrix is not fully connected: \n",
      " There are 27 disconnected components.\n",
      " There are 22 islands with ids: CY, JM, LK, AU, PH, MG, KR, MU, JP, MT, SG, BH, BB, PR, SN, MV, IS, CU, RE, TW, NZ, TT.\n",
      "  W.__init__(self, neighbors, ids=ids, **kw)\n",
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_8256/214813710.py:13: FutureWarning: `idVariable` is deprecated and will be removed in future. Use `ids` instead.\n",
      "  w = weights.Queen.from_dataframe(cmap, idVariable=\"iso2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5849299169848287 Moran's I\n",
      "0.001 significance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/libpysal/weights/contiguity.py:347: UserWarning: The weights matrix is not fully connected: \n",
      " There are 5 disconnected components.\n",
      "  W.__init__(self, neighbors, ids=ids, **kw)\n"
     ]
    }
   ],
   "source": [
    "### spatial autocorrelation\n",
    "\n",
    "# index setting\n",
    "cmap = cmap.set_index(\"iso2\", drop=False)\n",
    "\n",
    "# create the spatial weights matrix\n",
    "w = weights.Queen.from_dataframe(cmap, idVariable=\"iso2\")\n",
    "\n",
    "# drop island\n",
    "cmap = cmap.drop(w.islands)\n",
    "\n",
    "# create the spatial weights matrix\n",
    "w = weights.Queen.from_dataframe(cmap, idVariable=\"iso2\")\n",
    "\n",
    "# row standardize the matrix\n",
    "w.transform = \"R\"\n",
    "\n",
    "# spatial lag\n",
    "cmap[\"w_eci\"] = weights.lag_spatial(w, cmap[\"eci\"])\n",
    "\n",
    "# z score\n",
    "cmap[\"eci_std\"] = (cmap[\"eci\"] - cmap[\"eci\"].mean()) / cmap[\"eci\"].std()\n",
    "cmap[\"w_eci_std\"] = weights.lag_spatial(w, cmap[\"eci_std\"])\n",
    "\n",
    "# Moran I\n",
    "mi = esda.Moran(cmap[\"eci\"], w)\n",
    "print(mi.I, \"Moran's I\")\n",
    "print(mi.p_sim, \"significance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**clustered languages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n"
     ]
    }
   ],
   "source": [
    "# parameter to choose year / semester / quarter to construct period IDs\n",
    "selected_period = \"year\"\n",
    "\n",
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "\n",
    "# use data_prep_functions to clean the dataframe of ECI_software calculation\n",
    "data = data[data[\"year\"].isin([2020, 2021, 2022, 2023])]\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3458"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clusters of languages\n",
    "cl_df = pd.read_csv(\"../data/language_to_cluster_mapping.csv\")\\\n",
    "    .rename(columns={\"Language\":\"language\", \"Cluster\":\"cluster_id\", \"Cluster Name\":\"cluster_name\"})\n",
    "\n",
    "# combine\n",
    "cl_df = pd.merge(\n",
    "    df,\n",
    "    cl_df,\n",
    "    on=\"language\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# aggregate by clusters\n",
    "#cl_df = cl_df.groupby([\"iso2_code\", \"period\", \"cluster_id\", \"cluster_name\"])[\"num_pushers\"].agg(\"sum\").reset_index()\n",
    "cl_df[\"cluster_name\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Percentage of pairs compared that meet log-supermodularity condition: 9.78%\n",
      "1\n",
      "2020  DONE\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 1: Log-supermodularity condition is not fully satisfied (9.78% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 10.50%\n",
      "2\n",
      "2021  DONE\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 2: Log-supermodularity condition is not fully satisfied (10.50% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 11.00%\n",
      "3\n",
      "2022  DONE\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 3: Log-supermodularity condition is not fully satisfied (11.00% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 11.77%\n",
      "4\n",
      "2023  DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 4: Log-supermodularity condition is not fully satisfied (11.77% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# clusters of languages\n",
    "cl_df = pd.read_csv(\"../data/language_to_cluster_mapping.csv\")\\\n",
    "    .rename(columns={\"Language\":\"language\", \"Cluster\":\"cluster_id\", \"Cluster Name\":\"cluster_name\"})\n",
    "\n",
    "# combine\n",
    "cl_df = pd.merge(\n",
    "    df,\n",
    "    cl_df,\n",
    "    on=\"language\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# drop NAs... -- not so great\n",
    "cl_df.dropna(subset=[\"cluster_id\", \"cluster_name\"], inplace=True)\n",
    "\n",
    "# aggregate by clusters\n",
    "cl_df = cl_df.groupby([\"iso2_code\", \"period\", \"cluster_name\"])[\"num_pushers\"].agg(\"mean\").reset_index()\n",
    "\n",
    "# for ecomplexity calculcation\n",
    "key_cols = {\n",
    "    \"time\": \"period\",\n",
    "    \"loc\": \"iso2_code\",\n",
    "    \"prod\": \"cluster_name\",\n",
    "    \"val\": \"num_pushers\",\n",
    "}\n",
    "\n",
    "# software complexity calculation -- period IDs -- 1 means 2020 on yearly basis\n",
    "ccdf = []\n",
    "ppdf = []\n",
    "year_dict = {1 : 2020, 2 : 2021, 3 : 2022, 4 : 2023}\n",
    "for k in year_dict.keys():\n",
    "    dfb = cl_df[cl_df[\"period\"]==k]\n",
    "    cdf = ecomplexity(dfb, key_cols)\n",
    "    cdf[\"year\"] = year_dict[k]\n",
    "\n",
    "    pdf = proximity(dfb, key_cols)\n",
    "    pdf[\"year\"] = year_dict[k]\n",
    "\n",
    "    # combine yearly dataframes\n",
    "    ccdf.append(cdf)\n",
    "    ppdf.append(pdf)\n",
    "    print(year_dict[k], \" DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine and save -- complexity\n",
    "cluster_cdf = pd.concat(ccdf, axis=0, ignore_index=True)\n",
    "cluster_cdf.to_csv(\"../outputs/eci_clusters_2020_2023.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eci_software</th>\n",
       "      <th>eci_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eci_software</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eci_cluster</th>\n",
       "      <td>0.983028</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              eci_software  eci_cluster\n",
       "eci_software      1.000000     0.983028\n",
       "eci_cluster       0.983028     1.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparison\n",
    "cluster_cdf = pd.read_csv(\"../outputs/eci_clusters_2020_2023.csv\", sep=\";\")\n",
    "eci_software = pd.read_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\")\n",
    "\n",
    "cc_df = pd.merge(\n",
    "    eci_software[eci_software[\"year\"]==2020][[\"iso2_code\", \"eci\"]].drop_duplicates(),\n",
    "    cluster_cdf[cluster_cdf[\"year\"]==2020][[\"iso2_code\", \"eci\"]].drop_duplicates(),\n",
    "    on=[\"iso2_code\"],\n",
    "    how=\"left\",\n",
    "    suffixes=[\"_software\", \"_cluster\"]\n",
    ")\n",
    "\n",
    "cc_df[[\"eci_software\", \"eci_cluster\"]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 unique clusters\n",
      "150 unique clusters\n"
     ]
    }
   ],
   "source": [
    "print(cluster_cdf[\"cluster_name\"].nunique(), \"unique clusters\")\n",
    "print(eci_software[\"language\"].nunique(), \"unique clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iso2_code</th>\n",
       "      <th>cluster_name</th>\n",
       "      <th>num_pushers</th>\n",
       "      <th>period</th>\n",
       "      <th>diversity</th>\n",
       "      <th>ubiquity</th>\n",
       "      <th>mcp</th>\n",
       "      <th>eci</th>\n",
       "      <th>pci</th>\n",
       "      <th>density</th>\n",
       "      <th>coi</th>\n",
       "      <th>cog</th>\n",
       "      <th>rca</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AE</td>\n",
       "      <td>Batch/High-Level Tools</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.156504</td>\n",
       "      <td>2.063356</td>\n",
       "      <td>0.082443</td>\n",
       "      <td>-0.037114</td>\n",
       "      <td>1.144809</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AE</td>\n",
       "      <td>Concurrency-Oriented</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.156504</td>\n",
       "      <td>2.024662</td>\n",
       "      <td>0.079966</td>\n",
       "      <td>-0.037114</td>\n",
       "      <td>1.106217</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AE</td>\n",
       "      <td>Cross-Platform Build Systems</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.156504</td>\n",
       "      <td>1.675674</td>\n",
       "      <td>0.106298</td>\n",
       "      <td>-0.037114</td>\n",
       "      <td>0.908092</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AE</td>\n",
       "      <td>Data/Document DSLs</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.156504</td>\n",
       "      <td>1.857410</td>\n",
       "      <td>0.086427</td>\n",
       "      <td>-0.037114</td>\n",
       "      <td>1.011981</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AE</td>\n",
       "      <td>Database Scripting</td>\n",
       "      <td>143.00</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.156504</td>\n",
       "      <td>-0.420149</td>\n",
       "      <td>0.311666</td>\n",
       "      <td>-0.037114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.021919</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23555</th>\n",
       "      <td>ZW</td>\n",
       "      <td>Text Parsing DSLs</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.462247</td>\n",
       "      <td>2.438673</td>\n",
       "      <td>0.048620</td>\n",
       "      <td>-0.333527</td>\n",
       "      <td>1.372230</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23556</th>\n",
       "      <td>ZW</td>\n",
       "      <td>Typed Scripting</td>\n",
       "      <td>181.00</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.462247</td>\n",
       "      <td>1.454026</td>\n",
       "      <td>0.068067</td>\n",
       "      <td>-0.333527</td>\n",
       "      <td>0.856199</td>\n",
       "      <td>0.557868</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23557</th>\n",
       "      <td>ZW</td>\n",
       "      <td>Unix Build Tools</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.462247</td>\n",
       "      <td>1.102043</td>\n",
       "      <td>0.129158</td>\n",
       "      <td>-0.333527</td>\n",
       "      <td>0.725991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23558</th>\n",
       "      <td>ZW</td>\n",
       "      <td>Unix-Like Scripting</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.462247</td>\n",
       "      <td>2.432743</td>\n",
       "      <td>0.045595</td>\n",
       "      <td>-0.333527</td>\n",
       "      <td>1.384479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23559</th>\n",
       "      <td>ZW</td>\n",
       "      <td>Web Markup/Styling</td>\n",
       "      <td>604.25</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.462247</td>\n",
       "      <td>-1.357762</td>\n",
       "      <td>0.496984</td>\n",
       "      <td>-0.333527</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.421369</td>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23560 rows  14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      iso2_code                  cluster_name  num_pushers  period  diversity  \\\n",
       "0            AE        Batch/High-Level Tools         0.00       1          7   \n",
       "1            AE          Concurrency-Oriented         0.00       1          7   \n",
       "2            AE  Cross-Platform Build Systems         0.00       1          7   \n",
       "3            AE            Data/Document DSLs         0.00       1          7   \n",
       "4            AE            Database Scripting       143.00       1          7   \n",
       "...         ...                           ...          ...     ...        ...   \n",
       "23555        ZW             Text Parsing DSLs         0.00       4          6   \n",
       "23556        ZW               Typed Scripting       181.00       4          6   \n",
       "23557        ZW              Unix Build Tools         0.00       4          6   \n",
       "23558        ZW           Unix-Like Scripting         0.00       4          6   \n",
       "23559        ZW            Web Markup/Styling       604.25       4          6   \n",
       "\n",
       "       ubiquity  mcp       eci       pci   density       coi       cog  \\\n",
       "0            29    0 -0.156504  2.063356  0.082443 -0.037114  1.144809   \n",
       "1            20    0 -0.156504  2.024662  0.079966 -0.037114  1.106217   \n",
       "2            35    0 -0.156504  1.675674  0.106298 -0.037114  0.908092   \n",
       "3            32    0 -0.156504  1.857410  0.086427 -0.037114  1.011981   \n",
       "4            48    1 -0.156504 -0.420149  0.311666 -0.037114  0.000000   \n",
       "...         ...  ...       ...       ...       ...       ...       ...   \n",
       "23555        29    0 -0.462247  2.438673  0.048620 -0.333527  1.372230   \n",
       "23556        34    0 -0.462247  1.454026  0.068067 -0.333527  0.856199   \n",
       "23557        56    0 -0.462247  1.102043  0.129158 -0.333527  0.725991   \n",
       "23558        28    0 -0.462247  2.432743  0.045595 -0.333527  1.384479   \n",
       "23559       121    1 -0.462247 -1.357762  0.496984 -0.333527 -0.000000   \n",
       "\n",
       "            rca  year  \n",
       "0      0.000000  2020  \n",
       "1      0.000000  2020  \n",
       "2      0.000000  2020  \n",
       "3      0.000000  2020  \n",
       "4      1.021919  2020  \n",
       "...         ...   ...  \n",
       "23555  0.000000  2023  \n",
       "23556  0.557868  2023  \n",
       "23557  0.000000  2023  \n",
       "23558  0.000000  2023  \n",
       "23559  1.421369  2023  \n",
       "\n",
       "[23560 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "### ENTRY -- based on clusters\n",
    "\n",
    "# relatedness density -- as in Hidalgo et al. (2007) Science\n",
    "cdf = pd.read_csv(\"../outputs/eci_clusters_2020_2023.csv\", sep=\";\")\n",
    "rel_dens = cdf[cdf[\"year\"] == 2020][[\"iso2_code\", \"cluster_name\", \"density\"]].drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "selected_period = \"year\"\n",
    "\n",
    "# steps to prep dataframe of ecomplexity\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)\n",
    "\n",
    "# clusters of languages\n",
    "cl_df = pd.read_csv(\"../data/language_to_cluster_mapping.csv\")\\\n",
    "    .rename(columns={\"Language\":\"language\", \"Cluster\":\"cluster_id\", \"Cluster Name\":\"cluster_name\"})\n",
    "\n",
    "# combine\n",
    "df = pd.merge(\n",
    "    df,\n",
    "    cl_df,\n",
    "    on=\"language\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# drop NAs... -- not so great\n",
    "df.dropna(subset=[\"cluster_id\", \"cluster_name\"], inplace=True)\n",
    "\n",
    "# aggregate by clusters\n",
    "df = df.groupby([\"iso2_code\", \"period\", \"cluster_name\"])[\"num_pushers\"].agg(\"mean\").reset_index()\n",
    "\n",
    "\n",
    "def bundle_data_clusters(data, periods):\n",
    "    \"\"\"aggreagte data for period by taking the mean number active developers\"\"\"\n",
    "    data = (\n",
    "        data[data[\"period\"].isin(periods)]\n",
    "        .groupby([\"iso2_code\", \"cluster_name\"])[\"num_pushers\"]\n",
    "        .agg(\"mean\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    data[\"period\"] = 1\n",
    "    data[\"num_pushers\"] = data[\"num_pushers\"].astype(int)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# threshold for RCA : 1.00\n",
    "ps = [1, 2, 3, 4]\n",
    "rca_tables = []\n",
    "for p in ps:\n",
    "    print(p)\n",
    "    temp = bundle_data_clusters(df, periods=[p])\n",
    "    temp[\"period\"] = p\n",
    "    rca_tables.append(rca_calculation(temp, c_column=\"iso2_code\", p_column=\"cluster_name\", value_column=\"num_pushers\", threshold=1))\n",
    "\n",
    "#    dfbs.append(temp)\n",
    "rca_tables = pd.concat(rca_tables)\n",
    "\n",
    "\n",
    "# identify entry following the given patterns\n",
    "entry_pattern = [0,0,1,1]\n",
    "consider_pattern = [0,0,0,0]\n",
    "ent = rca_tables.sort_values([\"period\"], ascending=True).groupby([\"iso2_code\",\"cluster_name\"])[\"rca01\"].agg(list).reset_index()\n",
    "ent[\"entry01\"] = ent[\"rca01\"].apply(lambda x: x == entry_pattern).astype(int)\n",
    "ent[\"consider00\"] = ent[\"rca01\"].apply(lambda x: x == consider_pattern).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# full combination\n",
    "all_countries = ent[\"iso2_code\"].unique()\n",
    "all_languages = ent[\"cluster_name\"].unique()\n",
    "\n",
    "all_combinations = list(product(all_countries, all_languages))\n",
    "full_df = pd.DataFrame(all_combinations, columns=[\"iso2_code\", \"cluster_name\"])\\\n",
    "    .sort_values([\"iso2_code\", \"cluster_name\"])\n",
    "\n",
    "# join entries\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    ent[[\"iso2_code\", \"cluster_name\", \"entry01\", \"consider00\"]],\n",
    "    on=[\"iso2_code\", \"cluster_name\"],\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# join complexity\n",
    "cdf = pd.read_csv(\"../outputs/eci_clusters_2020_2023.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    cdf[[\"iso2_code\", \"cluster_name\", \"pci\", \"ubiquity\"]],\n",
    "    on=[\"iso2_code\", \"cluster_name\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# join RCA from the baseline period\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rca_tables[rca_tables[\"period\"]==3].loc[:,[\"iso2_code\", \"cluster_name\", \"rca01\"]],\n",
    "    on=[\"iso2_code\", \"cluster_name\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_df[\"rca01\"] = full_df[\"rca01\"].fillna(0)\n",
    "\n",
    "# drop languages with no complexity value\n",
    "full_df.dropna(subset=[\"pci\"], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# join to full_df with entries and PCI\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rel_dens,\n",
    "    on=[\"iso2_code\", \"cluster_name\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# export for entry models -- only consider 00, 01 patterns\n",
    "full_df[\"entry01\"] = full_df[\"entry01\"].astype(int)\n",
    "full_df[\"consider00\"] = full_df[\"consider00\"].astype(int)\n",
    "export_df = full_df[(full_df[\"entry01\"]==1) | (full_df[\"consider00\"]==1)]\n",
    "export_df.to_csv(\"../outputs/data_entry_regressions_0011_clusters.csv\", index=False, sep=\";\")\n",
    "#export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold05.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "### EXIT -- 1.00 threshold\n",
    "\n",
    "# relatedness density -- as in Hidalgo et al. (2007) Science\n",
    "cdf = pd.read_csv(\"../outputs/eci_clusters_2020_2023.csv\", sep=\";\")\n",
    "rel_dens = cdf[cdf[\"year\"] == 2020][[\"iso2_code\", \"cluster_name\", \"density\"]].drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "selected_period = \"year\"\n",
    "\n",
    "# steps to prep dataframe of ecomplexity\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)\n",
    "\n",
    "# clusters of languages\n",
    "cl_df = pd.read_csv(\"../data/language_to_cluster_mapping.csv\")\\\n",
    "    .rename(columns={\"Language\":\"language\", \"Cluster\":\"cluster_id\", \"Cluster Name\":\"cluster_name\"})\n",
    "\n",
    "# combine\n",
    "df = pd.merge(\n",
    "    df,\n",
    "    cl_df,\n",
    "    on=\"language\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# drop NAs... -- not so great\n",
    "df.dropna(subset=[\"cluster_id\", \"cluster_name\"], inplace=True)\n",
    "\n",
    "# aggregate by clusters\n",
    "df = df.groupby([\"iso2_code\", \"period\", \"cluster_name\"])[\"num_pushers\"].agg(\"mean\").reset_index()\n",
    "\n",
    "\n",
    "def bundle_data_clusters(data, periods):\n",
    "    \"\"\"aggreagte data for period by taking the mean number active developers\"\"\"\n",
    "    data = (\n",
    "        data[data[\"period\"].isin(periods)]\n",
    "        .groupby([\"iso2_code\", \"cluster_name\"])[\"num_pushers\"]\n",
    "        .agg(\"mean\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    data[\"period\"] = 1\n",
    "    data[\"num_pushers\"] = data[\"num_pushers\"].astype(int)\n",
    "    return data\n",
    "\n",
    "\n",
    "# threshold for RCA : 1.00\n",
    "ps = [1, 2, 3, 4]\n",
    "rca_tables = []\n",
    "for p in ps:\n",
    "    print(p)\n",
    "    temp = bundle_data_clusters(df, periods=[p])\n",
    "    temp[\"period\"] = p\n",
    "    rca_tables.append(rca_calculation(temp, c_column=\"iso2_code\", p_column=\"cluster_name\", value_column=\"num_pushers\", threshold=1))\n",
    "\n",
    "#    dfbs.append(temp)\n",
    "rca_tables = pd.concat(rca_tables)\n",
    "\n",
    "\n",
    "\n",
    "# identify entry following the given patterns\n",
    "exit_pattern = [1,1,0,0]\n",
    "consider_pattern = [1,1,1,1]\n",
    "ext = rca_tables.sort_values([\"period\"], ascending=True).groupby([\"iso2_code\",\"cluster_name\"])[\"rca01\"].agg(list).reset_index()\n",
    "ext[\"entry01\"] = ext[\"rca01\"].apply(lambda x: x == exit_pattern).astype(int)\n",
    "ext[\"consider00\"] = ext[\"rca01\"].apply(lambda x: x == consider_pattern).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# full combination\n",
    "all_countries = ext[\"iso2_code\"].unique()\n",
    "all_languages = ext[\"cluster_name\"].unique()\n",
    "\n",
    "all_combinations = list(product(all_countries, all_languages))\n",
    "full_df = pd.DataFrame(all_combinations, columns=[\"iso2_code\", \"cluster_name\"])\\\n",
    "    .sort_values([\"iso2_code\", \"cluster_name\"])\n",
    "\n",
    "# join entries\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    ext[[\"iso2_code\", \"cluster_name\", \"entry01\", \"consider00\"]],\n",
    "    on=[\"iso2_code\", \"cluster_name\"],\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# join complexity\n",
    "cdf = pd.read_csv(\"../outputs/eci_clusters_2020_2023.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    cdf[[\"iso2_code\", \"cluster_name\", \"pci\", \"ubiquity\"]],\n",
    "    on=[\"iso2_code\", \"cluster_name\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# join RCA from the baseline period\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rca_tables[rca_tables[\"period\"]==3].loc[:,[\"iso2_code\", \"cluster_name\", \"rca01\"]],\n",
    "    on=[\"iso2_code\", \"cluster_name\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_df[\"rca01\"] = full_df[\"rca01\"].fillna(0)\n",
    "\n",
    "# drop languages with no complexity value\n",
    "full_df.dropna(subset=[\"pci\"], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# join to full_df with entries and PCI\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rel_dens,\n",
    "    on=[\"iso2_code\", \"cluster_name\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# export for entry models -- only consider 00, 01 patterns\n",
    "full_df[\"entry01\"] = full_df[\"entry01\"].astype(int)\n",
    "full_df.rename(columns={\"entry01\":\"exit01\"}, inplace=True)\n",
    "full_df[\"consider00\"] = full_df[\"consider00\"].astype(int)\n",
    "export_df = full_df[(full_df[\"exit01\"]==1) | (full_df[\"consider00\"]==1)]\n",
    "export_df.to_csv(\"../outputs/data_exit_regressions_1100_clusters.csv\", index=False, sep=\";\")\n",
    "#export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold05.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 1 note -- RCA R2s for 2021**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../outputs/eci_regression_table.csv\", sep=\";\")\n",
    "df = df[df[\"year\"]==2021]\n",
    "df = df[[\"iso2_code\", \"eci_software\", \"eci_trade\", \"eci_tech\", \"eci_research\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_column_to_range(table, column_name):\n",
    "    min_val = table[column_name].min()\n",
    "    max_val = table[column_name].max()\n",
    "    table[column_name] = 2 * (table[column_name] - min_val) / (max_val - min_val) - 1\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = normalize_column_to_range(df, \"eci_software\")\n",
    "df = normalize_column_to_range(df, \"eci_trade\")\n",
    "df = normalize_column_to_range(df, \"eci_tech\")\n",
    "df = normalize_column_to_range(df, \"eci_research\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eci_correlations(df, key_variables):\n",
    "    df.dropna(subset=key_variables, inplace=True)\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(df[key_variables[0]], df[key_variables[1]])\n",
    "    r_squared = r_value ** 2\n",
    "    print(\"R2\", round(r_squared, 3), \"p-value\", round(p_value, 3), \"   \", key_variables[0], \"  \", key_variables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 0.665 p-value 0.0     eci_software    eci_trade\n"
     ]
    }
   ],
   "source": [
    "eci_correlations(df, key_variables=[\"eci_software\", \"eci_trade\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 0.7 p-value 0.0     eci_software    eci_tech\n"
     ]
    }
   ],
   "source": [
    "eci_correlations(df, key_variables=[\"eci_software\", \"eci_tech\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 0.488 p-value 0.0     eci_software    eci_research\n"
     ]
    }
   ],
   "source": [
    "eci_correlations(df, key_variables=[\"eci_software\", \"eci_research\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
