{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from itertools import product\n",
    "from ecomplexity import ecomplexity\n",
    "from ecomplexity import proximity\n",
    "from ecomplexity import calc_density\n",
    "import country_converter as coco\n",
    "import itertools\n",
    "\n",
    "# spatial autocorrelation\n",
    "import geopandas as gpd\n",
    "from pysal.lib import weights\n",
    "from libpysal.io import open as psopen\n",
    "from splot.esda import (\n",
    "    moran_scatterplot, lisa_cluster, plot_local_autocorrelation, plot_moran\n",
    ")\n",
    "from splot.libpysal import plot_spatial_weights\n",
    "import esda\n",
    "\n",
    "# stats\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import linregress\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**different RCA thresholds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved in data prep\n",
    "eci_software = pd.read_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_5603/1169018816.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eci_df[\"mcp075\"] = np.where(eci_df[\"rca\"]>=0.75, 1, 0)\n",
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_5603/1169018816.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eci_df[\"mcp125\"] = np.where(eci_df[\"rca\"]>=1.25, 1, 0)\n"
     ]
    }
   ],
   "source": [
    "# year focus\n",
    "eci_df = eci_software[eci_software[\"year\"]==2020]\n",
    "eci_df[\"mcp075\"] = np.where(eci_df[\"rca\"]>=0.75, 1, 0)\n",
    "eci_df[\"mcp125\"] = np.where(eci_df[\"rca\"]>=1.25, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcp_crosstable(df, columns):\n",
    "    mcp_crosstable = df[columns].value_counts().reset_index().sort_values(by=columns).rename(columns={\"count\":\"obs\"})\n",
    "    mcp_crosstable[\"obs_share\"] = round(mcp_crosstable[\"obs\"] / mcp_crosstable[\"obs\"].sum(), 2)\n",
    "    return mcp_crosstable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mcp</th>\n",
       "      <th>mcp075</th>\n",
       "      <th>obs</th>\n",
       "      <th>obs_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15766</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1074</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3593</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mcp  mcp075    obs  obs_share\n",
       "0    0       0  15766       0.77\n",
       "2    0       1   1074       0.05\n",
       "1    1       1   3593       0.18"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcp_crosstable(eci_df, columns=[\"mcp\", \"mcp075\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mcp</th>\n",
       "      <th>mcp125</th>\n",
       "      <th>obs</th>\n",
       "      <th>obs_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16840</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1191</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2402</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mcp  mcp125    obs  obs_share\n",
       "0    0       0  16840       0.82\n",
       "2    1       0   1191       0.06\n",
       "1    1       1   2402       0.12"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcp_crosstable(eci_df, columns=[\"mcp\", \"mcp125\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n"
     ]
    }
   ],
   "source": [
    "# data IN -- for threshold 0.75 AND 1.25\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "\n",
    "# parameter to choose year / semester / quarter to construct period IDs\n",
    "selected_period = \"year\"\n",
    "\n",
    "# for ecomplexity calculcation\n",
    "key_cols = {\n",
    "    \"time\": \"period\",\n",
    "    \"loc\": \"iso2_code\",\n",
    "    \"prod\": \"language\",\n",
    "    \"val\": \"num_pushers\",\n",
    "}\n",
    "\n",
    "# use data_prep_functions to clean the dataframe of ECI_software calculation\n",
    "data = data[data[\"year\"].isin([2020, 2021, 2022, 2023])]\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Percentage of pairs compared that meet log-supermodularity condition: 4.90%\n",
      "1\n",
      "2020  DONE\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 1: Log-supermodularity condition is not fully satisfied (4.90% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 4.28%\n",
      "1\n",
      "2021  DONE\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 1: Log-supermodularity condition is not fully satisfied (4.28% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 4.40%\n",
      "1\n",
      "2022  DONE\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 1: Log-supermodularity condition is not fully satisfied (4.40% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 5.11%\n",
      "1\n",
      "2023  DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 1: Log-supermodularity condition is not fully satisfied (5.11% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# software complexity calculation -- period IDs -- 1 means 2020 on yearly basis\n",
    "ccdf = []\n",
    "ppdf = []\n",
    "year_dict = {1 : 2020, 2 : 2021, 3 : 2022, 4 : 2023}\n",
    "for k in year_dict.keys():\n",
    "    dfb = bundle_data(df, periods=[k])\n",
    "    #cdf = ecomplexity(dfb, key_cols, rca_mcp_threshold=0.75)\n",
    "    cdf = ecomplexity(dfb, key_cols, rca_mcp_threshold=1.25)\n",
    "    cdf[\"year\"] = year_dict[k]\n",
    "\n",
    "    #pdf = proximity(dfb, key_cols, rca_mcp_threshold=0.75)\n",
    "    pdf = proximity(dfb, key_cols, rca_mcp_threshold=1.25)\n",
    "    pdf[\"year\"] = year_dict[k]\n",
    "\n",
    "    # combine yearly dataframes\n",
    "    ccdf.append(cdf)\n",
    "    ppdf.append(pdf)\n",
    "    print(year_dict[k], \" DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine and save -- complexity\n",
    "cdf = pd.concat(ccdf, axis=0, ignore_index=True)\n",
    "#cdf.to_csv(\"../outputs/eci_software_2020_2023_threshold_075.csv\", sep=\";\", index=False)\n",
    "cdf.to_csv(\"../outputs/eci_software_2020_2023_threshold_125.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine and save -- language proximity\n",
    "prox_df = pd.concat(ppdf, axis=0, ignore_index=True)\n",
    "#prox_df.to_csv(\"../outputs/proximity_2020_2023_threshold_075.csv\", sep=\";\", index=False)\n",
    "prox_df.to_csv(\"../outputs/proximity_2020_2023_threshold_125.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix\n",
    "cdf100 = pd.read_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\")\n",
    "cdf100 = cdf100[cdf100[\"year\"]==2020]\n",
    "cdf075 = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_075.csv\", sep=\";\")\n",
    "cdf075 = cdf075[cdf075[\"year\"]==2020]\n",
    "cdf125 = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_125.csv\", sep=\";\")\n",
    "cdf125 = cdf125[cdf125[\"year\"]==2020].rename(columns={\"eci\":\"eci125\"})\n",
    "\n",
    "full_cdf = pd.merge(\n",
    "    cdf100[[\"iso2_code\", \"eci\"]].drop_duplicates(),\n",
    "    cdf075[[\"iso2_code\", \"eci\"]].drop_duplicates(),\n",
    "    on=[\"iso2_code\"],\n",
    "    how=\"left\",\n",
    "    suffixes=[\"100\", \"075\"]\n",
    ")\n",
    "full_cdf = pd.merge(\n",
    "    full_cdf,\n",
    "    cdf125,\n",
    "    on=[\"iso2_code\"],\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "### ENTRY -- 0.75 threshold\n",
    "\n",
    "# relatedness density -- as in Hidalgo et al. (2007) Science\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_075.csv\", sep=\";\")\n",
    "rel_dens = cdf[cdf[\"year\"] == 2020][[\"iso2_code\", \"language\", \"density\"]].drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "selected_period = \"year\"\n",
    "\n",
    "# steps to prep dataframe of ecomplexity\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "\n",
    "# threshold for RCA : 0.75\n",
    "ps = [1, 2, 3, 4]\n",
    "rca_tables = []\n",
    "for p in ps:\n",
    "    print(p)\n",
    "    temp = bundle_data(df, periods=[p])\n",
    "    temp[\"period\"] = p\n",
    "    rca_tables.append(rca_calculation(temp, c_column=\"iso2_code\", p_column=\"language\", value_column=\"num_pushers\", threshold=0.75))\n",
    "\n",
    "#    dfbs.append(temp)\n",
    "rca_tables = pd.concat(rca_tables)\n",
    "\n",
    "\n",
    "\n",
    "# identify entry following the given patterns\n",
    "entry_pattern = [0,0,1,1]\n",
    "consider_pattern = [0,0,0,0]\n",
    "ent = rca_tables.sort_values([\"period\"], ascending=True).groupby([\"iso2_code\",\"language\"])[\"rca01\"].agg(list).reset_index()\n",
    "ent[\"entry01\"] = ent[\"rca01\"].apply(lambda x: x == entry_pattern).astype(int)\n",
    "ent[\"consider00\"] = ent[\"rca01\"].apply(lambda x: x == consider_pattern).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# full combination\n",
    "all_countries = ent[\"iso2_code\"].unique()\n",
    "all_languages = ent[\"language\"].unique()\n",
    "\n",
    "all_combinations = list(product(all_countries, all_languages))\n",
    "full_df = pd.DataFrame(all_combinations, columns=[\"iso2_code\", \"language\"])\\\n",
    "    .sort_values([\"iso2_code\", \"language\"])\n",
    "\n",
    "# join entries\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    ent[[\"iso2_code\", \"language\", \"entry01\", \"consider00\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# join complexity\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_075.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    cdf[[\"iso2_code\", \"language\", \"pci\", \"ubiquity\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# join RCA from the baseline period\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rca_tables[rca_tables[\"period\"]==3].loc[:,[\"iso2_code\", \"language\", \"rca01\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_df[\"rca01\"] = full_df[\"rca01\"].fillna(0)\n",
    "\n",
    "# drop languages with no complexity value\n",
    "full_df.dropna(subset=[\"pci\"], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# join to full_df with entries and PCI\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rel_dens,\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# export for entry models -- only consider 00, 01 patterns\n",
    "full_df[\"entry01\"] = full_df[\"entry01\"].astype(int)\n",
    "full_df[\"consider00\"] = full_df[\"consider00\"].astype(int)\n",
    "export_df = full_df[(full_df[\"entry01\"]==1) | (full_df[\"consider00\"]==1)]\n",
    "export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold_075.csv\", index=False, sep=\";\")\n",
    "#export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold05.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "### ENTRY -- 1.25 threshold\n",
    "\n",
    "# relatedness density -- as in Hidalgo et al. (2007) Science\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_125.csv\", sep=\";\")\n",
    "rel_dens = cdf[cdf[\"year\"] == 2020][[\"iso2_code\", \"language\", \"density\"]].drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "selected_period = \"year\"\n",
    "\n",
    "# steps to prep dataframe of ecomplexity\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "\n",
    "# threshold for RCA : 1.25\n",
    "ps = [1, 2, 3, 4]\n",
    "rca_tables = []\n",
    "for p in ps:\n",
    "    print(p)\n",
    "    temp = bundle_data(df, periods=[p])\n",
    "    temp[\"period\"] = p\n",
    "    rca_tables.append(rca_calculation(temp, c_column=\"iso2_code\", p_column=\"language\", value_column=\"num_pushers\", threshold=1.25))\n",
    "\n",
    "#    dfbs.append(temp)\n",
    "rca_tables = pd.concat(rca_tables)\n",
    "\n",
    "\n",
    "\n",
    "# identify entry following the given patterns\n",
    "entry_pattern = [0,0,1,1]\n",
    "consider_pattern = [0,0,0,0]\n",
    "ent = rca_tables.sort_values([\"period\"], ascending=True).groupby([\"iso2_code\",\"language\"])[\"rca01\"].agg(list).reset_index()\n",
    "ent[\"entry01\"] = ent[\"rca01\"].apply(lambda x: x == entry_pattern).astype(int)\n",
    "ent[\"consider00\"] = ent[\"rca01\"].apply(lambda x: x == consider_pattern).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# full combination\n",
    "all_countries = ent[\"iso2_code\"].unique()\n",
    "all_languages = ent[\"language\"].unique()\n",
    "\n",
    "all_combinations = list(product(all_countries, all_languages))\n",
    "full_df = pd.DataFrame(all_combinations, columns=[\"iso2_code\", \"language\"])\\\n",
    "    .sort_values([\"iso2_code\", \"language\"])\n",
    "\n",
    "# join entries\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    ent[[\"iso2_code\", \"language\", \"entry01\", \"consider00\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# join complexity\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_125.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    cdf[[\"iso2_code\", \"language\", \"pci\", \"ubiquity\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# join RCA from the baseline period\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rca_tables[rca_tables[\"period\"]==3].loc[:,[\"iso2_code\", \"language\", \"rca01\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_df[\"rca01\"] = full_df[\"rca01\"].fillna(0)\n",
    "\n",
    "# drop languages with no complexity value\n",
    "full_df.dropna(subset=[\"pci\"], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# join to full_df with entries and PCI\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rel_dens,\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# export for entry models -- only consider 00, 01 patterns\n",
    "full_df[\"entry01\"] = full_df[\"entry01\"].astype(int)\n",
    "full_df[\"consider00\"] = full_df[\"consider00\"].astype(int)\n",
    "export_df = full_df[(full_df[\"entry01\"]==1) | (full_df[\"consider00\"]==1)]\n",
    "export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold_125.csv\", index=False, sep=\";\")\n",
    "#export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold05.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "### EXIT -- 0.75 threshold\n",
    "\n",
    "# relatedness density -- as in Hidalgo et al. (2007) Science\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_075.csv\", sep=\";\")\n",
    "rel_dens = cdf[cdf[\"year\"] == 2020][[\"iso2_code\", \"language\", \"density\"]].drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "selected_period = \"year\"\n",
    "\n",
    "# steps to prep dataframe of ecomplexity\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "\n",
    "# threshold for RCA : 0.75\n",
    "ps = [1, 2, 3, 4]\n",
    "rca_tables = []\n",
    "for p in ps:\n",
    "    print(p)\n",
    "    temp = bundle_data(df, periods=[p])\n",
    "    temp[\"period\"] = p\n",
    "    rca_tables.append(rca_calculation(temp, c_column=\"iso2_code\", p_column=\"language\", value_column=\"num_pushers\", threshold=0.75))\n",
    "\n",
    "#    dfbs.append(temp)\n",
    "rca_tables = pd.concat(rca_tables)\n",
    "\n",
    "\n",
    "\n",
    "# identify entry following the given patterns\n",
    "exit_pattern = [1,1,0,0]\n",
    "consider_pattern = [1,1,1,1]\n",
    "ext = rca_tables.sort_values([\"period\"], ascending=True).groupby([\"iso2_code\",\"language\"])[\"rca01\"].agg(list).reset_index()\n",
    "ext[\"entry01\"] = ext[\"rca01\"].apply(lambda x: x == exit_pattern).astype(int)\n",
    "ext[\"consider00\"] = ext[\"rca01\"].apply(lambda x: x == consider_pattern).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# full combination\n",
    "all_countries = ext[\"iso2_code\"].unique()\n",
    "all_languages = ext[\"language\"].unique()\n",
    "\n",
    "all_combinations = list(product(all_countries, all_languages))\n",
    "full_df = pd.DataFrame(all_combinations, columns=[\"iso2_code\", \"language\"])\\\n",
    "    .sort_values([\"iso2_code\", \"language\"])\n",
    "\n",
    "# join entries\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    ext[[\"iso2_code\", \"language\", \"entry01\", \"consider00\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# join complexity\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_075.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    cdf[[\"iso2_code\", \"language\", \"pci\", \"ubiquity\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# join RCA from the baseline period\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rca_tables[rca_tables[\"period\"]==3].loc[:,[\"iso2_code\", \"language\", \"rca01\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_df[\"rca01\"] = full_df[\"rca01\"].fillna(0)\n",
    "\n",
    "# drop languages with no complexity value\n",
    "full_df.dropna(subset=[\"pci\"], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# join to full_df with entries and PCI\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rel_dens,\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# export for entry models -- only consider 00, 01 patterns\n",
    "full_df[\"entry01\"] = full_df[\"entry01\"].astype(int)\n",
    "full_df.rename(columns={\"entry01\":\"exit01\"}, inplace=True)\n",
    "full_df[\"consider00\"] = full_df[\"consider00\"].astype(int)\n",
    "export_df = full_df[(full_df[\"exit01\"]==1) | (full_df[\"consider00\"]==1)]\n",
    "export_df.to_csv(\"../outputs/data_exit_regressions_1100_threshold_075.csv\", index=False, sep=\";\")\n",
    "#export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold05.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "### EXIT -- 1.25 threshold\n",
    "\n",
    "# relatedness density -- as in Hidalgo et al. (2007) Science\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_125.csv\", sep=\";\")\n",
    "rel_dens = cdf[cdf[\"year\"] == 2020][[\"iso2_code\", \"language\", \"density\"]].drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "selected_period = \"year\"\n",
    "\n",
    "# steps to prep dataframe of ecomplexity\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "\n",
    "# threshold for RCA : 1.25\n",
    "ps = [1, 2, 3, 4]\n",
    "rca_tables = []\n",
    "for p in ps:\n",
    "    print(p)\n",
    "    temp = bundle_data(df, periods=[p])\n",
    "    temp[\"period\"] = p\n",
    "    rca_tables.append(rca_calculation(temp, c_column=\"iso2_code\", p_column=\"language\", value_column=\"num_pushers\", threshold=1.25))\n",
    "\n",
    "#    dfbs.append(temp)\n",
    "rca_tables = pd.concat(rca_tables)\n",
    "\n",
    "\n",
    "\n",
    "# identify entry following the given patterns\n",
    "exit_pattern = [1,1,0,0]\n",
    "consider_pattern = [1,1,1,1]\n",
    "ext = rca_tables.sort_values([\"period\"], ascending=True).groupby([\"iso2_code\",\"language\"])[\"rca01\"].agg(list).reset_index()\n",
    "ext[\"entry01\"] = ext[\"rca01\"].apply(lambda x: x == exit_pattern).astype(int)\n",
    "ext[\"consider00\"] = ext[\"rca01\"].apply(lambda x: x == consider_pattern).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# full combination\n",
    "all_countries = ext[\"iso2_code\"].unique()\n",
    "all_languages = ext[\"language\"].unique()\n",
    "\n",
    "all_combinations = list(product(all_countries, all_languages))\n",
    "full_df = pd.DataFrame(all_combinations, columns=[\"iso2_code\", \"language\"])\\\n",
    "    .sort_values([\"iso2_code\", \"language\"])\n",
    "\n",
    "# join entries\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    ext[[\"iso2_code\", \"language\", \"entry01\", \"consider00\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# join complexity\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023_threshold_125.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    cdf[[\"iso2_code\", \"language\", \"pci\", \"ubiquity\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# join RCA from the baseline period\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rca_tables[rca_tables[\"period\"]==3].loc[:,[\"iso2_code\", \"language\", \"rca01\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_df[\"rca01\"] = full_df[\"rca01\"].fillna(0)\n",
    "\n",
    "# drop languages with no complexity value\n",
    "full_df.dropna(subset=[\"pci\"], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# join to full_df with entries and PCI\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rel_dens,\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# export for entry models -- only consider 00, 01 patterns\n",
    "full_df[\"entry01\"] = full_df[\"entry01\"].astype(int)\n",
    "full_df.rename(columns={\"entry01\":\"exit01\"}, inplace=True)\n",
    "full_df[\"consider00\"] = full_df[\"consider00\"].astype(int)\n",
    "export_df = full_df[(full_df[\"exit01\"]==1) | (full_df[\"consider00\"]==1)]\n",
    "export_df.to_csv(\"../outputs/data_exit_regressions_1100_threshold_125.csv\", index=False, sep=\";\")\n",
    "#export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold05.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**spatial autocorrelation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data -- ECI_software\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "cdf = cdf[[\"iso2_code\", \"eci\"]].drop_duplicates()\n",
    "\n",
    "# data -- world map\n",
    "cmap = gpd.read_file(\"../data/world-administrative-boundaries.geojson\")\n",
    "cmap = cmap[[\"iso3\", \"iso_3166_1_alpha_2_codes\", \"name\", \"geometry\"]].rename(columns={\"iso_3166_1_alpha_2_codes\" : \"iso2\"})\n",
    "\n",
    "cmap = pd.merge(\n",
    "    cmap,\n",
    "    cdf,\n",
    "    left_on=\"iso2\",\n",
    "    right_on=\"iso2_code\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# clean up\n",
    "cmap = cmap.drop_duplicates(subset=[\"iso2\"])\n",
    "#cmap.dropna(subset=\"iso2\", inplace=True)\n",
    "cmap.dropna(subset=\"eci\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_8256/214813710.py:7: FutureWarning: `idVariable` is deprecated and will be removed in future. Use `ids` instead.\n",
      "  w = weights.Queen.from_dataframe(cmap, idVariable=\"iso2\")\n",
      "/opt/homebrew/lib/python3.12/site-packages/libpysal/weights/contiguity.py:347: UserWarning: The weights matrix is not fully connected: \n",
      " There are 27 disconnected components.\n",
      " There are 22 islands with ids: CY, JM, LK, AU, PH, MG, KR, MU, JP, MT, SG, BH, BB, PR, SN, MV, IS, CU, RE, TW, NZ, TT.\n",
      "  W.__init__(self, neighbors, ids=ids, **kw)\n",
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_8256/214813710.py:13: FutureWarning: `idVariable` is deprecated and will be removed in future. Use `ids` instead.\n",
      "  w = weights.Queen.from_dataframe(cmap, idVariable=\"iso2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5849299169848287 Moran's I\n",
      "0.001 significance\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/libpysal/weights/contiguity.py:347: UserWarning: The weights matrix is not fully connected: \n",
      " There are 5 disconnected components.\n",
      "  W.__init__(self, neighbors, ids=ids, **kw)\n"
     ]
    }
   ],
   "source": [
    "### spatial autocorrelation\n",
    "\n",
    "# index setting\n",
    "cmap = cmap.set_index(\"iso2\", drop=False)\n",
    "\n",
    "# create the spatial weights matrix\n",
    "w = weights.Queen.from_dataframe(cmap, idVariable=\"iso2\")\n",
    "\n",
    "# drop island\n",
    "cmap = cmap.drop(w.islands)\n",
    "\n",
    "# create the spatial weights matrix\n",
    "w = weights.Queen.from_dataframe(cmap, idVariable=\"iso2\")\n",
    "\n",
    "# row standardize the matrix\n",
    "w.transform = \"R\"\n",
    "\n",
    "# spatial lag\n",
    "cmap[\"w_eci\"] = weights.lag_spatial(w, cmap[\"eci\"])\n",
    "\n",
    "# z score\n",
    "cmap[\"eci_std\"] = (cmap[\"eci\"] - cmap[\"eci\"].mean()) / cmap[\"eci\"].std()\n",
    "cmap[\"w_eci_std\"] = weights.lag_spatial(w, cmap[\"eci_std\"])\n",
    "\n",
    "# Moran I\n",
    "mi = esda.Moran(cmap[\"eci\"], w)\n",
    "print(mi.I, \"Moran's I\")\n",
    "print(mi.p_sim, \"significance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**topics -- instead of languages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61857, 5)\n",
      "(22328, 6)\n"
     ]
    }
   ],
   "source": [
    "# parameter to choose year / semester / quarter to construct period IDs\n",
    "selected_period = \"year\"\n",
    "\n",
    "# for ecomplexity calculcation\n",
    "key_cols = {\n",
    "    \"time\": \"period\",\n",
    "    \"loc\": \"iso2_code\",\n",
    "    \"prod\": \"topic\",\n",
    "    \"val\": \"num_pushers\",\n",
    "}\n",
    "\n",
    "# data IN\n",
    "data = pd.read_csv(\"../data/topics.csv\")\n",
    "\n",
    "# use data_prep_functions to clean the dataframe of ECI_software calculation\n",
    "print(data.shape)\n",
    "data = data[data[\"year\"].isin([2020, 2021, 2022, 2023])]\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter, key_column=\"topic\")\n",
    "df = top_languages_filter(df, nr_languages=150, key_column=\"topic\")\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"topic\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Percentage of pairs compared that meet log-supermodularity condition: 1.33%\n",
      "1\n",
      "2020  DONE\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.13/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 1: Log-supermodularity condition is not fully satisfied (1.33% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 0.21%\n",
      "1\n",
      "2021  DONE\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.13/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 1: Log-supermodularity condition is not fully satisfied (0.21% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 0.55%\n",
      "1\n",
      "2022  DONE\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.13/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 1: Log-supermodularity condition is not fully satisfied (0.55% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 0.41%\n",
      "1\n",
      "2023  DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.13/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 1: Log-supermodularity condition is not fully satisfied (0.41% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# software complexity calculation -- period IDs -- 1 means 2020 on yearly basis\n",
    "ccdf = []\n",
    "ppdf = []\n",
    "year_dict = {1 : 2020, 2 : 2021, 3 : 2022, 4 : 2023}\n",
    "for k in year_dict.keys():\n",
    "    dfb = bundle_data(df, periods=[k], key_column=\"topic\")\n",
    "    cdf = ecomplexity(dfb, key_cols)\n",
    "    cdf[\"year\"] = year_dict[k]\n",
    "\n",
    "    pdf = proximity(dfb, key_cols)\n",
    "    pdf[\"year\"] = year_dict[k]\n",
    "\n",
    "    # combine yearly dataframes\n",
    "    ccdf.append(cdf)\n",
    "    ppdf.append(pdf)\n",
    "    print(year_dict[k], \" DONE\")\n",
    "\n",
    "cdf = pd.concat(ccdf, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['distributed-systems', 'big-data', 'hpc', 'aws-lambda', 'robotics',\n",
       "       'science', 'streaming', 'config', 'containers', 'microsoft', 'gpu',\n",
       "       'azure', 'advent-of-code', 'tailwindcss', 'pandas', 'numpy',\n",
       "       'portfolio-website', 'jupyter-notebook', 'readme', 'simulation',\n",
       "       'cloud', 'terraform', 'spark', 'visualization', 'raspberry-pi',\n",
       "       'minecraft', 'r', 'ai', 'data-structures', 'symfony', 'jest',\n",
       "       'dotnet-core', 'open-source', 'flask', 'github',\n",
       "       'html-css-javascript', 'styled-components', 'mongoose',\n",
       "       'material-ui', 'tensorflow', 'discord-bot', 'graphql',\n",
       "       'automation', 'testing', 'portfolio', 'hactoberfest2020',\n",
       "       'competitive-programming', 'webdevelopment', 'hactoberfest',\n",
       "       'hactoberfest-accepted', 'dsa', 'monitoring', 'postgresql',\n",
       "       'security', 'node', 'sql', 'nextjs', 'react-native', 'macos',\n",
       "       'windows', 'bootstrap', 'database', 'git', 'data-science',\n",
       "       'express', 'rust', 'bot', 'ruby', 'pytorch', 'firebase',\n",
       "       'expressjs', 'discord', 'library', 'student', 'rocketseat', 'aws',\n",
       "       'documentation', 'c', 'game', 'css3', 'algorithms', 'ios', 'cli',\n",
       "       'angular', 'deep-learning', 'csharp', 'go', 'leetcode', 'blog',\n",
       "       'website', 'html5', 'dotfiles', 'cpp', 'mongodb', 'django',\n",
       "       'kubernetes', 'hacktoberfest-accepted', 'linux', 'api', 'golang',\n",
       "       'mysql', 'android', 'vue', 'machine-learning',\n",
       "       'binary-studio-academy', 'bsa20', 'homepage', 'gh-pages', 'php',\n",
       "       'dataworkshop', 'python3', 'docker', 'html', 'hacktoberfest2020',\n",
       "       'reactjs', 'css', 'typescript', 'nodejs', 'java', 'react',\n",
       "       'javascript', 'python', 'hacktoberfest'], dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf[cdf[\"year\"]==2020].sort_values(by=\"pci\", ascending=False)[\"topic\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['actions', 'accessibility', 'a11y', 'julia', 'gis', 'jupyter',\n",
       "       'circleci', 'jsx', 'jdbc', 'javascript-library', 'javafx',\n",
       "       'jamstack', 'cicd', 'gov', 'real-time', 'realtime', 'google-cloud',\n",
       "       'google-cloud-platform', 'google-maps-api', 'reactive',\n",
       "       'government', 'irc', 'gpu', 'ios-app', 'grafana', 'interpreter',\n",
       "       'graphics', 'interactive', 'integration', 'civic-tech',\n",
       "       'cloud-computing', 'geospatial', 'game-engine', 'frc-robot',\n",
       "       'labspt11', 'labs20', 'collaboration', 'fsharp', 'full-stack',\n",
       "       'python-library', 'qt', 'quantum-computing', 'r-package',\n",
       "       'functional-programming', 'kibana', 'kernel', 'code-for-america',\n",
       "       'cocoapods', 'genomics', 'radiuss', 'gamedev', 'games', 'gatsbyjs',\n",
       "       'gcp', 'cncf', 'clustering', 'cluster', 'cloudformation',\n",
       "       'cloud-native', 'infrastructure-as-code', 'clojure', 'climate',\n",
       "       'k8s', 'chrome', 'icons', 'reddit', 'chess', 'robotics',\n",
       "       'high-performance-computing', 'home-assistant', 'helm-charts',\n",
       "       'bots', 'bot-framework', 'ros', 'router', 'homebrew', 'healthcare',\n",
       "       'health', 'hpc', 'headless', 'haskell', 'hardware', 'handlebars',\n",
       "       'http-server', 'hadoop', 'i18n', 'ide', 'identity', 'gulp',\n",
       "       'influxdb', 'high-performance', 'frc-infinite-recharge',\n",
       "       'chemistry', 'charts', 'calendar', 'robot', 'rendering',\n",
       "       'repository', 'research', 'bukkit', 'build-tool', 'build',\n",
       "       'reverse-engineering', 'frc-java', 'decentralized', 'frc', 'docs',\n",
       "       'discord-py', 'distributed', 'distributed-computing',\n",
       "       'distributed-database', 'physics', 'distributed-systems', 'crypto',\n",
       "       'pipeline', 'npm-package', 'notifications', 'dns', 'nosql',\n",
       "       'non-profit', 'news', 'neuroscience', 'dynamodb',\n",
       "       'documentation-tool', 'neovim', 'dotnetcore', 'mxnet',\n",
       "       'multiplayer', 'create-react-app', 'plotly', 'cpp17', 'cpp11',\n",
       "       'pokemon', 'cplusplus', 'powershell', 'productivity', 'discord-js',\n",
       "       'perl', 'parsing', 'parallel-computing', 'operator', 'openshift',\n",
       "       'oracle', 'orchestration', 'datadog', 'openapi', 'open-data',\n",
       "       'onnx', 'ocaml', 'observability', 'design-systems', 'desktop',\n",
       "       'osx', 'developer-tools', 'p2p', 'package', 'development',\n",
       "       'data-mining', 'package-manager', 'dashboards', 'paper',\n",
       "       'objective-c', 'd3js', 'd3', 'cybersecurity', 'oauth2',\n",
       "       'curriculum', 'cuda', 'parallel', 'mpi', 'monorepo', 'labspt7',\n",
       "       'components', 'protobuf', 'mapbox', 'ffmpeg', 'fhir', 'filesystem',\n",
       "       'finance', 'finite-elements', 'map', 'management', 'firefox',\n",
       "       'mac', 'firmware', 'computer-science', 'compression',\n",
       "       'first-robotics-competition', 'modeling', 'logger', 'llvm',\n",
       "       'component-library', 'list', 'linter', 'protocol', 'leaflet',\n",
       "       'lambda', 'pubsub', 'fortran', 'openstreetmap', 'puppet',\n",
       "       'community', 'command-line-tool', 'concurrency', 'config',\n",
       "       'mapping', 'maps', 'mobile-app', 'minecraft-mod', 'midi', 'elixir',\n",
       "       'emacs', 'middleware', 'microsoft', 'ember', 'emulator',\n",
       "       'metadata', 'engineering', 'messaging', 'media', 'matlab', 'etl',\n",
       "       'events', 'example', 'core', 'continuous-integration',\n",
       "       'continuous-delivery', 'examples', 'containers', 'excel',\n",
       "       'mathematics', 'container', 'console', 'configuration-management',\n",
       "       'programming-language', 'math', 'fpga', 'big-data', 'script',\n",
       "       'scraper', 'scientific-computing', 'science', 'school', 'schema',\n",
       "       'biology', 'bitcoin', 'samples', 'salesforce', 's3', 'rust-lang',\n",
       "       'book', 'runtime', 'ruby-on-rails', 'rstats', 'scripts',\n",
       "       'best-practices', 'automl', 'bert', 'slack', 'aws-lambda',\n",
       "       'simulator', 'aws-s3', 'simple', 'azure-functions', 'service',\n",
       "       'azure-sdk', 'babel', 'serialization', 'bash-script', 'bashrc',\n",
       "       'security-tools', 'bazel', 'secrets', 'search-engine', 'benchmark',\n",
       "       'rpg', 'smart-contracts', 'autograd', 'websockets', 'tls', 'wiki',\n",
       "       'time-series', 'threejs', 'wordpress-theme', 'testing-tools',\n",
       "       'workshop', 'test-automation', 'test', 'terraform-provider', 'wpf',\n",
       "       'wrapper', 'xamarin', 'xcode', 'tensor', 'yarn', 'zshrc', 'tmux',\n",
       "       'tracing', 'software-engineering', 'trading', 'uptime-monitor',\n",
       "       'upptime', 'unix', 'unity3d', 'web-app', 'unit-testing',\n",
       "       'ui-components', 'web-components', 'webassembly', 'webgl',\n",
       "       'twitter-api', 'twitch', 'twilio', 'tutorials', 'webrtc',\n",
       "       'travis-ci', 'translation', 'tableau', 'swiftui', 'svelte',\n",
       "       'amazon', 'sql-server', 'apollo', 'application', 'architecture',\n",
       "       'arm', 'art', 'asp-net-core', 'aspnetcore', 'spotify', 'assembly',\n",
       "       'astronomy', 'speech-recognition', 'async', 'space', 'asyncio',\n",
       "       'solr', 'solidity', 'sqlalchemy', 'api-wrapper', 'api-gateway',\n",
       "       'stocks', 'analytics', 'stripe', 'streaming', 'stream-processing',\n",
       "       'stream', 'storybook', 'storage', 'stock-market', 'ssg',\n",
       "       'status-page', 'static-site-generator', 'static-site',\n",
       "       'static-analysis', 'static', 'ssl', 'apache', 'vscode-extension',\n",
       "       'vr', 'vpn', 'visualizations', 'visual-studio-code',\n",
       "       'visual-studio', 'vimrc', 'verilog', 'vulkan', 'vault', 'wasm',\n",
       "       'weather', 'vercel', 'validation', 'uwp', 'vagrant', 'utilities',\n",
       "       'addon', '3d-printing', 'google', 'sentiment-analysis',\n",
       "       'coronavirus-tracking', 'java-8', 'selenium', 'web-application',\n",
       "       'ml', 'mern-stack', 'random-forest', 'nlp-machine-learning',\n",
       "       'webscraping', 'angularjs', 'express-js', 'image-classification',\n",
       "       'scikit-learn', 'reinforcement-learning', 'hackathon',\n",
       "       'classification', 'code', 'resources',\n",
       "       'machine-learning-algorithms', 'jupyter-notebook', 'netlify',\n",
       "       'flask-application', 'calculator', 'react-redux',\n",
       "       'portfolio-website', 'github-api', 'mysql-database', 'keras',\n",
       "       'wordpress-plugin', 'firebase-database', 'restful-api', 'html-css',\n",
       "       'convolutional-neural-networks', 'profile', 'web-development',\n",
       "       'ajax', 'programming', 'react-components', 'heroku-deployment',\n",
       "       'boilerplate', 'chatbot', 'chat', 'twitter', 'dataset',\n",
       "       'neural-networks', 'numpy', 'readme', 'object-detection',\n",
       "       'matplotlib', 'animation', 'ecommerce', 'material-design',\n",
       "       'python-3', 'data-visualization', 'django-rest-framework',\n",
       "       'data-analysis', 'deep-neural-networks', 'pandas', 'cryptography',\n",
       "       'socket-io', 'regression', 'covid19-data', 'covid19', 'covid',\n",
       "       'tailwindcss', '3d', 'ubuntu', 'utility', 'terminal',\n",
       "       'cross-platform', 'serverless', 'generator', 'extension',\n",
       "       'api-client', 'deployment', 'dockerfile', 'search', 'debian',\n",
       "       'analysis', 'workflow', 'esp32', 'ssh', 'prometheus', 'spigot',\n",
       "       'encryption', 'privacy', 'awesome-list', 'csv', 'embedded',\n",
       "       'email', 'cryptocurrency', 'backup', 'command-line',\n",
       "       'configuration', 'client', 'ci', 'optimization', 'networking',\n",
       "       'opengl', 'cmake', 'performance', 'pdf', 'mqtt', 'mod', 'jenkins',\n",
       "       'language', 'latex', 'logging', 'metrics', 'hugo',\n",
       "       'home-automation', 'helm', 'webapp', 'microservices', 'tutorial',\n",
       "       'app', 'image-processing', 'graph', 'youtube', 'gui',\n",
       "       'natural-language-processing', 'pwa', 'design', 'authentication',\n",
       "       'opensource', 'dashboard', 'education', 'linked-list',\n",
       "       'logistic-regression', 'firestore', 'queue', 'cnn', 'stack',\n",
       "       'tensorflow2', 'beginner-friendly', 'jwt-authentication', 'pygame',\n",
       "       'algorithms-and-data-structures', 'keras-tensorflow',\n",
       "       'android-app', 'firebase-realtime-database', 'android-application',\n",
       "       'linear-regression', 'beginner', 'kotlin-android',\n",
       "       'django-application', 'deeplearning', 'sorting-algorithms',\n",
       "       'face-detection', 'face-recognition', 'sklearn', 'angular8',\n",
       "       'lstm', 'django-framework', 'beautifulsoup4',\n",
       "       'android-development', 'hackerrank', 'hacktoberfest-2020',\n",
       "       'hacktober', 'webdevelopment', 'hackerrank-solutions',\n",
       "       'hactoberfest', 'coursera', 'tkinter', 'tkinter-gui',\n",
       "       'mvvm-architecture', 'flutter-ui', 'hactoberfest-accepted',\n",
       "       'projects', 'hactoberfest2020', 'problem-solving',\n",
       "       'coronavirus-real-time', 'flutter-examples', 'flutter-apps',\n",
       "       'covid-19-india', 'flutter-app', 'passportjs',\n",
       "       'selenium-webdriver', 'seaborn', 'good-first-issue', 'redux-thunk',\n",
       "       'datastructures', 'datascience', 'retrofit2', 'opencv', 'whatsapp',\n",
       "       'opencv-python', 'ejs', 'competitive-programming',\n",
       "       'dynamic-programming', 'interview-questions', 'dsa',\n",
       "       'chat-application', 'coding', 'codeforces', 'codechef',\n",
       "       'android-studio', 'india', 'instagram', 'interview-preparation',\n",
       "       'chrome-extension', 'neural-network', 'github-pages', 'es6', 'ai',\n",
       "       'websocket', 'component', 'demo', 'cache', 'spark', 'orm', 'tools',\n",
       "       'swagger', 'grpc', 'proxy', 'canvas', 'network', 'image', 'sqlite',\n",
       "       'node-js', 'react-hooks', 'heroku', 'learning', 'hooks',\n",
       "       'react-router', 'devops', 'artificial-intelligence', 'sequelize',\n",
       "       'mvc', 'dotnet-core', 'php7', 'jest', 'design-system', 'iot',\n",
       "       'kafka', 'nginx', 'microservice', 'awesome', 'editor', 'tool',\n",
       "       'video', 'theme', 'infrastructure', 'statistics', 'ethereum',\n",
       "       'rest', 'postgres', 'eslint', 'jquery', 'advent-of-code', 'azure',\n",
       "       'audio', 'scala', 'data', 'advent-of-code-2020', 'gradle',\n",
       "       'bioinformatics', 'vscode', 'c-sharp', 'mobile', 'docker-image',\n",
       "       'simulation', 'nlp', 'github-actions', 'notes',\n",
       "       'leetcode-solutions', 'data-structures', 'sass', 'maven',\n",
       "       'arduino', 'npm', 'visualization', 'browser', 'cms', 'smarthome',\n",
       "       'typo3', 'sample', 'esp8266', 'wirvsvirushack', 'wirvsvirus',\n",
       "       'nextcloud', 'game-development', 'backend', 'zsh', 'scss',\n",
       "       'styled-components', 'project', 'html-css-javascript', 'mongoose',\n",
       "       'material-ui', 'sqlite3', 'personal-website', 'computer-vision',\n",
       "       'lua', 'open-source', 'tensorflow', 'ui', 'cloud', 'bash',\n",
       "       'terraform', 'music', 'ansible', 'dart', 'flutter', 'electron',\n",
       "       'axios', 'jwt', 'jekyll', 'http', 'raspberry-pi', 'minecraft',\n",
       "       'portfolio', 'wordpress', 'dotnet', 'compiler', 'sdk',\n",
       "       'coronavirus', 'unity', 'parser', 'shell', 'automation', 'gatsby',\n",
       "       'rails', 'redis', 'elasticsearch', 'wechat', 'vue-router', 'admin',\n",
       "       'webpack4', 'antd', 'vuex', 'crawler', 'gin', 'rpc', 'vue3',\n",
       "       'ant-design', 'interview', 'vuepress', 'vue-cli',\n",
       "       'wechat-mini-program', 'chinese', 'rabbitmq', 'hexo-blog', 'hexo',\n",
       "       'koa2', 'springcloud', 'dubbo', 'element-ui', 'springboot2',\n",
       "       'paddlepaddle', 'springboot', 'socket', 'spider', 'spring-cloud',\n",
       "       'mybatis', 'mybatis-plus', 'netty', 'miniprogram', 'testing',\n",
       "       'github', 'r', 'discord-bot', 'graphql', 'frontend', 'monitoring',\n",
       "       'security', 'server', 'front-end', 'symfony', 'vim',\n",
       "       'data-science', 'algorithm', 'template', 'css3', 'algorithms',\n",
       "       'sql', 'flask', 'leetcode', 'macos', 'firebase-auth', 'bootstrap4',\n",
       "       'knex', 'tdd', 'brazil', 'students', 'typeorm', 'api-rest',\n",
       "       'brasil', 'expo', 'rocketseat', 'nlw-2', 'nlw', 'student',\n",
       "       'nextlevelweek', 'proffy', 'machinelearning', 'database',\n",
       "       'docker-compose', 'telegram', 'telegram-bot', 'c-plus-plus',\n",
       "       'framework', 'plugin', 'spring', 'rest-api',\n",
       "       'hacktoberfest-accepted', 'aws', 'node', 'web', 'ruby', 'discord',\n",
       "       'rust', 'webpack', 'expressjs', 'govuk', 'react-native',\n",
       "       'bootstrap', 'express', 'postgresql', 'nextjs', 'vuejs', 'git',\n",
       "       'js', 'documentation', 'windows', 'covid-19', 'spring-boot',\n",
       "       'firebase', 'html5', 'blockchain', 'bot', 'swift', 'deep-learning',\n",
       "       'laravel', 'redux', 'cpp', 'c', 'django', 'pytorch', 'library',\n",
       "       'game', 'mongodb', 'csharp', 'kubernetes', 'angular', 'ios',\n",
       "       'kotlin', 'blog', 'go', 'cli', 'vk', 'mysql', 'android',\n",
       "       'dotfiles', 'website', 'hacktoberfest2020', 'linux', 'japanese',\n",
       "       'api', 'vue', 'golang', 'machine-learning', 'python3', 'html',\n",
       "       'php', 'docker', 'css', 'study', 'korean', 'reactjs', 'matrix',\n",
       "       'dataworkshop', 'road-signs', 'coderscamp', 'dwmatrix',\n",
       "       'typescript', 'nodejs', 'bsa20', 'binary-studio-academy',\n",
       "       'homepage', 'gh-pages', 'personal', 'resume', 'java', 'react',\n",
       "       'hacktoberfest', 'python', 'javascript'], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf[cdf[\"year\"]==2020].sort_values(by=\"pci\", ascending=False)[\"topic\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"topic\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"topic\"].str.contains(\"gh\")][\"topic\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**clustered languages -- theoretical version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n"
     ]
    }
   ],
   "source": [
    "# parameter to choose year / semester / quarter to construct period IDs\n",
    "selected_period = \"year\"\n",
    "\n",
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "\n",
    "# use data_prep_functions to clean the dataframe of ECI_software calculation\n",
    "data = data[data[\"year\"].isin([2020, 2021, 2022, 2023])]\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Percentage of pairs compared that meet log-supermodularity condition: 9.78%\n",
      "1\n",
      "2020  DONE\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 1: Log-supermodularity condition is not fully satisfied (9.78% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 10.50%\n",
      "2\n",
      "2021  DONE\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 2: Log-supermodularity condition is not fully satisfied (10.50% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 11.00%\n",
      "3\n",
      "2022  DONE\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 3: Log-supermodularity condition is not fully satisfied (11.00% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 11.77%\n",
      "4\n",
      "2023  DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 4: Log-supermodularity condition is not fully satisfied (11.77% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# clusters of languages\n",
    "cl_df = pd.read_csv(\"../data/language_to_cluster_mapping.csv\")\\\n",
    "    .rename(columns={\"Language\":\"language\", \"Cluster\":\"cluster_id\", \"Cluster Name\":\"cluster_name\"})\n",
    "\n",
    "# combine\n",
    "cl_df = pd.merge(\n",
    "    df,\n",
    "    cl_df,\n",
    "    on=\"language\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# drop NAs... -- not so great\n",
    "cl_df.dropna(subset=[\"cluster_id\", \"cluster_name\"], inplace=True)\n",
    "\n",
    "# aggregate by clusters\n",
    "cl_df = cl_df.groupby([\"iso2_code\", \"period\", \"cluster_name\"])[\"num_pushers\"].agg(\"mean\").reset_index()\n",
    "\n",
    "# for ecomplexity calculcation\n",
    "key_cols = {\n",
    "    \"time\": \"period\",\n",
    "    \"loc\": \"iso2_code\",\n",
    "    \"prod\": \"cluster_name\",\n",
    "    \"val\": \"num_pushers\",\n",
    "}\n",
    "\n",
    "# software complexity calculation -- period IDs -- 1 means 2020 on yearly basis\n",
    "ccdf = []\n",
    "ppdf = []\n",
    "year_dict = {1 : 2020, 2 : 2021, 3 : 2022, 4 : 2023}\n",
    "for k in year_dict.keys():\n",
    "    dfb = cl_df[cl_df[\"period\"]==k]\n",
    "    cdf = ecomplexity(dfb, key_cols)\n",
    "    cdf[\"year\"] = year_dict[k]\n",
    "\n",
    "    pdf = proximity(dfb, key_cols)\n",
    "    pdf[\"year\"] = year_dict[k]\n",
    "\n",
    "    # combine yearly dataframes\n",
    "    ccdf.append(cdf)\n",
    "    ppdf.append(pdf)\n",
    "    print(year_dict[k], \" DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine and save -- complexity\n",
    "cluster_cdf = pd.concat(ccdf, axis=0, ignore_index=True)\n",
    "cluster_cdf.to_csv(\"../outputs/eci_clusters_2020_2023.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eci_software</th>\n",
       "      <th>eci_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eci_software</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eci_cluster</th>\n",
       "      <td>0.983028</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              eci_software  eci_cluster\n",
       "eci_software      1.000000     0.983028\n",
       "eci_cluster       0.983028     1.000000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparison\n",
    "cluster_cdf = pd.read_csv(\"../outputs/eci_clusters_2020_2023.csv\", sep=\";\")\n",
    "eci_software = pd.read_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\")\n",
    "\n",
    "cc_df = pd.merge(\n",
    "    eci_software[eci_software[\"year\"]==2020][[\"iso2_code\", \"eci\"]].drop_duplicates(),\n",
    "    cluster_cdf[cluster_cdf[\"year\"]==2020][[\"iso2_code\", \"eci\"]].drop_duplicates(),\n",
    "    on=[\"iso2_code\"],\n",
    "    how=\"left\",\n",
    "    suffixes=[\"_software\", \"_cluster\"]\n",
    ")\n",
    "\n",
    "cc_df[[\"eci_software\", \"eci_cluster\"]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 unique clusters\n",
      "150 unique clusters\n"
     ]
    }
   ],
   "source": [
    "print(cluster_cdf[\"cluster_name\"].nunique(), \"unique clusters\")\n",
    "print(eci_software[\"language\"].nunique(), \"unique clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "### ENTRY -- based on clusters\n",
    "\n",
    "# relatedness density -- as in Hidalgo et al. (2007) Science\n",
    "cdf = pd.read_csv(\"../outputs/eci_clusters_2020_2023.csv\", sep=\";\")\n",
    "rel_dens = cdf[cdf[\"year\"] == 2020][[\"iso2_code\", \"cluster_name\", \"density\"]].drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "selected_period = \"year\"\n",
    "\n",
    "# steps to prep dataframe of ecomplexity\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)\n",
    "\n",
    "# clusters of languages\n",
    "cl_df = pd.read_csv(\"../data/language_to_cluster_mapping.csv\")\\\n",
    "    .rename(columns={\"Language\":\"language\", \"Cluster\":\"cluster_id\", \"Cluster Name\":\"cluster_name\"})\n",
    "\n",
    "# combine\n",
    "df = pd.merge(\n",
    "    df,\n",
    "    cl_df,\n",
    "    on=\"language\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# drop NAs... -- not so great\n",
    "df.dropna(subset=[\"cluster_id\", \"cluster_name\"], inplace=True)\n",
    "\n",
    "# aggregate by clusters\n",
    "df = df.groupby([\"iso2_code\", \"period\", \"cluster_name\"])[\"num_pushers\"].agg(\"mean\").reset_index()\n",
    "\n",
    "\n",
    "def bundle_data_clusters(data, periods):\n",
    "    \"\"\"aggreagte data for period by taking the mean number active developers\"\"\"\n",
    "    data = (\n",
    "        data[data[\"period\"].isin(periods)]\n",
    "        .groupby([\"iso2_code\", \"cluster_name\"])[\"num_pushers\"]\n",
    "        .agg(\"mean\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    data[\"period\"] = 1\n",
    "    data[\"num_pushers\"] = data[\"num_pushers\"].astype(int)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# threshold for RCA : 1.00\n",
    "ps = [1, 2, 3, 4]\n",
    "rca_tables = []\n",
    "for p in ps:\n",
    "    print(p)\n",
    "    temp = bundle_data_clusters(df, periods=[p])\n",
    "    temp[\"period\"] = p\n",
    "    rca_tables.append(rca_calculation(temp, c_column=\"iso2_code\", p_column=\"cluster_name\", value_column=\"num_pushers\", threshold=1))\n",
    "\n",
    "#    dfbs.append(temp)\n",
    "rca_tables = pd.concat(rca_tables)\n",
    "\n",
    "\n",
    "# identify entry following the given patterns\n",
    "entry_pattern = [0,0,1,1]\n",
    "consider_pattern = [0,0,0,0]\n",
    "ent = rca_tables.sort_values([\"period\"], ascending=True).groupby([\"iso2_code\",\"cluster_name\"])[\"rca01\"].agg(list).reset_index()\n",
    "ent[\"entry01\"] = ent[\"rca01\"].apply(lambda x: x == entry_pattern).astype(int)\n",
    "ent[\"consider00\"] = ent[\"rca01\"].apply(lambda x: x == consider_pattern).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# full combination\n",
    "all_countries = ent[\"iso2_code\"].unique()\n",
    "all_languages = ent[\"cluster_name\"].unique()\n",
    "\n",
    "all_combinations = list(product(all_countries, all_languages))\n",
    "full_df = pd.DataFrame(all_combinations, columns=[\"iso2_code\", \"cluster_name\"])\\\n",
    "    .sort_values([\"iso2_code\", \"cluster_name\"])\n",
    "\n",
    "# join entries\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    ent[[\"iso2_code\", \"cluster_name\", \"entry01\", \"consider00\"]],\n",
    "    on=[\"iso2_code\", \"cluster_name\"],\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# join complexity\n",
    "cdf = pd.read_csv(\"../outputs/eci_clusters_2020_2023.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    cdf[[\"iso2_code\", \"cluster_name\", \"pci\", \"ubiquity\"]],\n",
    "    on=[\"iso2_code\", \"cluster_name\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# join RCA from the baseline period\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rca_tables[rca_tables[\"period\"]==3].loc[:,[\"iso2_code\", \"cluster_name\", \"rca01\"]],\n",
    "    on=[\"iso2_code\", \"cluster_name\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_df[\"rca01\"] = full_df[\"rca01\"].fillna(0)\n",
    "\n",
    "# drop languages with no complexity value\n",
    "full_df.dropna(subset=[\"pci\"], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# join to full_df with entries and PCI\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rel_dens,\n",
    "    on=[\"iso2_code\", \"cluster_name\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# export for entry models -- only consider 00, 01 patterns\n",
    "full_df[\"entry01\"] = full_df[\"entry01\"].astype(int)\n",
    "full_df[\"consider00\"] = full_df[\"consider00\"].astype(int)\n",
    "export_df = full_df[(full_df[\"entry01\"]==1) | (full_df[\"consider00\"]==1)]\n",
    "export_df.to_csv(\"../outputs/data_entry_regressions_0011_clusters.csv\", index=False, sep=\";\")\n",
    "#export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold05.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "### EXIT -- 1.00 threshold\n",
    "\n",
    "# relatedness density -- as in Hidalgo et al. (2007) Science\n",
    "cdf = pd.read_csv(\"../outputs/eci_clusters_2020_2023.csv\", sep=\";\")\n",
    "rel_dens = cdf[cdf[\"year\"] == 2020][[\"iso2_code\", \"cluster_name\", \"density\"]].drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "selected_period = \"year\"\n",
    "\n",
    "# steps to prep dataframe of ecomplexity\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)\n",
    "\n",
    "# clusters of languages\n",
    "cl_df = pd.read_csv(\"../data/language_to_cluster_mapping.csv\")\\\n",
    "    .rename(columns={\"Language\":\"language\", \"Cluster\":\"cluster_id\", \"Cluster Name\":\"cluster_name\"})\n",
    "\n",
    "# combine\n",
    "df = pd.merge(\n",
    "    df,\n",
    "    cl_df,\n",
    "    on=\"language\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# drop NAs... -- not so great\n",
    "df.dropna(subset=[\"cluster_id\", \"cluster_name\"], inplace=True)\n",
    "\n",
    "# aggregate by clusters\n",
    "df = df.groupby([\"iso2_code\", \"period\", \"cluster_name\"])[\"num_pushers\"].agg(\"mean\").reset_index()\n",
    "\n",
    "\n",
    "def bundle_data_clusters(data, periods):\n",
    "    \"\"\"aggreagte data for period by taking the mean number active developers\"\"\"\n",
    "    data = (\n",
    "        data[data[\"period\"].isin(periods)]\n",
    "        .groupby([\"iso2_code\", \"cluster_name\"])[\"num_pushers\"]\n",
    "        .agg(\"mean\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    data[\"period\"] = 1\n",
    "    data[\"num_pushers\"] = data[\"num_pushers\"].astype(int)\n",
    "    return data\n",
    "\n",
    "\n",
    "# threshold for RCA : 1.00\n",
    "ps = [1, 2, 3, 4]\n",
    "rca_tables = []\n",
    "for p in ps:\n",
    "    print(p)\n",
    "    temp = bundle_data_clusters(df, periods=[p])\n",
    "    temp[\"period\"] = p\n",
    "    rca_tables.append(rca_calculation(temp, c_column=\"iso2_code\", p_column=\"cluster_name\", value_column=\"num_pushers\", threshold=1))\n",
    "\n",
    "#    dfbs.append(temp)\n",
    "rca_tables = pd.concat(rca_tables)\n",
    "\n",
    "\n",
    "\n",
    "# identify entry following the given patterns\n",
    "exit_pattern = [1,1,0,0]\n",
    "consider_pattern = [1,1,1,1]\n",
    "ext = rca_tables.sort_values([\"period\"], ascending=True).groupby([\"iso2_code\",\"cluster_name\"])[\"rca01\"].agg(list).reset_index()\n",
    "ext[\"entry01\"] = ext[\"rca01\"].apply(lambda x: x == exit_pattern).astype(int)\n",
    "ext[\"consider00\"] = ext[\"rca01\"].apply(lambda x: x == consider_pattern).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# full combination\n",
    "all_countries = ext[\"iso2_code\"].unique()\n",
    "all_languages = ext[\"cluster_name\"].unique()\n",
    "\n",
    "all_combinations = list(product(all_countries, all_languages))\n",
    "full_df = pd.DataFrame(all_combinations, columns=[\"iso2_code\", \"cluster_name\"])\\\n",
    "    .sort_values([\"iso2_code\", \"cluster_name\"])\n",
    "\n",
    "# join entries\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    ext[[\"iso2_code\", \"cluster_name\", \"entry01\", \"consider00\"]],\n",
    "    on=[\"iso2_code\", \"cluster_name\"],\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# join complexity\n",
    "cdf = pd.read_csv(\"../outputs/eci_clusters_2020_2023.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    cdf[[\"iso2_code\", \"cluster_name\", \"pci\", \"ubiquity\"]],\n",
    "    on=[\"iso2_code\", \"cluster_name\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# join RCA from the baseline period\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rca_tables[rca_tables[\"period\"]==3].loc[:,[\"iso2_code\", \"cluster_name\", \"rca01\"]],\n",
    "    on=[\"iso2_code\", \"cluster_name\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_df[\"rca01\"] = full_df[\"rca01\"].fillna(0)\n",
    "\n",
    "# drop languages with no complexity value\n",
    "full_df.dropna(subset=[\"pci\"], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# join to full_df with entries and PCI\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rel_dens,\n",
    "    on=[\"iso2_code\", \"cluster_name\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# export for entry models -- only consider 00, 01 patterns\n",
    "full_df[\"entry01\"] = full_df[\"entry01\"].astype(int)\n",
    "full_df.rename(columns={\"entry01\":\"exit01\"}, inplace=True)\n",
    "full_df[\"consider00\"] = full_df[\"consider00\"].astype(int)\n",
    "export_df = full_df[(full_df[\"exit01\"]==1) | (full_df[\"consider00\"]==1)]\n",
    "export_df.to_csv(\"../outputs/data_exit_regressions_1100_clusters.csv\", index=False, sep=\";\")\n",
    "#export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold05.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**clustered languages -- co-occurrence version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n"
     ]
    }
   ],
   "source": [
    "# parameter to choose year / semester / quarter to construct period IDs\n",
    "selected_period = \"year\"\n",
    "\n",
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "\n",
    "# use data_prep_functions to clean the dataframe of ECI_software calculation\n",
    "data = data[data[\"year\"].isin([2020, 2021, 2022, 2023])]\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Percentage of pairs compared that meet log-supermodularity condition: 6.68%\n",
      "1\n",
      "2020  DONE\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 1: Log-supermodularity condition is not fully satisfied (6.68% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 7.22%\n",
      "2\n",
      "2021  DONE\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 2: Log-supermodularity condition is not fully satisfied (7.22% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 7.73%\n",
      "3\n",
      "2022  DONE\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 3: Log-supermodularity condition is not fully satisfied (7.73% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 8.72%\n",
      "4\n",
      "2023  DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 4: Log-supermodularity condition is not fully satisfied (8.72% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# clusters of languages\n",
    "cl_df = pd.read_csv(\"../data/language_clusters_gh_cos_hier_ward_d1.csv\")\\\n",
    "    .rename(columns={\"Language\":\"language\", \"Cluster\":\"cluster_id\"})\\\n",
    "    .iloc[:,1:]\n",
    "\n",
    "# combine\n",
    "cl_df = pd.merge(\n",
    "    df,\n",
    "    cl_df,\n",
    "    on=\"language\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# drop NAs... -- not so great\n",
    "cl_df.dropna(subset=[\"cluster_id\", \"cluster_id\"], inplace=True)\n",
    "\n",
    "# aggregate by clusters\n",
    "cl_df = cl_df.groupby([\"iso2_code\", \"period\", \"cluster_id\"])[\"num_pushers\"].agg(\"mean\").reset_index()\n",
    "\n",
    "# for ecomplexity calculcation\n",
    "key_cols = {\n",
    "    \"time\": \"period\",\n",
    "    \"loc\": \"iso2_code\",\n",
    "    \"prod\": \"cluster_id\",\n",
    "    \"val\": \"num_pushers\",\n",
    "}\n",
    "\n",
    "# software complexity calculation -- period IDs -- 1 means 2020 on yearly basis\n",
    "ccdf = []\n",
    "ppdf = []\n",
    "year_dict = {1 : 2020, 2 : 2021, 3 : 2022, 4 : 2023}\n",
    "for k in year_dict.keys():\n",
    "    dfb = cl_df[cl_df[\"period\"]==k]\n",
    "    cdf = ecomplexity(dfb, key_cols)\n",
    "    cdf[\"year\"] = year_dict[k]\n",
    "\n",
    "    pdf = proximity(dfb, key_cols)\n",
    "    pdf[\"year\"] = year_dict[k]\n",
    "\n",
    "    # combine yearly dataframes\n",
    "    ccdf.append(cdf)\n",
    "    ppdf.append(pdf)\n",
    "    print(year_dict[k], \" DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine and save -- complexity\n",
    "cluster_cdf = pd.concat(ccdf, axis=0, ignore_index=True)\n",
    "cluster_cdf.to_csv(\"../outputs/eci_clusters_cooc_2020_2023.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eci_software</th>\n",
       "      <th>eci_cluster_theory</th>\n",
       "      <th>eci_cluster_cooccurrence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eci_software</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983028</td>\n",
       "      <td>0.970453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eci_cluster_theory</th>\n",
       "      <td>0.983028</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.973888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eci_cluster_cooccurrence</th>\n",
       "      <td>0.970453</td>\n",
       "      <td>0.973888</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          eci_software  eci_cluster_theory  \\\n",
       "eci_software                  1.000000            0.983028   \n",
       "eci_cluster_theory            0.983028            1.000000   \n",
       "eci_cluster_cooccurrence      0.970453            0.973888   \n",
       "\n",
       "                          eci_cluster_cooccurrence  \n",
       "eci_software                              0.970453  \n",
       "eci_cluster_theory                        0.973888  \n",
       "eci_cluster_cooccurrence                  1.000000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparison\n",
    "cluster_cdf1 = pd.read_csv(\"../outputs/eci_clusters_2020_2023.csv\", sep=\";\")\n",
    "cluster_cdf = pd.read_csv(\"../outputs/eci_clusters_cooc_2020_2023.csv\", sep=\";\")\n",
    "eci_software = pd.read_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\")\n",
    "\n",
    "cc_df = pd.merge(\n",
    "    eci_software[eci_software[\"year\"]==2020][[\"iso2_code\", \"eci\"]].drop_duplicates(),\n",
    "    cluster_cdf[cluster_cdf[\"year\"]==2020][[\"iso2_code\", \"eci\"]].drop_duplicates(),\n",
    "    on=[\"iso2_code\"],\n",
    "    how=\"left\",\n",
    "    suffixes=[\"_software\", \"_cluster\"]\n",
    ")\n",
    "temp = pd.merge(\n",
    "    cc_df,\n",
    "    cluster_cdf1[cluster_cdf1[\"year\"]==2020][[\"iso2_code\", \"eci\"]].drop_duplicates(),\n",
    "    on=[\"iso2_code\"],\n",
    "    how=\"left\"\n",
    ").rename(columns={\"eci\":\"eci_cluster_theory\", \"eci_cluster\":\"eci_cluster_cooccurrence\"})\n",
    "\n",
    "temp[[\"eci_software\", \"eci_cluster_theory\", \"eci_cluster_cooccurrence\"]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 unique clusters\n",
      "150 unique clusters\n"
     ]
    }
   ],
   "source": [
    "print(cluster_cdf[\"cluster_id\"].nunique(), \"unique clusters\")\n",
    "print(eci_software[\"language\"].nunique(), \"unique clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "### ENTRY -- based on clusters\n",
    "\n",
    "# relatedness density -- as in Hidalgo et al. (2007) Science\n",
    "cdf = pd.read_csv(\"../outputs/eci_clusters_cooc_2020_2023.csv\", sep=\";\")\n",
    "rel_dens = cdf[cdf[\"year\"] == 2020][[\"iso2_code\", \"cluster_id\", \"density\"]].drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "selected_period = \"year\"\n",
    "\n",
    "# steps to prep dataframe of ecomplexity\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)\n",
    "\n",
    "# clusters of languages\n",
    "cl_df = pd.read_csv(\"../data/language_clusters_gh_cos_hier_ward_d1.csv\")\\\n",
    "    .rename(columns={\"Language\":\"language\", \"Cluster\":\"cluster_id\"})\\\n",
    "    .iloc[:,1:]\n",
    "\n",
    "# combine\n",
    "df = pd.merge(\n",
    "    df,\n",
    "    cl_df,\n",
    "    on=\"language\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# drop NAs... -- not so great\n",
    "df.dropna(subset=[\"cluster_id\"], inplace=True)\n",
    "\n",
    "# aggregate by clusters\n",
    "df = df.groupby([\"iso2_code\", \"period\", \"cluster_id\"])[\"num_pushers\"].agg(\"mean\").reset_index()\n",
    "\n",
    "\n",
    "def bundle_data_clusters(data, periods):\n",
    "    \"\"\"aggreagte data for period by taking the mean number active developers\"\"\"\n",
    "    data = (\n",
    "        data[data[\"period\"].isin(periods)]\n",
    "        .groupby([\"iso2_code\", \"cluster_id\"])[\"num_pushers\"]\n",
    "        .agg(\"mean\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    data[\"period\"] = 1\n",
    "    data[\"num_pushers\"] = data[\"num_pushers\"].astype(int)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# threshold for RCA : 1.00\n",
    "ps = [1, 2, 3, 4]\n",
    "rca_tables = []\n",
    "for p in ps:\n",
    "    print(p)\n",
    "    temp = bundle_data_clusters(df, periods=[p])\n",
    "    temp[\"period\"] = p\n",
    "    rca_tables.append(rca_calculation(temp, c_column=\"iso2_code\", p_column=\"cluster_id\", value_column=\"num_pushers\", threshold=1))\n",
    "\n",
    "#    dfbs.append(temp)\n",
    "rca_tables = pd.concat(rca_tables)\n",
    "\n",
    "\n",
    "# identify entry following the given patterns\n",
    "entry_pattern = [0,0,1,1]\n",
    "consider_pattern = [0,0,0,0]\n",
    "ent = rca_tables.sort_values([\"period\"], ascending=True).groupby([\"iso2_code\",\"cluster_id\"])[\"rca01\"].agg(list).reset_index()\n",
    "ent[\"entry01\"] = ent[\"rca01\"].apply(lambda x: x == entry_pattern).astype(int)\n",
    "ent[\"consider00\"] = ent[\"rca01\"].apply(lambda x: x == consider_pattern).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# full combination\n",
    "all_countries = ent[\"iso2_code\"].unique()\n",
    "all_languages = ent[\"cluster_id\"].unique()\n",
    "\n",
    "all_combinations = list(product(all_countries, all_languages))\n",
    "full_df = pd.DataFrame(all_combinations, columns=[\"iso2_code\", \"cluster_id\"])\\\n",
    "    .sort_values([\"iso2_code\", \"cluster_id\"])\n",
    "\n",
    "# join entries\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    ent[[\"iso2_code\", \"cluster_id\", \"entry01\", \"consider00\"]],\n",
    "    on=[\"iso2_code\", \"cluster_id\"],\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# join complexity\n",
    "cdf = pd.read_csv(\"../outputs/eci_clusters_cooc_2020_2023.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    cdf[[\"iso2_code\", \"cluster_id\", \"pci\", \"ubiquity\"]],\n",
    "    on=[\"iso2_code\", \"cluster_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# join RCA from the baseline period\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rca_tables[rca_tables[\"period\"]==3].loc[:,[\"iso2_code\", \"cluster_id\", \"rca01\"]],\n",
    "    on=[\"iso2_code\", \"cluster_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_df[\"rca01\"] = full_df[\"rca01\"].fillna(0)\n",
    "\n",
    "# drop languages with no complexity value\n",
    "full_df.dropna(subset=[\"pci\"], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# join to full_df with entries and PCI\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rel_dens,\n",
    "    on=[\"iso2_code\", \"cluster_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# export for entry models -- only consider 00, 01 patterns\n",
    "full_df[\"entry01\"] = full_df[\"entry01\"].astype(int)\n",
    "full_df[\"consider00\"] = full_df[\"consider00\"].astype(int)\n",
    "export_df = full_df[(full_df[\"entry01\"]==1) | (full_df[\"consider00\"]==1)]\n",
    "export_df.to_csv(\"../outputs/data_entry_regressions_0011_clusters_cooc.csv\", index=False, sep=\";\")\n",
    "#export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold05.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "### EXIT -- 1.00 threshold\n",
    "\n",
    "# relatedness density -- as in Hidalgo et al. (2007) Science\n",
    "cdf = pd.read_csv(\"../outputs/eci_clusters_cooc_2020_2023.csv\", sep=\";\")\n",
    "rel_dens = cdf[cdf[\"year\"] == 2020][[\"iso2_code\", \"cluster_id\", \"density\"]].drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "selected_period = \"year\"\n",
    "\n",
    "# steps to prep dataframe of ecomplexity\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)\n",
    "\n",
    "# clusters of languages\n",
    "cl_df = pd.read_csv(\"../data/language_clusters_gh_cos_hier_ward_d1.csv\")\\\n",
    "    .rename(columns={\"Language\":\"language\", \"Cluster\":\"cluster_id\"})\\\n",
    "    .iloc[:,1:]\n",
    "\n",
    "# combine\n",
    "df = pd.merge(\n",
    "    df,\n",
    "    cl_df,\n",
    "    on=\"language\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# drop NAs... -- not so great\n",
    "df.dropna(subset=[\"cluster_id\", \"cluster_id\"], inplace=True)\n",
    "\n",
    "# aggregate by clusters\n",
    "df = df.groupby([\"iso2_code\", \"period\", \"cluster_id\"])[\"num_pushers\"].agg(\"mean\").reset_index()\n",
    "\n",
    "\n",
    "def bundle_data_clusters(data, periods):\n",
    "    \"\"\"aggreagte data for period by taking the mean number active developers\"\"\"\n",
    "    data = (\n",
    "        data[data[\"period\"].isin(periods)]\n",
    "        .groupby([\"iso2_code\", \"cluster_id\"])[\"num_pushers\"]\n",
    "        .agg(\"mean\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    data[\"period\"] = 1\n",
    "    data[\"num_pushers\"] = data[\"num_pushers\"].astype(int)\n",
    "    return data\n",
    "\n",
    "\n",
    "# threshold for RCA : 1.00\n",
    "ps = [1, 2, 3, 4]\n",
    "rca_tables = []\n",
    "for p in ps:\n",
    "    print(p)\n",
    "    temp = bundle_data_clusters(df, periods=[p])\n",
    "    temp[\"period\"] = p\n",
    "    rca_tables.append(rca_calculation(temp, c_column=\"iso2_code\", p_column=\"cluster_id\", value_column=\"num_pushers\", threshold=1))\n",
    "\n",
    "#    dfbs.append(temp)\n",
    "rca_tables = pd.concat(rca_tables)\n",
    "\n",
    "\n",
    "\n",
    "# identify entry following the given patterns\n",
    "exit_pattern = [1,1,0,0]\n",
    "consider_pattern = [1,1,1,1]\n",
    "ext = rca_tables.sort_values([\"period\"], ascending=True).groupby([\"iso2_code\",\"cluster_id\"])[\"rca01\"].agg(list).reset_index()\n",
    "ext[\"entry01\"] = ext[\"rca01\"].apply(lambda x: x == exit_pattern).astype(int)\n",
    "ext[\"consider00\"] = ext[\"rca01\"].apply(lambda x: x == consider_pattern).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# full combination\n",
    "all_countries = ext[\"iso2_code\"].unique()\n",
    "all_languages = ext[\"cluster_id\"].unique()\n",
    "\n",
    "all_combinations = list(product(all_countries, all_languages))\n",
    "full_df = pd.DataFrame(all_combinations, columns=[\"iso2_code\", \"cluster_id\"])\\\n",
    "    .sort_values([\"iso2_code\", \"cluster_id\"])\n",
    "\n",
    "# join entries\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    ext[[\"iso2_code\", \"cluster_id\", \"entry01\", \"consider00\"]],\n",
    "    on=[\"iso2_code\", \"cluster_id\"],\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# join complexity\n",
    "cdf = pd.read_csv(\"../outputs/eci_clusters_cooc_2020_2023.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    cdf[[\"iso2_code\", \"cluster_id\", \"pci\", \"ubiquity\"]],\n",
    "    on=[\"iso2_code\", \"cluster_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# join RCA from the baseline period\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rca_tables[rca_tables[\"period\"]==3].loc[:,[\"iso2_code\", \"cluster_id\", \"rca01\"]],\n",
    "    on=[\"iso2_code\", \"cluster_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_df[\"rca01\"] = full_df[\"rca01\"].fillna(0)\n",
    "\n",
    "# drop languages with no complexity value\n",
    "full_df.dropna(subset=[\"pci\"], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# join to full_df with entries and PCI\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rel_dens,\n",
    "    on=[\"iso2_code\", \"cluster_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# export for entry models -- only consider 00, 01 patterns\n",
    "full_df[\"entry01\"] = full_df[\"entry01\"].astype(int)\n",
    "full_df.rename(columns={\"entry01\":\"exit01\"}, inplace=True)\n",
    "full_df[\"consider00\"] = full_df[\"consider00\"].astype(int)\n",
    "export_df = full_df[(full_df[\"exit01\"]==1) | (full_df[\"consider00\"]==1)]\n",
    "export_df.to_csv(\"../outputs/data_exit_regressions_1100_clusters_cooc.csv\", index=False, sep=\";\")\n",
    "#export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold05.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 1 note -- RCA R2s for 2021**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../outputs/eci_regression_table.csv\", sep=\";\")\n",
    "df = df[df[\"year\"]==2021]\n",
    "df = df[[\"iso2_code\", \"eci_software\", \"eci_trade\", \"eci_tech\", \"eci_research\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_column_to_range(table, column_name):\n",
    "    min_val = table[column_name].min()\n",
    "    max_val = table[column_name].max()\n",
    "    table[column_name] = 2 * (table[column_name] - min_val) / (max_val - min_val) - 1\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = normalize_column_to_range(df, \"eci_software\")\n",
    "df = normalize_column_to_range(df, \"eci_trade\")\n",
    "df = normalize_column_to_range(df, \"eci_tech\")\n",
    "df = normalize_column_to_range(df, \"eci_research\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eci_correlations(df, key_variables):\n",
    "    df.dropna(subset=key_variables, inplace=True)\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(df[key_variables[0]], df[key_variables[1]])\n",
    "    r_squared = r_value ** 2\n",
    "    print(\"R2\", round(r_squared, 3), \"p-value\", round(p_value, 3), \"   \", key_variables[0], \"  \", key_variables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 0.665 p-value 0.0     eci_software    eci_trade\n"
     ]
    }
   ],
   "source": [
    "eci_correlations(df, key_variables=[\"eci_software\", \"eci_trade\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 0.7 p-value 0.0     eci_software    eci_tech\n"
     ]
    }
   ],
   "source": [
    "eci_correlations(df, key_variables=[\"eci_software\", \"eci_tech\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 0.488 p-value 0.0     eci_software    eci_research\n"
     ]
    }
   ],
   "source": [
    "eci_correlations(df, key_variables=[\"eci_software\", \"eci_research\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "6ca8f35c51bc4625b88aba368e0e3e6b52bd4f043902a68ceae684135a43d0f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
