{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from itertools import product\n",
    "from ecomplexity import ecomplexity\n",
    "from ecomplexity import proximity\n",
    "from ecomplexity import calc_density\n",
    "import country_converter as coco\n",
    "import itertools\n",
    "\n",
    "# stats\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import linregress\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**clustered languages -- co-occurrence version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n"
     ]
    }
   ],
   "source": [
    "# parameter to choose year / semester / quarter to construct period IDs\n",
    "selected_period = \"year\"\n",
    "\n",
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "\n",
    "# use data_prep_functions to clean the dataframe of ECI_software calculation\n",
    "data = data[data[\"year\"].isin([2020, 2021, 2022, 2023])]\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Percentage of pairs compared that meet log-supermodularity condition: 6.68%\n",
      "1\n",
      "2020  DONE\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.13/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 1: Log-supermodularity condition is not fully satisfied (6.68% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 7.22%\n",
      "2\n",
      "2021  DONE\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.13/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 2: Log-supermodularity condition is not fully satisfied (7.22% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 7.73%\n",
      "3\n",
      "2022  DONE\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.13/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 3: Log-supermodularity condition is not fully satisfied (7.73% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 8.72%\n",
      "4\n",
      "2023  DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.13/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 4: Log-supermodularity condition is not fully satisfied (8.72% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# clusters of languages\n",
    "cl_df = pd.read_csv(\"../data/language_clusters_gh_cos_hier_ward_d1.csv\")\\\n",
    "    .rename(columns={\"Language\":\"language\", \"Cluster\":\"cluster_id\"})\\\n",
    "    .iloc[:,1:]\n",
    "\n",
    "# combine\n",
    "cl_df = pd.merge(\n",
    "    df,\n",
    "    cl_df,\n",
    "    on=\"language\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# drop NAs... -- not so great\n",
    "cl_df.dropna(subset=[\"cluster_id\", \"cluster_id\"], inplace=True)\n",
    "\n",
    "# aggregate by clusters\n",
    "cl_df = cl_df.groupby([\"iso2_code\", \"period\", \"cluster_id\"])[\"num_pushers\"].agg(\"mean\").reset_index()\n",
    "\n",
    "# for ecomplexity calculcation\n",
    "key_cols = {\n",
    "    \"time\": \"period\",\n",
    "    \"loc\": \"iso2_code\",\n",
    "    \"prod\": \"cluster_id\",\n",
    "    \"val\": \"num_pushers\",\n",
    "}\n",
    "\n",
    "# software complexity calculation -- period IDs -- 1 means 2020 on yearly basis\n",
    "ccdf = []\n",
    "ppdf = []\n",
    "year_dict = {1 : 2020, 2 : 2021, 3 : 2022, 4 : 2023}\n",
    "for k in year_dict.keys():\n",
    "    dfb = cl_df[cl_df[\"period\"]==k]\n",
    "    cdf = ecomplexity(dfb, key_cols)\n",
    "    cdf[\"year\"] = year_dict[k]\n",
    "\n",
    "    pdf = proximity(dfb, key_cols)\n",
    "    pdf[\"year\"] = year_dict[k]\n",
    "\n",
    "    # combine yearly dataframes\n",
    "    ccdf.append(cdf)\n",
    "    ppdf.append(pdf)\n",
    "    print(year_dict[k], \" DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine and save -- complexity\n",
    "cluster_cdf = pd.concat(ccdf, axis=0, ignore_index=True)\n",
    "cluster_cdf.to_csv(\"../outputs/eci_clusters_cooc_2020_2023.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine and save -- language-cluster proximity\n",
    "prox_df = pd.concat(ppdf, axis=0, ignore_index=True)\n",
    "prox_df.to_csv(\"../outputs/cluster_proximity_2020_2023.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eci_software</th>\n",
       "      <th>eci_cluster_theory</th>\n",
       "      <th>eci_cluster_cooccurrence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>eci_software</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983028</td>\n",
       "      <td>0.970453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eci_cluster_theory</th>\n",
       "      <td>0.983028</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.973888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eci_cluster_cooccurrence</th>\n",
       "      <td>0.970453</td>\n",
       "      <td>0.973888</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          eci_software  eci_cluster_theory  \\\n",
       "eci_software                  1.000000            0.983028   \n",
       "eci_cluster_theory            0.983028            1.000000   \n",
       "eci_cluster_cooccurrence      0.970453            0.973888   \n",
       "\n",
       "                          eci_cluster_cooccurrence  \n",
       "eci_software                              0.970453  \n",
       "eci_cluster_theory                        0.973888  \n",
       "eci_cluster_cooccurrence                  1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparison\n",
    "cluster_cdf1 = pd.read_csv(\"../outputs/eci_clusters_2020_2023.csv\", sep=\";\")\n",
    "cluster_cdf = pd.read_csv(\"../outputs/eci_clusters_cooc_2020_2023.csv\", sep=\";\")\n",
    "eci_software = pd.read_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\")\n",
    "\n",
    "cc_df = pd.merge(\n",
    "    eci_software[eci_software[\"year\"]==2020][[\"iso2_code\", \"eci\"]].drop_duplicates(),\n",
    "    cluster_cdf[cluster_cdf[\"year\"]==2020][[\"iso2_code\", \"eci\"]].drop_duplicates(),\n",
    "    on=[\"iso2_code\"],\n",
    "    how=\"left\",\n",
    "    suffixes=[\"_software\", \"_cluster\"]\n",
    ")\n",
    "temp = pd.merge(\n",
    "    cc_df,\n",
    "    cluster_cdf1[cluster_cdf1[\"year\"]==2020][[\"iso2_code\", \"eci\"]].drop_duplicates(),\n",
    "    on=[\"iso2_code\"],\n",
    "    how=\"left\"\n",
    ").rename(columns={\"eci\":\"eci_cluster_theory\", \"eci_cluster\":\"eci_cluster_cooccurrence\"})\n",
    "\n",
    "temp[[\"eci_software\", \"eci_cluster_theory\", \"eci_cluster_cooccurrence\"]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 unique clusters\n",
      "150 unique clusters\n"
     ]
    }
   ],
   "source": [
    "print(cluster_cdf[\"cluster_id\"].nunique(), \"unique clusters\")\n",
    "print(eci_software[\"language\"].nunique(), \"unique clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "### ENTRY -- based on clusters\n",
    "\n",
    "# relatedness density -- as in Hidalgo et al. (2007) Science\n",
    "cdf = pd.read_csv(\"../outputs/eci_clusters_cooc_2020_2023.csv\", sep=\";\")\n",
    "rel_dens = cdf[cdf[\"year\"] == 2020][[\"iso2_code\", \"cluster_id\", \"density\"]].drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "selected_period = \"year\"\n",
    "\n",
    "# steps to prep dataframe of ecomplexity\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)\n",
    "\n",
    "# clusters of languages\n",
    "cl_df = pd.read_csv(\"../data/language_clusters_gh_cos_hier_ward_d1.csv\")\\\n",
    "    .rename(columns={\"Language\":\"language\", \"Cluster\":\"cluster_id\"})\\\n",
    "    .iloc[:,1:]\n",
    "\n",
    "# combine\n",
    "df = pd.merge(\n",
    "    df,\n",
    "    cl_df,\n",
    "    on=\"language\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# drop NAs... -- not so great\n",
    "df.dropna(subset=[\"cluster_id\"], inplace=True)\n",
    "\n",
    "# aggregate by clusters\n",
    "df = df.groupby([\"iso2_code\", \"period\", \"cluster_id\"])[\"num_pushers\"].agg(\"mean\").reset_index()\n",
    "\n",
    "\n",
    "def bundle_data_clusters(data, periods):\n",
    "    \"\"\"aggreagte data for period by taking the mean number active developers\"\"\"\n",
    "    data = (\n",
    "        data[data[\"period\"].isin(periods)]\n",
    "        .groupby([\"iso2_code\", \"cluster_id\"])[\"num_pushers\"]\n",
    "        .agg(\"mean\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    data[\"period\"] = 1\n",
    "    data[\"num_pushers\"] = data[\"num_pushers\"].astype(int)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "# threshold for RCA : 1.00\n",
    "ps = [1, 2, 3, 4]\n",
    "rca_tables = []\n",
    "for p in ps:\n",
    "    print(p)\n",
    "    temp = bundle_data_clusters(df, periods=[p])\n",
    "    temp[\"period\"] = p\n",
    "    rca_tables.append(rca_calculation(temp, c_column=\"iso2_code\", p_column=\"cluster_id\", value_column=\"num_pushers\", threshold=1))\n",
    "\n",
    "#    dfbs.append(temp)\n",
    "rca_tables = pd.concat(rca_tables)\n",
    "\n",
    "\n",
    "# identify entry following the given patterns\n",
    "entry_pattern = [0,0,1,1]\n",
    "consider_pattern = [0,0,0,0]\n",
    "ent = rca_tables.sort_values([\"period\"], ascending=True).groupby([\"iso2_code\",\"cluster_id\"])[\"rca01\"].agg(list).reset_index()\n",
    "ent[\"entry01\"] = ent[\"rca01\"].apply(lambda x: x == entry_pattern).astype(int)\n",
    "ent[\"consider00\"] = ent[\"rca01\"].apply(lambda x: x == consider_pattern).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# full combination\n",
    "all_countries = ent[\"iso2_code\"].unique()\n",
    "all_languages = ent[\"cluster_id\"].unique()\n",
    "\n",
    "all_combinations = list(product(all_countries, all_languages))\n",
    "full_df = pd.DataFrame(all_combinations, columns=[\"iso2_code\", \"cluster_id\"])\\\n",
    "    .sort_values([\"iso2_code\", \"cluster_id\"])\n",
    "\n",
    "# join entries\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    ent[[\"iso2_code\", \"cluster_id\", \"entry01\", \"consider00\"]],\n",
    "    on=[\"iso2_code\", \"cluster_id\"],\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# join complexity\n",
    "cdf = pd.read_csv(\"../outputs/eci_clusters_cooc_2020_2023.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    cdf[[\"iso2_code\", \"cluster_id\", \"pci\", \"ubiquity\"]],\n",
    "    on=[\"iso2_code\", \"cluster_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# join RCA from the baseline period\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rca_tables[rca_tables[\"period\"]==3].loc[:,[\"iso2_code\", \"cluster_id\", \"rca01\"]],\n",
    "    on=[\"iso2_code\", \"cluster_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_df[\"rca01\"] = full_df[\"rca01\"].fillna(0)\n",
    "\n",
    "# drop languages with no complexity value\n",
    "full_df.dropna(subset=[\"pci\"], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# join to full_df with entries and PCI\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rel_dens,\n",
    "    on=[\"iso2_code\", \"cluster_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# export for entry models -- only consider 00, 01 patterns\n",
    "full_df[\"entry01\"] = full_df[\"entry01\"].astype(int)\n",
    "full_df[\"consider00\"] = full_df[\"consider00\"].astype(int)\n",
    "export_df = full_df[(full_df[\"entry01\"]==1) | (full_df[\"consider00\"]==1)]\n",
    "export_df.to_csv(\"../outputs/data_entry_regressions_0011_clusters_cooc.csv\", index=False, sep=\";\")\n",
    "#export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold05.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "### EXIT -- 1.00 threshold\n",
    "\n",
    "# relatedness density -- as in Hidalgo et al. (2007) Science\n",
    "cdf = pd.read_csv(\"../outputs/eci_clusters_cooc_2020_2023.csv\", sep=\";\")\n",
    "rel_dens = cdf[cdf[\"year\"] == 2020][[\"iso2_code\", \"cluster_id\", \"density\"]].drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "selected_period = \"year\"\n",
    "\n",
    "# steps to prep dataframe of ecomplexity\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)\n",
    "\n",
    "# clusters of languages\n",
    "cl_df = pd.read_csv(\"../data/language_clusters_gh_cos_hier_ward_d1.csv\")\\\n",
    "    .rename(columns={\"Language\":\"language\", \"Cluster\":\"cluster_id\"})\\\n",
    "    .iloc[:,1:]\n",
    "\n",
    "# combine\n",
    "df = pd.merge(\n",
    "    df,\n",
    "    cl_df,\n",
    "    on=\"language\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# drop NAs... -- not so great\n",
    "df.dropna(subset=[\"cluster_id\", \"cluster_id\"], inplace=True)\n",
    "\n",
    "# aggregate by clusters\n",
    "df = df.groupby([\"iso2_code\", \"period\", \"cluster_id\"])[\"num_pushers\"].agg(\"mean\").reset_index()\n",
    "\n",
    "\n",
    "def bundle_data_clusters(data, periods):\n",
    "    \"\"\"aggreagte data for period by taking the mean number active developers\"\"\"\n",
    "    data = (\n",
    "        data[data[\"period\"].isin(periods)]\n",
    "        .groupby([\"iso2_code\", \"cluster_id\"])[\"num_pushers\"]\n",
    "        .agg(\"mean\")\n",
    "        .reset_index()\n",
    "    )\n",
    "    data[\"period\"] = 1\n",
    "    data[\"num_pushers\"] = data[\"num_pushers\"].astype(int)\n",
    "    return data\n",
    "\n",
    "\n",
    "# threshold for RCA : 1.00\n",
    "ps = [1, 2, 3, 4]\n",
    "rca_tables = []\n",
    "for p in ps:\n",
    "    print(p)\n",
    "    temp = bundle_data_clusters(df, periods=[p])\n",
    "    temp[\"period\"] = p\n",
    "    rca_tables.append(rca_calculation(temp, c_column=\"iso2_code\", p_column=\"cluster_id\", value_column=\"num_pushers\", threshold=1))\n",
    "\n",
    "#    dfbs.append(temp)\n",
    "rca_tables = pd.concat(rca_tables)\n",
    "\n",
    "\n",
    "\n",
    "# identify entry following the given patterns\n",
    "exit_pattern = [1,1,0,0]\n",
    "consider_pattern = [1,1,1,1]\n",
    "ext = rca_tables.sort_values([\"period\"], ascending=True).groupby([\"iso2_code\",\"cluster_id\"])[\"rca01\"].agg(list).reset_index()\n",
    "ext[\"entry01\"] = ext[\"rca01\"].apply(lambda x: x == exit_pattern).astype(int)\n",
    "ext[\"consider00\"] = ext[\"rca01\"].apply(lambda x: x == consider_pattern).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "# full combination\n",
    "all_countries = ext[\"iso2_code\"].unique()\n",
    "all_languages = ext[\"cluster_id\"].unique()\n",
    "\n",
    "all_combinations = list(product(all_countries, all_languages))\n",
    "full_df = pd.DataFrame(all_combinations, columns=[\"iso2_code\", \"cluster_id\"])\\\n",
    "    .sort_values([\"iso2_code\", \"cluster_id\"])\n",
    "\n",
    "# join entries\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    ext[[\"iso2_code\", \"cluster_id\", \"entry01\", \"consider00\"]],\n",
    "    on=[\"iso2_code\", \"cluster_id\"],\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# join complexity\n",
    "cdf = pd.read_csv(\"../outputs/eci_clusters_cooc_2020_2023.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    cdf[[\"iso2_code\", \"cluster_id\", \"pci\", \"ubiquity\"]],\n",
    "    on=[\"iso2_code\", \"cluster_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# join RCA from the baseline period\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rca_tables[rca_tables[\"period\"]==3].loc[:,[\"iso2_code\", \"cluster_id\", \"rca01\"]],\n",
    "    on=[\"iso2_code\", \"cluster_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_df[\"rca01\"] = full_df[\"rca01\"].fillna(0)\n",
    "\n",
    "# drop languages with no complexity value\n",
    "full_df.dropna(subset=[\"pci\"], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# join to full_df with entries and PCI\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rel_dens,\n",
    "    on=[\"iso2_code\", \"cluster_id\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# export for entry models -- only consider 00, 01 patterns\n",
    "full_df[\"entry01\"] = full_df[\"entry01\"].astype(int)\n",
    "full_df.rename(columns={\"entry01\":\"exit01\"}, inplace=True)\n",
    "full_df[\"consider00\"] = full_df[\"consider00\"].astype(int)\n",
    "export_df = full_df[(full_df[\"exit01\"]==1) | (full_df[\"consider00\"]==1)]\n",
    "export_df.to_csv(\"../outputs/data_exit_regressions_1100_clusters_cooc.csv\", index=False, sep=\";\")\n",
    "#export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold05.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IV for co-occurrence clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECI_software table\n",
    "#cdf = pd.read_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\")\n",
    "cdf = pd.read_csv(\"../outputs/eci_clusters_cooc_2020_2023.csv\", sep=\";\")\n",
    "\n",
    "# neighboring countries from https://github.com/geodatasource/country-borders\n",
    "nc = pd.read_csv(\"../data/geodatasource_country_borders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = list(set(cdf[\"iso2_code\"].to_list()))\n",
    "full_prod_countries = pd.DataFrame(itertools.product(locations, repeat=2), columns=[\"iso2_code1\", \"iso2_code2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_prod_countries = pd.merge(\n",
    "    full_prod_countries,\n",
    "    nc,\n",
    "    left_on=[\"iso2_code1\", \"iso2_code2\"],\n",
    "    right_on=[\"country_code\", \"country_border_code\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_prod_countries[\"neighbor01\"] = full_prod_countries[\"country_border_code\"].notna().astype(int)\n",
    "full_prod_countries = full_prod_countries[[\"iso2_code1\", \"iso2_code2\", \"neighbor01\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_82948/3383011093.py:83: FutureWarning: The provided callable <function mean at 0x1070a5d00> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  .agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_82948/3383011093.py:83: FutureWarning: The provided callable <function mean at 0x1070a5d00> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  .agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_82948/3383011093.py:83: FutureWarning: The provided callable <function mean at 0x1070a5d00> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  .agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_82948/3383011093.py:83: FutureWarning: The provided callable <function mean at 0x1070a5d00> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  .agg(\n"
     ]
    }
   ],
   "source": [
    "# select year\n",
    "year_list = [2020, 2021, 2022, 2023]\n",
    "cdf2 = []\n",
    "for y in year_list:\n",
    "    print(y)\n",
    "\n",
    "    tcdf = cdf[cdf[\"year\"] == y]\n",
    "    tcdf.year.isna().sum()\n",
    "\n",
    "    # generate full product dataframe\n",
    "    locations = list(set(tcdf[\"iso2_code\"].to_list()))\n",
    "    full_prod_countries = pd.DataFrame(itertools.product(locations, repeat=2), columns=[\"iso2_code1\", \"iso2_code2\"])\n",
    "    full_prod_countries = pd.merge(\n",
    "        full_prod_countries,\n",
    "        nc,\n",
    "        left_on=[\"iso2_code1\", \"iso2_code2\"],\n",
    "        right_on=[\"country_code\", \"country_border_code\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    full_prod_countries[\"neighbor01\"] = full_prod_countries[\"country_border_code\"].notna().astype(int)\n",
    "    full_prod_countries = full_prod_countries[[\"iso2_code1\", \"iso2_code2\", \"neighbor01\"]]\n",
    "\n",
    "    # add location - mcp array to location pairs\n",
    "    mcp_temp = tcdf.groupby(\"iso2_code\")[\"mcp\"].apply(np.array).reset_index()\n",
    "    full_prod_countries = pd.merge(\n",
    "        full_prod_countries,\n",
    "        mcp_temp,\n",
    "        left_on=\"iso2_code1\",\n",
    "        right_on=\"iso2_code\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    full_prod_countries = pd.merge(\n",
    "        full_prod_countries,\n",
    "        mcp_temp,\n",
    "        left_on=\"iso2_code2\",\n",
    "        right_on=\"iso2_code\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    full_prod_countries = full_prod_countries\\\n",
    "        .drop(columns=[\"iso2_code_x\", \"iso2_code_y\"])\\\n",
    "        .rename(columns={\"mcp_x\":\"mcp_array1\", \"mcp_y\":\"mcp_array2\"})\n",
    "\n",
    "    # minimum conditional probability -- to measure similarity between locations\n",
    "    full_prod_countries[\"spec_similarity\"] = full_prod_countries.apply(lambda r: round(sum(r[\"mcp_array1\"] * r[\"mcp_array2\"]) / max(sum(r[\"mcp_array1\"]), sum(r[\"mcp_array2\"])), 3), axis=1)\n",
    "\n",
    "    # drop iso2_code1 == iso2_code2 cases and neighbors\n",
    "    sim_spec_df = full_prod_countries[(full_prod_countries[\"iso2_code1\"] != full_prod_countries[\"iso2_code2\"]) & (full_prod_countries[\"neighbor01\"] == 0)]\n",
    "    \n",
    "    # keep the top3 most similar countries\n",
    "    sim_spec_df = sim_spec_df.groupby([\"iso2_code1\"])[\"spec_similarity\"]\\\n",
    "        .nlargest(3)\\\n",
    "        .reset_index()\\\n",
    "        .rename(columns={\"level_1\":\"iso2_code2_index\"})\n",
    "\n",
    "    # merge similar location names by index\n",
    "    sim_spec_df = pd.merge(\n",
    "        sim_spec_df,\n",
    "        full_prod_countries[[\"iso2_code2\"]].reset_index(),\n",
    "        left_on=\"iso2_code2_index\",\n",
    "        right_on=\"index\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # merge ECI values by location name\n",
    "    sim_spec_df = pd.merge(\n",
    "        sim_spec_df,\n",
    "        tcdf[[\"iso2_code\", \"eci\"]].drop_duplicates(),\n",
    "        left_on=\"iso2_code2\",\n",
    "        right_on=\"iso2_code\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # merge distance values by location name\n",
    "    sim_spec_df = pd.merge(\n",
    "        sim_spec_df,\n",
    "        full_prod_countries,\n",
    "        on=[\"iso2_code1\", \"iso2_code2\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # average ECI of the top 3 most similar location \n",
    "    avg_comp_sim_spec = sim_spec_df.groupby([\"iso2_code1\"])\\\n",
    "        .agg(\n",
    "            avg_eci_similar_spec = pd.NamedAgg(\"eci\", np.mean))\\\n",
    "        .reset_index()\\\n",
    "        .rename(columns={\"iso2_code1\" : \"iso2_code\"})\n",
    "\n",
    "    # join to full comb table\n",
    "    tcdf = pd.merge(\n",
    "        tcdf,\n",
    "        avg_comp_sim_spec,\n",
    "        on=\"iso2_code\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    cdf2.append(tcdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iso2_code</th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>num_pushers</th>\n",
       "      <th>period</th>\n",
       "      <th>diversity</th>\n",
       "      <th>ubiquity</th>\n",
       "      <th>mcp</th>\n",
       "      <th>eci</th>\n",
       "      <th>pci</th>\n",
       "      <th>density</th>\n",
       "      <th>coi</th>\n",
       "      <th>cog</th>\n",
       "      <th>rca</th>\n",
       "      <th>year</th>\n",
       "      <th>avg_eci_similar_spec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AE</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2414.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.604691</td>\n",
       "      <td>-1.531008</td>\n",
       "      <td>0.551220</td>\n",
       "      <td>-0.486011</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>1.033574</td>\n",
       "      <td>2020</td>\n",
       "      <td>-0.49600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AE</td>\n",
       "      <td>2.0</td>\n",
       "      <td>292.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.604691</td>\n",
       "      <td>1.048489</td>\n",
       "      <td>0.076690</td>\n",
       "      <td>-0.486011</td>\n",
       "      <td>0.724783</td>\n",
       "      <td>0.887635</td>\n",
       "      <td>2020</td>\n",
       "      <td>-0.49600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AE</td>\n",
       "      <td>3.0</td>\n",
       "      <td>246.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.604691</td>\n",
       "      <td>1.410831</td>\n",
       "      <td>0.066669</td>\n",
       "      <td>-0.486011</td>\n",
       "      <td>0.842606</td>\n",
       "      <td>0.764871</td>\n",
       "      <td>2020</td>\n",
       "      <td>-0.49600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AE</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.604691</td>\n",
       "      <td>1.548653</td>\n",
       "      <td>0.061788</td>\n",
       "      <td>-0.486011</td>\n",
       "      <td>0.812965</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2020</td>\n",
       "      <td>-0.49600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AE</td>\n",
       "      <td>5.0</td>\n",
       "      <td>189.785714</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.604691</td>\n",
       "      <td>-0.420012</td>\n",
       "      <td>0.388770</td>\n",
       "      <td>-0.486011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.274215</td>\n",
       "      <td>2020</td>\n",
       "      <td>-0.49600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8374</th>\n",
       "      <td>ZW</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.753676</td>\n",
       "      <td>1.028947</td>\n",
       "      <td>0.065524</td>\n",
       "      <td>-0.678235</td>\n",
       "      <td>0.445385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2020</td>\n",
       "      <td>-0.73992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8375</th>\n",
       "      <td>ZW</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.753676</td>\n",
       "      <td>2.183614</td>\n",
       "      <td>0.021669</td>\n",
       "      <td>-0.678235</td>\n",
       "      <td>1.070497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2020</td>\n",
       "      <td>-0.73992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8376</th>\n",
       "      <td>ZW</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.753676</td>\n",
       "      <td>2.169926</td>\n",
       "      <td>0.019789</td>\n",
       "      <td>-0.678235</td>\n",
       "      <td>0.999220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2020</td>\n",
       "      <td>-0.73992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8377</th>\n",
       "      <td>ZW</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.753676</td>\n",
       "      <td>1.449351</td>\n",
       "      <td>0.031942</td>\n",
       "      <td>-0.678235</td>\n",
       "      <td>0.792187</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2020</td>\n",
       "      <td>-0.73992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8378</th>\n",
       "      <td>ZW</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.753676</td>\n",
       "      <td>1.790266</td>\n",
       "      <td>0.029819</td>\n",
       "      <td>-0.678235</td>\n",
       "      <td>1.022479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2020</td>\n",
       "      <td>-0.73992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8379 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     iso2_code  cluster_id  num_pushers  period  diversity  ubiquity  mcp  \\\n",
       "0           AE         1.0  2414.333333       1          8        85    1   \n",
       "1           AE         2.0   292.250000       1          8        36    0   \n",
       "2           AE         3.0   246.500000       1          8        38    0   \n",
       "3           AE         4.0     0.000000       1          8        32    0   \n",
       "4           AE         5.0   189.785714       1          8        50    1   \n",
       "...        ...         ...          ...     ...        ...       ...  ...   \n",
       "8374        ZW        54.0     0.000000       1          6        42    0   \n",
       "8375        ZW        55.0     0.000000       1          6        16    0   \n",
       "8376        ZW        56.0     0.000000       1          6        14    0   \n",
       "8377        ZW        57.0     0.000000       1          6        22    0   \n",
       "8378        ZW        59.0     0.000000       1          6        23    0   \n",
       "\n",
       "           eci       pci   density       coi       cog       rca  year  \\\n",
       "0    -0.604691 -1.531008  0.551220 -0.486011 -0.000000  1.033574  2020   \n",
       "1    -0.604691  1.048489  0.076690 -0.486011  0.724783  0.887635  2020   \n",
       "2    -0.604691  1.410831  0.066669 -0.486011  0.842606  0.764871  2020   \n",
       "3    -0.604691  1.548653  0.061788 -0.486011  0.812965  0.000000  2020   \n",
       "4    -0.604691 -0.420012  0.388770 -0.486011  0.000000  1.274215  2020   \n",
       "...        ...       ...       ...       ...       ...       ...   ...   \n",
       "8374 -0.753676  1.028947  0.065524 -0.678235  0.445385  0.000000  2020   \n",
       "8375 -0.753676  2.183614  0.021669 -0.678235  1.070497  0.000000  2020   \n",
       "8376 -0.753676  2.169926  0.019789 -0.678235  0.999220  0.000000  2020   \n",
       "8377 -0.753676  1.449351  0.031942 -0.678235  0.792187  0.000000  2020   \n",
       "8378 -0.753676  1.790266  0.029819 -0.678235  1.022479  0.000000  2020   \n",
       "\n",
       "      avg_eci_similar_spec  \n",
       "0                 -0.49600  \n",
       "1                 -0.49600  \n",
       "2                 -0.49600  \n",
       "3                 -0.49600  \n",
       "4                 -0.49600  \n",
       "...                    ...  \n",
       "8374              -0.73992  \n",
       "8375              -0.73992  \n",
       "8376              -0.73992  \n",
       "8377              -0.73992  \n",
       "8378              -0.73992  \n",
       "\n",
       "[8379 rows x 15 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join and save\n",
    "cdf2 = pd.concat(cdf2)\n",
    "cdf2.to_csv(f\"../outputs/si_eci_clusters_cooc_2020_2023_ivreg.csv\", index=False, sep=\";\")\n",
    "# eci_clusters_cooc_2020_2023"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
