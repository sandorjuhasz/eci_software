{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from itertools import product\n",
    "from ecomplexity import ecomplexity\n",
    "from ecomplexity import proximity\n",
    "from ecomplexity import calc_density\n",
    "import country_converter as coco\n",
    "import itertools\n",
    "\n",
    "from utils import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from GitHub Innovation Graph\n",
      "languages.csv already exists\n",
      "developers.csv already exists\n"
     ]
    }
   ],
   "source": [
    "files = [\"languages.csv\", \"developers.csv\"]\n",
    "download_github_data(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1 - GitHub data preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter to choose year / semester / quarter to construct period IDs\n",
    "selected_period = \"year\"\n",
    "\n",
    "# for ecomplexity calculcation\n",
    "key_cols = {\n",
    "    \"time\": \"period\",\n",
    "    \"loc\": \"iso2_code\",\n",
    "    \"prod\": \"language\",\n",
    "    \"val\": \"num_pushers\",\n",
    "}\n",
    "\n",
    "# main GitHub data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n"
     ]
    }
   ],
   "source": [
    "# use data_prep_functions to clean the dataframe of ECI_software calculation\n",
    "data = data[data[\"year\"].isin([2020, 2021, 2022, 2023])]\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2 - from M_cl to ECI_software and language proximity - based on yearly data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Percentage of pairs compared that meet log-supermodularity condition: 2.67%\n",
      "1\n",
      "2020  DONE\n",
      "1\n",
      "Percentage of pairs compared that meet log-supermodularity condition: 4.96%\n",
      "1\n",
      "2021  DONE\n",
      "1\n",
      "Percentage of pairs compared that meet log-supermodularity condition: 5.28%\n",
      "1\n",
      "2022  DONE\n",
      "1\n",
      "Percentage of pairs compared that meet log-supermodularity condition: 4.93%\n",
      "1\n",
      "2023  DONE\n"
     ]
    }
   ],
   "source": [
    "# software complexity calculation -- period IDs -- 1 means 2020 on yearly basis\n",
    "ccdf = []\n",
    "ppdf = []\n",
    "year_dict = {1 : 2020, 2 : 2021, 3 : 2022, 4 : 2023}\n",
    "for k in year_dict.keys():\n",
    "    dfb = bundle_data(df, periods=[k])\n",
    "    cdf = ecomplexity(dfb, key_cols)\n",
    "    cdf[\"year\"] = year_dict[k]\n",
    "\n",
    "    pdf = proximity(dfb, key_cols)\n",
    "    pdf[\"year\"] = year_dict[k]\n",
    "\n",
    "    # combine yearly dataframes\n",
    "    ccdf.append(cdf)\n",
    "    ppdf.append(pdf)\n",
    "    print(year_dict[k], \" DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine and save -- complexity\n",
    "cdf = pd.concat(ccdf, axis=0, ignore_index=True)\n",
    "cdf.to_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine and save -- language proximity\n",
    "prox_df = pd.concat(ppdf, axis=0, ignore_index=True)\n",
    "prox_df.to_csv(\"../outputs/proximity_2020_2023.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 3 - ECI(software, trade, technology, research) and macroecon indicators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add ECI_software value\n",
    "eci_software = pd.read_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\")\n",
    "\n",
    "# country code switcher\n",
    "iso2_codes = eci_software[\"iso2_code\"]\n",
    "iso3_codes = coco.convert(names=iso2_codes, to=\"ISO3\")\n",
    "codes = pd.DataFrame(iso3_codes, iso2_codes).reset_index()\n",
    "codes.columns = [\"iso2_code\", \"iso3_code\"]\n",
    "codes = dict(zip(codes[\"iso2_code\"], codes[\"iso3_code\"]))\n",
    "\n",
    "# clean table for 2020\n",
    "eci_software[\"iso3_code\"] = eci_software[\"iso2_code\"].map(codes)\n",
    "eci_software = eci_software[[\"iso2_code\", \"iso3_code\", \"eci\", \"year\"]].drop_duplicates()\n",
    "eci_software.rename(columns={\"eci\":\"eci_software\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in 3 other ECI measures from OEC\n",
    "eci_trade = pd.read_csv(\"../data/Data-ECI-Trade.csv\")\n",
    "eci_trade = eci_trade[[\"Country\", \"2020\", \"2021\", \"2022\"]]\n",
    "eci_trade = pd.melt(\n",
    "        eci_trade,\n",
    "        id_vars=[\"Country\"],\n",
    "        value_vars=[\"2020\", \"2021\", \"2022\"],\n",
    "        var_name=\"year\",\n",
    "        value_name=\"eci_trade\",\n",
    "    ).rename(columns={\"Country\":\"country_name\"})\n",
    "eci_tech = pd.read_csv(\"../data/Data-ECI-Technology.csv\")\n",
    "eci_tech = eci_tech[[\"Country\", \"2020\", \"2021\", \"2022\"]]\n",
    "eci_tech = pd.melt(\n",
    "        eci_tech,\n",
    "        id_vars=[\"Country\"],\n",
    "        value_vars=[\"2020\", \"2021\", \"2022\"],\n",
    "        var_name=\"year\",\n",
    "        value_name=\"eci_tech\",\n",
    "    ).rename(columns={\"Country\":\"country_name\"})\n",
    "eci_research = pd.read_csv(\"../data/Data-ECI-Research.csv\")\n",
    "eci_research = eci_research[[\"Country\", \"2020\", \"2021\", \"2022\"]]\n",
    "eci_research = pd.melt(\n",
    "        eci_research,\n",
    "        id_vars=[\"Country\"],\n",
    "        value_vars=[\"2020\", \"2021\", \"2022\"],\n",
    "        var_name=\"year\",\n",
    "        value_name=\"eci_research\",\n",
    "    ).rename(columns={\"Country\":\"country_name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map country names to iso2_codes - using the country_converter package\n",
    "c_to_iso = dict(\n",
    "    zip(eci_trade.country_name.unique(), coco.convert(names=eci_trade.country_name.unique(), to=\"ISO2\")))\n",
    "eci_trade[\"iso2_code\"] = eci_trade[\"country_name\"].map(c_to_iso)\n",
    "c_to_iso = dict(\n",
    "    zip(eci_tech.country_name.unique(), coco.convert(names=eci_tech.country_name.unique(), to=\"ISO2\")))\n",
    "eci_tech[\"iso2_code\"] = eci_tech[\"country_name\"].map(c_to_iso)\n",
    "c_to_iso = dict(\n",
    "    zip(eci_research.country_name.unique(), coco.convert(names=eci_research.country_name.unique(), to=\"ISO2\")))\n",
    "eci_research[\"iso2_code\"] = eci_research[\"country_name\"].map(c_to_iso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join ECI tables\n",
    "eci_software[\"year\"] = eci_software[\"year\"].astype(int)\n",
    "eci_trade[\"year\"] = eci_trade[\"year\"].astype(int)\n",
    "eci_tech[\"year\"] = eci_tech[\"year\"].astype(int)\n",
    "eci_research[\"year\"] = eci_research[\"year\"].astype(int)\n",
    "\n",
    "eci_df = pd.merge(\n",
    "    eci_software,\n",
    "    eci_trade,\n",
    "    on=[\"iso2_code\", \"year\"]\n",
    ")\n",
    "eci_df = pd.merge(\n",
    "    eci_df,\n",
    "    eci_tech,\n",
    "    on=[\"iso2_code\", \"year\"]\n",
    ")\n",
    "eci_df = pd.merge(\n",
    "    eci_df,\n",
    "    eci_research,\n",
    "    on=[\"iso2_code\", \"year\"]\n",
    ").drop(columns=[\"country_name_x\", \"country_name_y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# World Bank -- general indicators\n",
    "wdf = pd.read_csv(\"../data/worldbank_general_indicators_2018_2023.csv\")\\\n",
    "    .rename(columns={\"Series Name\":\"variable\", \"Country Name\":\"country_name\", \"Country Code\":\"iso3_code\"})\n",
    "\n",
    "# variable names\n",
    "names_list = [\"Population, total\", \"GDP per capita (current US$)\", \"GDP growth (annual %)\", \"GDP (current US$)\"]\n",
    "new_names = [\"population\", \"gdp_per_capita\", \"gdp_growth\", \"gdp_current_USD\"]\n",
    "\n",
    "country_df_general = world_bank_data_cleaner(wdf, names_list=names_list, new_names=new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# World Bank -- GDP PPP\n",
    "wdf_gdp_ppp = pd.read_excel(\"../data/worldbank_gdp_ppp_1960_2023.xls\")\\\n",
    "    .rename(columns={\"Indicator Name\":\"variable\", \"Country Name\":\"country_name\", \"Country Code\":\"iso3_code\"})\n",
    "\n",
    "# variable names\n",
    "gdp_ppp_df = pd.melt(\n",
    "        wdf_gdp_ppp,\n",
    "        id_vars=[\"variable\", \"country_name\", \"iso3_code\"],\n",
    "        value_vars=[str(year) for year in range(2010, 2023)],\n",
    "        var_name=\"year\",\n",
    "        value_name=\"gdp_ppp\",\n",
    "    ).drop(columns=[\"variable\"])\n",
    "gdp_ppp_df[\"year\"] = gdp_ppp_df[\"year\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# World Bank data on natural resources\n",
    "wdf_nat = pd.read_csv(\"../data/worldbank_total_natural_resources_rents_GPD_perc_2014_2021.csv\")\\\n",
    "    .rename(columns={\"Series Name\":\"variable\", \"Country Name\":\"country_name\", \"Country Code\":\"iso3_code\"})\n",
    "\n",
    "# variable names\n",
    "names_list = [\"Total natural resources rents (% of GDP)\"]\n",
    "new_names = [\"natural_resources\"]\n",
    "\n",
    "country_df_natural_res = world_bank_data_cleaner(wdf_nat, names_list=names_list, new_names=new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# World Bank data on human capital\n",
    "wdf_hum = pd.read_csv(\"../data/worldbank_human_capital_indicators_2018_2020.csv\")\\\n",
    "    .rename(columns={\"Series Name\":\"variable\", \"Country Name\":\"country_name\", \"Country Code\":\"iso3_code\"})\n",
    "\n",
    "# variable names\n",
    "names_list = [\"Human Capital Index (HCI) (scale 0-1)\"]\n",
    "new_names = [\"human_capital_index\"]\n",
    "\n",
    "country_df_human_cap = world_bank_data_cleaner(wdf_hum, names_list=names_list, new_names=new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# World Bank data on Gini -- https://data.worldbank.org/indicator/SI.POV.GINI\n",
    "wdf_gini = pd.read_excel(\"../data/worldbank_gini.xls\")\\\n",
    "    .rename(columns={\"Indicator Name\":\"variable\", \"Country Name\":\"country_name\", \"Country Code\":\"iso3_code\"})\n",
    "\n",
    "# variable names\n",
    "gini_df = pd.melt(\n",
    "        wdf_gini,\n",
    "        id_vars=[\"variable\", \"country_name\", \"iso3_code\"],\n",
    "        value_vars=[str(year) for year in range(2010, 2023)],\n",
    "        var_name=\"year\",\n",
    "        value_name=\"gini\",\n",
    "    ).drop(columns=[\"variable\"])\n",
    "\n",
    "# mean Gini -- too many missing data points\n",
    "gini_df[\"gini_mean\"] = gini_df.groupby([\"country_name\", \"iso3_code\"])[\"gini\"].transform(\"mean\")\n",
    "gini_df[\"year\"] = gini_df[\"year\"].astype(int)\n",
    "gini_df[\"gini_mean_2020_2022\"] = gini_df[gini_df[\"year\"]>=2020].groupby([\"country_name\", \"iso3_code\"])[\"gini\"].transform(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# World Bank data on emissions\n",
    "wdf_emission = pd.read_excel(\"../data/worldbank_emission_1960_2022.xlsx\")\n",
    "#wdf_emission = wdf_emission[wdf_emission[\"Indicator\"].str.contains(\"Total greenhouse gas emissions (kt of CO2 equivalent)\", na=False)]\\\n",
    "wdf_emission = wdf_emission[wdf_emission[\"Indicator\"] == \"Total greenhouse gas emissions (kt of CO2 equivalent)\"]\\\n",
    "    .rename(columns={\"Indicator\":\"variable\", \"Economy Name\":\"country_name\", \"Economy ISO3\":\"iso3_code\"})\n",
    "\n",
    "# variable names\n",
    "emission_df = pd.melt(\n",
    "        wdf_emission,\n",
    "        id_vars=[\"variable\", \"iso3_code\"],\n",
    "        value_vars=[str(year) for year in range(2010, 2022)],\n",
    "        var_name=\"year\",\n",
    "        value_name=\"total_ghg_emissions\",\n",
    "    ).drop(columns=[\"variable\"])\n",
    "emission_df[\"year\"] = emission_df[\"year\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join emission data from Viktor\n",
    "emissions_viktor = pd.read_csv(\"../data/regressions_emissions_data.csv\")\n",
    "emissions_viktor = emissions_viktor[[\"country\", \"year\", \"emissions\"]].drop_duplicates().rename(columns={\"country\":\"iso3_code\", \"emissions\":\"emission_viktor\"})\n",
    "emissions_viktor[\"year\"] = emissions_viktor[\"year\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join World Bank tables\n",
    "rdf = pd.merge(\n",
    "    country_df_general,\n",
    "    gdp_ppp_df,\n",
    "    on=[\"country_name\", \"iso3_code\", \"year\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "rdf = pd.merge(\n",
    "    rdf,\n",
    "    country_df_natural_res,\n",
    "    on=[\"country_name\", \"iso3_code\", \"year\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "rdf = pd.merge(\n",
    "    rdf,\n",
    "    country_df_human_cap,\n",
    "    on=[\"country_name\", \"iso3_code\", \"year\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "rdf = pd.merge(\n",
    "    rdf,\n",
    "    gini_df,\n",
    "    on=[\"country_name\", \"iso3_code\", \"year\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# mean Gini correction\n",
    "rdf[\"gini_mean\"] = rdf.groupby([\"country_name\", \"iso3_code\"])[\"gini_mean\"].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "# add emissions\n",
    "rdf[\"year\"] = rdf[\"year\"].astype(int)\n",
    "rdf = pd.merge(\n",
    "    rdf,\n",
    "    emission_df,\n",
    "    on=[\"iso3_code\", \"year\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "rdf = pd.merge(\n",
    "    rdf,\n",
    "    emissions_viktor,\n",
    "    on=[\"iso3_code\", \"year\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# replace .. w/ NA\n",
    "rdf.replace([\"..\"], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join ECI and country information\n",
    "reg_df = pd.merge(\n",
    "    eci_df,\n",
    "    rdf,\n",
    "    on=[\"iso3_code\", \"year\"],\n",
    "    how=\"outer\",\n",
    "    suffixes=[\"\", \"2\"]\n",
    ").drop(columns=\"country_name2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for regressions in R\n",
    "reg_df.to_csv(\"../outputs/eci_regression_table.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2b - matrices based on trade/research/publications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and reshape matrices\n",
    "trade_df = mat_reshape(path=\"../data/stojkoski_etal_data/trade_matrix_data_2020.csv\", column_labels=[\"product\", \"iso3_code\", \"value\"])\n",
    "patent_df = mat_reshape(path=\"../data/stojkoski_etal_data/pct_data_2020.csv\", column_labels=[\"class\", \"iso3_code\", \"value\"])\n",
    "research_df = mat_reshape(path=\"../data/stojkoski_etal_data/pub_matrix_data_2020.csv\", column_labels=[\"category\", \"iso3_code\", \"value\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ANT not found in ISO3\n",
      "YUG not found in ISO3\n",
      "CSE not found in ISO3\n",
      "DDE not found in ISO3\n",
      "EPO not found in ISO3\n",
      "XKO not found in ISO3\n",
      "SFE not found in ISO3\n",
      "SUE not found in ISO3\n",
      "XTP not found in ISO3\n",
      "XUB not found in ISO3\n",
      "FST not found in ISO3\n",
      "PIT not found in ISO3\n"
     ]
    }
   ],
   "source": [
    "# read and reshape matrices\n",
    "trade_df = mat_reshape(path=\"../data/stojkoski_etal_data/trade_matrix_data_2020.csv\", column_labels=[\"product\", \"iso3_code\", \"value\"])\n",
    "patent_df = mat_reshape(path=\"../data/stojkoski_etal_data/pct_data_2020.csv\", column_labels=[\"class\", \"iso3_code\", \"value\"])\n",
    "research_df = mat_reshape(path=\"../data/stojkoski_etal_data/pub_matrix_data_2020.csv\", column_labels=[\"category\", \"iso3_code\", \"value\"])\n",
    "\n",
    "# country code correction\n",
    "iso3_codes = pd.concat([trade_df[\"iso3_code\"], patent_df[\"iso3_code\"], research_df[\"iso3_code\"]]).unique().tolist()\n",
    "iso2_codes = coco.convert(names=iso3_codes, to=\"ISO2\")\n",
    "codes2 = pd.DataFrame(iso3_codes, iso2_codes).reset_index()\n",
    "codes2.columns = [\"iso2_code\", \"iso3_code\"]\n",
    "\n",
    "trade_df = pd.merge(\n",
    "    trade_df,\n",
    "    codes2,\n",
    "    on=\"iso3_code\",\n",
    "    how=\"left\"\n",
    ")\n",
    "trade_df = trade_df[trade_df[\"iso2_code\"] != \"not found\"]\n",
    "\n",
    "patent_df = pd.merge(\n",
    "    patent_df,\n",
    "    codes2,\n",
    "    on=\"iso3_code\",\n",
    "    how=\"left\"\n",
    ")\n",
    "patent_df = patent_df[patent_df[\"iso2_code\"] != \"not found\"]\n",
    "\n",
    "research_df = pd.merge(\n",
    "    research_df,\n",
    "    codes2,\n",
    "    on=\"iso3_code\",\n",
    "    how=\"left\"\n",
    ")\n",
    "research_df = research_df[research_df[\"iso2_code\"] != \"not found\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178788, 4)\n",
      "(50768, 4)\n",
      "(49131, 4)\n"
     ]
    }
   ],
   "source": [
    "# World Bank -- general indicators\n",
    "wdf = pd.read_csv(\"../data/worldbank_general_indicators_2018_2023.csv\")\\\n",
    "    .rename(columns={\"Series Name\":\"variable\", \"Country Name\":\"country_name\", \"Country Code\":\"iso3_code\"})\n",
    "\n",
    "# variable names\n",
    "names_list = [\"Population, total\", \"GDP per capita (current US$)\", \"GDP growth (annual %)\", \"GDP (current US$)\"]\n",
    "new_names = [\"population\", \"gdp_per_capita\", \"gdp_growth\", \"gdp_current_USD\"]\n",
    "\n",
    "country_df_general = world_bank_data_cleaner(wdf, names_list=names_list, new_names=new_names)\n",
    "\n",
    "# population above 1 million\n",
    "country_df_general[\"population\"] = pd.to_numeric(country_df_general[\"population\"], errors=\"coerce\").round().astype(\"Int64\")\n",
    "countries_1m_pop = list(set(country_df_general[country_df_general[\"population\"]>1000000][\"iso3_code\"].to_list()))\n",
    "trade_df = trade_df[trade_df[\"iso3_code\"].isin(countries_1m_pop)]\n",
    "patent_df = patent_df[patent_df[\"iso3_code\"].isin(countries_1m_pop)]\n",
    "research_df = research_df[research_df[\"iso3_code\"].isin(countries_1m_pop)]\n",
    "\n",
    "# total export value of 1 billion USD\n",
    "above_1b_export = trade_df.groupby([\"iso2_code\"])[\"value\"].agg(\"sum\").reset_index()\n",
    "above_1b_export = list(set(above_1b_export[above_1b_export[\"value\"]>10**9][\"iso2_code\"].to_list()))\n",
    "trade_df = trade_df[trade_df[\"iso2_code\"].isin(above_1b_export)]\n",
    "print(trade_df.shape)\n",
    "\n",
    "# MIN 4 patent\n",
    "min4_patents = patent_df.groupby([\"iso2_code\"])[\"value\"].agg(\"sum\").reset_index()\n",
    "min4_patents = list(set(min4_patents[min4_patents[\"value\"] > 4][\"iso2_code\"].to_list()))\n",
    "patent_df = patent_df[patent_df[\"iso2_code\"].isin(min4_patents)]\n",
    "print(patent_df.shape)\n",
    "\n",
    "# countries w/ MIN 100 publications in a year - category w/ more than 30 published papers a year\n",
    "min100_publications = research_df.groupby([\"iso2_code\"])[\"value\"].agg(\"sum\").reset_index()\n",
    "min100_publications = list(set(min100_publications[min100_publications[\"value\"] >= 100][\"iso2_code\"].to_list()))\n",
    "min30_papers = research_df.groupby([\"category\"])[\"value\"].agg(\"sum\").reset_index()\n",
    "min30_papers = list(set(min30_papers[min30_papers[\"value\"] >= 30][\"category\"].to_list()))\n",
    "research_df = research_df[(research_df[\"category\"].isin(min30_papers))]\n",
    "\n",
    "# replace below 3 papers per country/category to 0\n",
    "research_df[\"value\"] = np.where(research_df[\"value\"]<3, 0, research_df[\"value\"])\n",
    "print(research_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace below avg 100 citations per country/category to 0\n",
    "years = [2017, 2018, 2019, 2020]\n",
    "citations = []\n",
    "for y in years:\n",
    "    temp = mat_reshape(path=f\"../data/stojkoski_etal_data/cit_matrix_data_{y}.csv\", column_labels=[\"category\", \"iso3_code\", \"citations\"])\n",
    "    temp[\"year\"] = y\n",
    "    citations.append(temp)\n",
    "\n",
    "citations = pd.concat(citations)\n",
    "citations = citations.groupby([\"category\", \"iso3_code\"])[\"citations\"].agg(\"mean\").reset_index()\n",
    "\n",
    "research_df = pd.merge(\n",
    "    research_df,\n",
    "    citations,\n",
    "    on=[\"iso3_code\", \"category\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "research_df[\"value\"] = np.where(research_df[\"citations\"]<100, 0, research_df[\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Percentage of pairs compared that meet log-supermodularity condition: 38.67%\n",
      "1\n",
      "Percentage of pairs compared that meet log-supermodularity condition: 3.64%\n",
      "1\n",
      "Percentage of pairs compared that meet log-supermodularity condition: 14.47%\n"
     ]
    }
   ],
   "source": [
    "# calculate complexity and mcp\n",
    "key_cols_trade = {\n",
    "    \"time\": \"period\",\n",
    "    \"loc\": \"iso2_code\",\n",
    "    \"prod\": \"product\",\n",
    "    \"val\": \"value\",\n",
    "}\n",
    "trade_df[\"period\"] = 1\n",
    "trade_cdf = ecomplexity(trade_df, key_cols_trade)\n",
    "\n",
    "key_cols_patent = {\n",
    "    \"time\": \"period\",\n",
    "    \"loc\": \"iso2_code\",\n",
    "    \"prod\": \"class\",\n",
    "    \"val\": \"value\",\n",
    "}\n",
    "patent_df[\"period\"] = 1\n",
    "patent_cdf = ecomplexity(patent_df, key_cols_patent)\n",
    "\n",
    "key_cols_research = {\n",
    "    \"time\": \"period\",\n",
    "    \"loc\": \"iso2_code\",\n",
    "    \"prod\": \"category\",\n",
    "    \"val\": \"value\",\n",
    "}\n",
    "research_df[\"period\"] = 1\n",
    "research_cdf = ecomplexity(research_df, key_cols_research)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for figures\n",
    "trade_cdf.to_csv(\"../outputs/trade_cdf_2020.csv\", sep=\";\", index=False)\n",
    "patent_cdf.to_csv(\"../outputs/patent_cdf_2020.csv\", sep=\";\", index=False)\n",
    "research_cdf.to_csv(\"../outputs/research_cdf_2020.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 3 - for entry regressions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relatedness density -- as in Hidalgo et al. (2007) Science\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\")\n",
    "rel_dens = cdf[cdf[\"year\"] == 2020][[\"iso2_code\", \"language\", \"density\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n"
     ]
    }
   ],
   "source": [
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "selected_period = \"year\"\n",
    "\n",
    "# steps to prep dataframe of ecomplexity\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# threshold for RCA : 1\n",
    "ps = [1, 2, 3, 4]\n",
    "rca_tables = []\n",
    "for p in ps:\n",
    "    print(p)\n",
    "    temp = bundle_data(df, periods=[p])\n",
    "    temp[\"period\"] = p\n",
    "    rca_tables.append(rca_calculation(temp, c_column=\"iso2_code\", p_column=\"language\", value_column=\"num_pushers\", threshold=1))\n",
    "\n",
    "#    dfbs.append(temp)\n",
    "rca_tables = pd.concat(rca_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify entry following the given patterns\n",
    "entry_pattern = [0,0,1,1]\n",
    "consider_pattern = [0,0,0,0]\n",
    "ent = rca_tables.sort_values([\"period\"], ascending=True).groupby([\"iso2_code\",\"language\"])[\"rca01\"].agg(list).reset_index()\n",
    "ent[\"entry01\"] = ent[\"rca01\"].apply(lambda x: x == entry_pattern).astype(int)\n",
    "ent[\"consider00\"] = ent[\"rca01\"].apply(lambda x: x == consider_pattern).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full combination\n",
    "all_countries = ent[\"iso2_code\"].unique()\n",
    "all_languages = ent[\"language\"].unique()\n",
    "\n",
    "all_combinations = list(product(all_countries, all_languages))\n",
    "full_df = pd.DataFrame(all_combinations, columns=[\"iso2_code\", \"language\"])\\\n",
    "    .sort_values([\"iso2_code\", \"language\"])\n",
    "\n",
    "# join entries\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    ent[[\"iso2_code\", \"language\", \"entry01\", \"consider00\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# join complexity\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    cdf[[\"iso2_code\", \"language\", \"pci\", \"ubiquity\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# join RCA from the baseline period\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rca_tables[rca_tables[\"period\"]==3].loc[:,[\"iso2_code\", \"language\", \"rca01\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_df[\"rca01\"] = full_df[\"rca01\"].fillna(0)\n",
    "\n",
    "# drop languages with no complexity value\n",
    "full_df.dropna(subset=[\"pci\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join to full_df with entries and PCI\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rel_dens,\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for entry models -- only consider 00, 01 patterns\n",
    "full_df[\"entry01\"] = full_df[\"entry01\"].astype(int)\n",
    "full_df[\"consider00\"] = full_df[\"consider00\"].astype(int)\n",
    "export_df = full_df[(full_df[\"entry01\"]==1) | (full_df[\"consider00\"]==1)]\n",
    "export_df.to_csv(\"../outputs/data_entry_regressions_0011.csv\", index=False, sep=\";\")\n",
    "#export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold05.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 4 - for exit regressions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relatedness density -- as in Hidalgo et al. (2007) Science\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\")\n",
    "rel_dens = cdf[cdf[\"year\"] == 2020][[\"iso2_code\", \"language\", \"density\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93076, 7)\n"
     ]
    }
   ],
   "source": [
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "selected_period = \"year\"\n",
    "\n",
    "# steps to prep dataframe of ecomplexity\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# threshold for RCA : 1\n",
    "ps = [1, 2, 3, 4]\n",
    "rca_tables = []\n",
    "for p in ps:\n",
    "    print(p)\n",
    "    temp = bundle_data(df, periods=[p])\n",
    "    temp[\"period\"] = p\n",
    "    rca_tables.append(rca_calculation(temp, c_column=\"iso2_code\", p_column=\"language\", value_column=\"num_pushers\", threshold=1))\n",
    "\n",
    "#    dfbs.append(temp)\n",
    "rca_tables = pd.concat(rca_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify entry following the given patterns\n",
    "exit_pattern = [1,1,0,0]\n",
    "consider_pattern = [1,1,1,1]\n",
    "ext = rca_tables.sort_values([\"period\"], ascending=True).groupby([\"iso2_code\",\"language\"])[\"rca01\"].agg(list).reset_index()\n",
    "ext[\"entry01\"] = ext[\"rca01\"].apply(lambda x: x == exit_pattern).astype(int)\n",
    "ext[\"consider00\"] = ext[\"rca01\"].apply(lambda x: x == consider_pattern).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full combination\n",
    "all_countries = ext[\"iso2_code\"].unique()\n",
    "all_languages = ext[\"language\"].unique()\n",
    "\n",
    "all_combinations = list(product(all_countries, all_languages))\n",
    "full_df = pd.DataFrame(all_combinations, columns=[\"iso2_code\", \"language\"])\\\n",
    "    .sort_values([\"iso2_code\", \"language\"])\n",
    "\n",
    "# join entries\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    ext[[\"iso2_code\", \"language\", \"entry01\", \"consider00\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# join complexity\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    cdf[[\"iso2_code\", \"language\", \"pci\", \"ubiquity\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# join RCA from the baseline period\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rca_tables[rca_tables[\"period\"]==3].loc[:,[\"iso2_code\", \"language\", \"rca01\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_df[\"rca01\"] = full_df[\"rca01\"].fillna(0)\n",
    "\n",
    "# drop languages with no complexity value\n",
    "full_df.dropna(subset=[\"pci\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join to full_df with entries and PCI\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rel_dens,\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for entry models -- only consider 00, 01 patterns\n",
    "full_df[\"entry01\"] = full_df[\"entry01\"].astype(int)\n",
    "full_df.rename(columns={\"entry01\":\"exit01\"}, inplace=True)\n",
    "full_df[\"consider00\"] = full_df[\"consider00\"].astype(int)\n",
    "export_df = full_df[(full_df[\"exit01\"]==1) | (full_df[\"consider00\"]==1)]\n",
    "export_df.to_csv(\"../outputs/data_exit_regressions_1100.csv\", index=False, sep=\";\")\n",
    "#export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold05.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IV -- 3 most similar non-neighboring countries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECI_software table\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\")\n",
    "\n",
    "# neighboring countries from https://github.com/geodatasource/country-borders\n",
    "nc = pd.read_csv(\"../data/geodatasource_country_borders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = list(set(cdf[\"iso2_code\"].to_list()))\n",
    "full_prod_countries = pd.DataFrame(itertools.product(locations, repeat=2), columns=[\"iso2_code1\", \"iso2_code2\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_prod_countries = pd.merge(\n",
    "    full_prod_countries,\n",
    "    nc,\n",
    "    left_on=[\"iso2_code1\", \"iso2_code2\"],\n",
    "    right_on=[\"country_code\", \"country_border_code\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_prod_countries[\"neighbor01\"] = full_prod_countries[\"country_border_code\"].notna().astype(int)\n",
    "full_prod_countries = full_prod_countries[[\"iso2_code1\", \"iso2_code2\", \"neighbor01\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n"
     ]
    }
   ],
   "source": [
    "# select year\n",
    "year_list = [2020, 2021, 2022, 2023]\n",
    "cdf2 = []\n",
    "for y in year_list:\n",
    "    print(y)\n",
    "\n",
    "    tcdf = cdf[cdf[\"year\"] == y]\n",
    "    tcdf.year.isna().sum()\n",
    "\n",
    "    # generate full product dataframe\n",
    "    locations = list(set(tcdf[\"iso2_code\"].to_list()))\n",
    "    full_prod_countries = pd.DataFrame(itertools.product(locations, repeat=2), columns=[\"iso2_code1\", \"iso2_code2\"])\n",
    "    full_prod_countries = pd.merge(\n",
    "        full_prod_countries,\n",
    "        nc,\n",
    "        left_on=[\"iso2_code1\", \"iso2_code2\"],\n",
    "        right_on=[\"country_code\", \"country_border_code\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    full_prod_countries[\"neighbor01\"] = full_prod_countries[\"country_border_code\"].notna().astype(int)\n",
    "    full_prod_countries = full_prod_countries[[\"iso2_code1\", \"iso2_code2\", \"neighbor01\"]]\n",
    "\n",
    "    # add location - mcp array to location pairs\n",
    "    mcp_temp = tcdf.groupby(\"iso2_code\")[\"mcp\"].apply(np.array).reset_index()\n",
    "    full_prod_countries = pd.merge(\n",
    "        full_prod_countries,\n",
    "        mcp_temp,\n",
    "        left_on=\"iso2_code1\",\n",
    "        right_on=\"iso2_code\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    full_prod_countries = pd.merge(\n",
    "        full_prod_countries,\n",
    "        mcp_temp,\n",
    "        left_on=\"iso2_code2\",\n",
    "        right_on=\"iso2_code\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    full_prod_countries = full_prod_countries\\\n",
    "        .drop(columns=[\"iso2_code_x\", \"iso2_code_y\"])\\\n",
    "        .rename(columns={\"mcp_x\":\"mcp_array1\", \"mcp_y\":\"mcp_array2\"})\n",
    "\n",
    "    # minimum conditional probability -- to measure similarity between locations\n",
    "    full_prod_countries[\"spec_similarity\"] = full_prod_countries.apply(lambda r: round(sum(r[\"mcp_array1\"] * r[\"mcp_array2\"]) / max(sum(r[\"mcp_array1\"]), sum(r[\"mcp_array2\"])), 3), axis=1)\n",
    "\n",
    "    # drop iso2_code1 == iso2_code2 cases and neighbors\n",
    "    sim_spec_df = full_prod_countries[(full_prod_countries[\"iso2_code1\"] != full_prod_countries[\"iso2_code2\"]) & (full_prod_countries[\"neighbor01\"] == 0)]\n",
    "    \n",
    "    # keep the top3 most similar countries\n",
    "    sim_spec_df = sim_spec_df.groupby([\"iso2_code1\"])[\"spec_similarity\"]\\\n",
    "        .nlargest(3)\\\n",
    "        .reset_index()\\\n",
    "        .rename(columns={\"level_1\":\"iso2_code2_index\"})\n",
    "\n",
    "    # merge similar location names by index\n",
    "    sim_spec_df = pd.merge(\n",
    "        sim_spec_df,\n",
    "        full_prod_countries[[\"iso2_code2\"]].reset_index(),\n",
    "        left_on=\"iso2_code2_index\",\n",
    "        right_on=\"index\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # merge ECI values by location name\n",
    "    sim_spec_df = pd.merge(\n",
    "        sim_spec_df,\n",
    "        tcdf[[\"iso2_code\", \"eci\"]].drop_duplicates(),\n",
    "        left_on=\"iso2_code2\",\n",
    "        right_on=\"iso2_code\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # merge distance values by location name\n",
    "    sim_spec_df = pd.merge(\n",
    "        sim_spec_df,\n",
    "        full_prod_countries,\n",
    "        on=[\"iso2_code1\", \"iso2_code2\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # average ECI of the top 3 most similar location \n",
    "    avg_comp_sim_spec = sim_spec_df.groupby([\"iso2_code1\"])\\\n",
    "        .agg(\n",
    "            avg_eci_similar_spec = pd.NamedAgg(\"eci\", np.mean))\\\n",
    "        .reset_index()\\\n",
    "        .rename(columns={\"iso2_code1\" : \"iso2_code\"})\n",
    "\n",
    "    # join to full comb table\n",
    "    tcdf = pd.merge(\n",
    "        tcdf,\n",
    "        avg_comp_sim_spec,\n",
    "        on=\"iso2_code\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    cdf2.append(tcdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join and save\n",
    "cdf2 = pd.concat(cdf2)\n",
    "cdf2.to_csv(f\"../outputs/si_eci_software_2020_2023_ivreg.csv\", index=False, sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
