{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from itertools import product\n",
    "from ecomplexity import ecomplexity\n",
    "from ecomplexity import proximity\n",
    "from ecomplexity import calc_density\n",
    "import country_converter as coco\n",
    "import itertools\n",
    "\n",
    "from data_prep_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 0 - general data preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter to choose year / semester / quarter to construct period IDs\n",
    "selected_period = \"year\"\n",
    "\n",
    "# for ecomplexity calculcation\n",
    "key_cols = {\n",
    "    \"time\": \"period\",\n",
    "    \"loc\": \"iso2_code\",\n",
    "    \"prod\": \"language\",\n",
    "    \"val\": \"num_pushers\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88775, 7)\n"
     ]
    }
   ],
   "source": [
    "# use data_prep_functions to clean the dataframe of ECI_software calculation\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1 - from M_cl to ECI_software and language proximity - based on yearly data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Percentage of pairs compared that meet log-supermodularity condition: 6.75%\n",
      "1\n",
      "2020  DONE\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 1: Log-supermodularity condition is not fully satisfied (6.75% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 3.69%\n",
      "1\n",
      "2021  DONE\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 1: Log-supermodularity condition is not fully satisfied (3.69% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 3.91%\n",
      "1\n",
      "2022  DONE\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 1: Log-supermodularity condition is not fully satisfied (3.91% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 6.38%\n",
      "1\n",
      "2023  DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 1: Log-supermodularity condition is not fully satisfied (6.38% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# software complexity calculation -- period IDs -- 1 means 2020 on yearly basis\n",
    "ccdf = []\n",
    "ppdf = []\n",
    "year_dict = {1 : 2020, 2 : 2021, 3 : 2022, 4 : 2023}\n",
    "for k in year_dict.keys():\n",
    "    dfb = bundle_data(df, periods=[k])\n",
    "    cdf = ecomplexity(dfb, key_cols)\n",
    "    cdf[\"year\"] = year_dict[k]\n",
    "\n",
    "    pdf = proximity(dfb, key_cols)\n",
    "    pdf[\"year\"] = year_dict[k]\n",
    "\n",
    "    # combine yearly dataframes\n",
    "    ccdf.append(cdf)\n",
    "    ppdf.append(pdf)\n",
    "    print(year_dict[k], \" DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine and save -- complexity\n",
    "cdf = pd.concat(ccdf, axis=0, ignore_index=True)\n",
    "cdf.to_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine and save -- language proximity\n",
    "prox_df = pd.concat(ppdf, axis=0, ignore_index=True)\n",
    "prox_df.to_csv(\"../outputs/proximity_2020_2023.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2 - comparing ECI(software, trade, technology, research)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in developers (for filtering option)\n",
    "df = pd.read_csv(\"../data/developers.csv\")\n",
    "df = df.groupby([\"iso2_code\", \"year\"])[\"developers\"].agg(\"mean\").reset_index()\n",
    "df[\"developers\"] = df[\"developers\"].astype(int)\n",
    "df = df[df[\"year\"]==2020]\n",
    "\n",
    "# add ECI_software\n",
    "eci_software = pd.read_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\")\n",
    "eci_software = eci_software[eci_software[\"year\"]==2020]\n",
    "country_to_seci_2020 = dict(zip(eci_software[\"iso2_code\"], eci_software[\"eci\"]))\n",
    "df[\"software_eci_2020\"] = df[\"iso2_code\"].map(country_to_seci_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in 3 other ECI measures\n",
    "trade_eci = pd.read_csv(\"../data/eci_hs6_hs96_trade.csv\")\n",
    "trade_eci = trade_eci[[\"Country\", \"2020\"]]\n",
    "tech_eci = pd.read_csv(\"../data/eci_tech.csv\")\n",
    "tech_eci = tech_eci[[\"Country\", \"2020\"]]\n",
    "research_eci = pd.read_csv(\"../data/Data-ECI-Research.csv\")\n",
    "research_eci = research_eci[[\"Country\", \"2020\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map country names to iso2_codes - using the country_converter package\n",
    "c_to_iso = dict(\n",
    "    zip(trade_eci.Country.unique(), coco.convert(names=trade_eci.Country.unique(), to=\"ISO2\")))\n",
    "trade_eci[\"Country\"] = trade_eci[\"Country\"].map(c_to_iso)\n",
    "iso_to_trade_eci = dict(trade_eci.values)\n",
    "df[\"trade_eci_2020\"] = df[\"iso2_code\"].map(iso_to_trade_eci)\n",
    "\n",
    "tech_eci[\"Country\"] = tech_eci[\"Country\"].map(c_to_iso)\n",
    "iso_to_tech_eci = dict(tech_eci.values)\n",
    "df[\"tech_eci_2020\"] = df[\"iso2_code\"].map(iso_to_tech_eci)\n",
    "\n",
    "research_eci[\"Country\"] = research_eci[\"Country\"].map(c_to_iso)\n",
    "iso_to_research_eci = dict(research_eci.values)\n",
    "df[\"research_eci_2020\"] = df[\"iso2_code\"].map(iso_to_research_eci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_20344/1367150806.py:2: DtypeWarning: Columns (40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  trade_df = pd.read_csv(\"../data/Gravity_V202211.csv\")\n"
     ]
    }
   ],
   "source": [
    "# data from CEPII -- http://www.cepii.fr/CEPII/en/bdd_modele/bdd_modele_item.asp?id=8\n",
    "trade_df = pd.read_csv(\"../data/Gravity_V202211.csv\")\n",
    "trade_df = trade_df[trade_df[\"year\"] == 2020]\n",
    "trade_df['country_id_o'] = trade_df['country_id_o'].map(lambda x: x.replace('.2',''))\n",
    "trade_df['country_id_d'] = trade_df['country_id_d'].map(lambda x: x.replace('.2',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ANT not found in ISO3\n",
      "CSK not found in ISO3\n",
      "DDR not found in ISO3\n",
      "SCG not found in ISO3\n",
      "SUN not found in ISO3\n",
      "VDR not found in ISO3\n",
      "YMD not found in ISO3\n",
      "YUG not found in ISO3\n"
     ]
    }
   ],
   "source": [
    "# code transformation\n",
    "iso3_codes = pd.concat([trade_df[\"iso3_o\"], trade_df[\"iso3_d\"]]).unique().tolist()\n",
    "iso2_codes = coco.convert(names=iso3_codes, to=\"ISO2\")\n",
    "codes = pd.DataFrame(iso3_codes, iso2_codes).reset_index()\n",
    "codes.columns = [\"iso2_code\", \"iso3_code\"]\n",
    "\n",
    "trade_df = pd.merge(\n",
    "    trade_df,\n",
    "    codes,\n",
    "    left_on=\"iso3_o\",\n",
    "    right_on=\"iso3_code\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# keep useful country data and join to \n",
    "key_country_info = [\n",
    "    \"year\",\n",
    "    \"iso3_code\",\n",
    "    \"iso2_code\",\n",
    "    \"pop_o\",\n",
    "    \"gdp_o\",\n",
    "    \"gdpcap_o\",\n",
    "    \"gdp_ppp_o\",\n",
    "    \"gdpcap_ppp_o\"\n",
    "]\n",
    "country_info = trade_df[key_country_info].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine country info the ECI collector dataframe\n",
    "df = pd.merge(\n",
    "    df,\n",
    "    country_info,\n",
    "    on=[\"iso2_code\", \"year\"],\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add GINI -- https://data.worldbank.org/indicator/SI.POV.GINI\n",
    "gini_df = pd.read_excel(\"../data/gini_worldbank_data.xls\")\n",
    "\n",
    "# too many NAs -- take the average across 10+ years\n",
    "years_list = [str(year) for year in range(2010, 2020)]\n",
    "gini_df[\"gini_mean\"] = gini_df[years_list].mean(axis=1)\n",
    "gini_df = gini_df[[\"Country Code\", \"gini_mean\"]]\n",
    "gini_df.columns = [\"iso3_code\", \"gini_mean\"]\n",
    "gini_df.dropna(subset=\"gini_mean\", inplace=True)\n",
    "\n",
    "# join iso2_codes -- create above\n",
    "gini_df = pd.merge(\n",
    "    gini_df,\n",
    "    codes,\n",
    "    on=\"iso3_code\",\n",
    "    how=\"left\"\n",
    ")\n",
    "gini_df.dropna(subset=\"iso2_code\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine GINI with ECI collector dataframe\n",
    "df = pd.merge(\n",
    "    df,\n",
    "    gini_df,\n",
    "    on=[\"iso2_code\", \"iso3_code\"],\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add emissions -- new data from Viktor -- ask about preparation details\n",
    "emdf = pd.read_csv(\"../data/regressions_emissions_data.csv\")\n",
    "emdf = emdf[[\"country\", \"emissions\", \"nat_res\"]].drop_duplicates()\n",
    "\n",
    "df = pd.merge(\n",
    "    df,\n",
    "    emdf,\n",
    "    left_on=\"iso3_code\",\n",
    "    right_on=\"country\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "df.to_csv(\"../outputs/eci_comparisons_2020.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2b - matrices based on trade/research/publications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mat_reshape(path, column_labels):\n",
    "    \"\"\"to reshape the matrices from Viktor Stojkoski\"\"\"\n",
    "    mat = pd.read_csv(path)\n",
    "    mat.set_index(\"Row\", inplace=True)\n",
    "    mat = mat.unstack().reset_index()\n",
    "    mat.columns = column_labels\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ANT not found in ISO3\n",
      "YUG not found in ISO3\n",
      "CSE not found in ISO3\n",
      "DDE not found in ISO3\n",
      "EPO not found in ISO3\n",
      "XKO not found in ISO3\n",
      "SFE not found in ISO3\n",
      "SUE not found in ISO3\n",
      "XTP not found in ISO3\n",
      "XUB not found in ISO3\n",
      "FST not found in ISO3\n",
      "PIT not found in ISO3\n"
     ]
    }
   ],
   "source": [
    "# read and reshape matrices\n",
    "trade_df = mat_reshape(path=\"../data/stojkoski_etal_data/trade_matrix_data_2020.csv\", column_labels=[\"product\", \"iso3_code\", \"value\"])\n",
    "patent_df = mat_reshape(path=\"../data/stojkoski_etal_data/pct_data_2020.csv\", column_labels=[\"class\", \"iso3_code\", \"value\"])\n",
    "research_df = mat_reshape(path=\"../data/stojkoski_etal_data/pub_matrix_data_2020.csv\", column_labels=[\"category\", \"iso3_code\", \"value\"])\n",
    "\n",
    "# country code correction\n",
    "iso3_codes = pd.concat([trade_df[\"iso3_code\"], patent_df[\"iso3_code\"], research_df[\"iso3_code\"]]).unique().tolist()\n",
    "iso2_codes = coco.convert(names=iso3_codes, to=\"ISO2\")\n",
    "codes2 = pd.DataFrame(iso3_codes, iso2_codes).reset_index()\n",
    "codes2.columns = [\"iso2_code\", \"iso3_code\"]\n",
    "\n",
    "trade_df = pd.merge(\n",
    "    trade_df,\n",
    "    codes2,\n",
    "    on=\"iso3_code\",\n",
    "    how=\"left\"\n",
    ")\n",
    "trade_df = trade_df[trade_df[\"iso2_code\"] != \"not found\"]\n",
    "\n",
    "patent_df = pd.merge(\n",
    "    patent_df,\n",
    "    codes2,\n",
    "    on=\"iso3_code\",\n",
    "    how=\"left\"\n",
    ")\n",
    "patent_df = patent_df[patent_df[\"iso2_code\"] != \"not found\"]\n",
    "\n",
    "research_df = pd.merge(\n",
    "    research_df,\n",
    "    codes2,\n",
    "    on=\"iso3_code\",\n",
    "    how=\"left\"\n",
    ")\n",
    "research_df = research_df[research_df[\"iso2_code\"] != \"not found\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_20344/2382482397.py:2: DtypeWarning: Columns (40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  grav_df = pd.read_csv(\"../data/Gravity_V202211.csv\")\n"
     ]
    }
   ],
   "source": [
    "# country level info from the gravity dataset\n",
    "grav_df = pd.read_csv(\"../data/Gravity_V202211.csv\")\n",
    "grav_df = grav_df[grav_df[\"year\"] == 2020]\n",
    "grav_df['country_id_o'] = grav_df['country_id_o'].map(lambda x: x.replace('.2',''))\n",
    "grav_df['country_id_d'] = grav_df['country_id_d'].map(lambda x: x.replace('.2',''))\n",
    "\n",
    "# population above 1 million\n",
    "countries_1m_pop = list(set(grav_df[grav_df[\"pop_o\"]>1000][\"iso3_o\"].to_list()))\n",
    "trade_df = trade_df[trade_df[\"iso3_code\"].isin(countries_1m_pop)]\n",
    "patent_df = patent_df[patent_df[\"iso3_code\"].isin(countries_1m_pop)]\n",
    "research_df = research_df[research_df[\"iso3_code\"].isin(countries_1m_pop)]\n",
    "\n",
    "# total export value of 1 billion USD\n",
    "above_1b_export = trade_df.groupby([\"iso2_code\"])[\"value\"].agg(\"sum\").reset_index()\n",
    "above_1b_export = list(set(above_1b_export[above_1b_export[\"value\"]>10**9][\"iso2_code\"].to_list()))\n",
    "trade_df = trade_df[trade_df[\"iso2_code\"].isin(above_1b_export)]\n",
    "\n",
    "# MIN 4 patent\n",
    "min4_patents = patent_df.groupby([\"iso2_code\"])[\"value\"].agg(\"sum\").reset_index()\n",
    "min4_patents = list(set(min4_patents[min4_patents[\"value\"] > 4][\"iso2_code\"].to_list()))\n",
    "patent_df = patent_df[patent_df[\"iso2_code\"].isin(min4_patents)]\n",
    "\n",
    "# countries w/ MIN 100 publications in a year - category w/ more than 30 published papers a year\n",
    "min100_publications = research_df.groupby([\"iso2_code\"])[\"value\"].agg(\"sum\").reset_index()\n",
    "min100_publications = list(set(min100_publications[min100_publications[\"value\"] >= 100][\"iso2_code\"].to_list()))\n",
    "min30_papers = research_df.groupby([\"category\"])[\"value\"].agg(\"sum\").reset_index()\n",
    "min30_papers = list(set(min30_papers[min30_papers[\"value\"] >= 30][\"category\"].to_list()))\n",
    "research_df = research_df[(research_df[\"category\"].isin(min30_papers))]\n",
    "\n",
    "# replace below 3 papers per country/category to 0\n",
    "research_df[\"value\"] = np.where(research_df[\"value\"]<3, 0, research_df[\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace below avg 100 citations per country/category to 0\n",
    "years = [2017, 2018, 2019, 2020]\n",
    "citations = []\n",
    "for y in years:\n",
    "    temp = mat_reshape(path=f\"../data/stojkoski_etal_data/cit_matrix_data_{y}.csv\", column_labels=[\"category\", \"iso3_code\", \"citations\"])\n",
    "    temp[\"year\"] = y\n",
    "    citations.append(temp)\n",
    "\n",
    "citations = pd.concat(citations)\n",
    "citations = citations.groupby([\"category\", \"iso3_code\"])[\"citations\"].agg(\"mean\").reset_index()\n",
    "\n",
    "research_df = pd.merge(\n",
    "    research_df,\n",
    "    citations,\n",
    "    on=[\"iso3_code\", \"category\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "research_df[\"value\"] = np.where(research_df[\"citations\"]<100, 0, research_df[\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Percentage of pairs compared that meet log-supermodularity condition: 31.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 1: Log-supermodularity condition is not fully satisfied (31.28% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Percentage of pairs compared that meet log-supermodularity condition: 3.43%\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 1: Log-supermodularity condition is not fully satisfied (3.43% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of pairs compared that meet log-supermodularity condition: 14.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/ecomplexity/ecomplexity.py:252: UserWarning: Year 1: Log-supermodularity condition is not fully satisfied (14.35% of pairs compared satisfy this condition). The ECI and PCI values may not be a true representation of the complexity. More details at: https://growthlab.hks.harvard.edu/publications/structural-ranking-economic-complexity\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# calculate complexity and mcp\n",
    "key_cols_trade = {\n",
    "    \"time\": \"period\",\n",
    "    \"loc\": \"iso2_code\",\n",
    "    \"prod\": \"product\",\n",
    "    \"val\": \"value\",\n",
    "}\n",
    "trade_df[\"period\"] = 1\n",
    "trade_cdf = ecomplexity(trade_df, key_cols_trade)\n",
    "\n",
    "key_cols_patent = {\n",
    "    \"time\": \"period\",\n",
    "    \"loc\": \"iso2_code\",\n",
    "    \"prod\": \"class\",\n",
    "    \"val\": \"value\",\n",
    "}\n",
    "patent_df[\"period\"] = 1\n",
    "patent_cdf = ecomplexity(patent_df, key_cols_patent)\n",
    "\n",
    "key_cols_research = {\n",
    "    \"time\": \"period\",\n",
    "    \"loc\": \"iso2_code\",\n",
    "    \"prod\": \"category\",\n",
    "    \"val\": \"value\",\n",
    "}\n",
    "research_df[\"period\"] = 1\n",
    "research_cdf = ecomplexity(research_df, key_cols_research)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for figures\n",
    "trade_cdf.to_csv(\"../outputs/trade_cdf_2020.csv\", sep=\";\", index=False)\n",
    "patent_cdf.to_csv(\"../outputs/patent_cdf_2020.csv\", sep=\";\", index=False)\n",
    "research_cdf.to_csv(\"../outputs/research_cdf_2020.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 3 - for entry regressions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relatedness density -- as in Hidalgo et al. (2007) Science\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\")\n",
    "rel_dens = cdf[cdf[\"year\"] == 2020][[\"iso2_code\", \"language\", \"density\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88775, 7)\n"
     ]
    }
   ],
   "source": [
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "selected_period = \"year\"\n",
    "\n",
    "# steps to prep dataframe of ecomplexity\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# threshold for RCA : 1\n",
    "ps = [1, 2, 3, 4]\n",
    "rca_tables = []\n",
    "for p in ps:\n",
    "    print(p)\n",
    "    temp = bundle_data(df, periods=[p])\n",
    "    temp[\"period\"] = p\n",
    "    rca_tables.append(rca_calculation(temp, c_column=\"iso2_code\", p_column=\"language\", value_column=\"num_pushers\", threshold=1))\n",
    "\n",
    "#    dfbs.append(temp)\n",
    "rca_tables = pd.concat(rca_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify entry following the given patterns\n",
    "entry_pattern = [0,0,1,1]\n",
    "consider_pattern = [0,0,0,0]\n",
    "ent = rca_tables.sort_values([\"period\"], ascending=True).groupby([\"iso2_code\",\"language\"])[\"rca01\"].agg(list).reset_index()\n",
    "ent[\"entry01\"] = ent[\"rca01\"].apply(lambda x: x == entry_pattern).astype(int)\n",
    "ent[\"consider00\"] = ent[\"rca01\"].apply(lambda x: x == consider_pattern).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full combination\n",
    "all_countries = ent[\"iso2_code\"].unique()\n",
    "all_languages = ent[\"language\"].unique()\n",
    "\n",
    "all_combinations = list(product(all_countries, all_languages))\n",
    "full_df = pd.DataFrame(all_combinations, columns=[\"iso2_code\", \"language\"])\\\n",
    "    .sort_values([\"iso2_code\", \"language\"])\n",
    "\n",
    "# join entries\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    ent[[\"iso2_code\", \"language\", \"entry01\", \"consider00\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# join complexity\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    cdf[[\"iso2_code\", \"language\", \"pci\", \"ubiquity\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# join RCA from the baseline period\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rca_tables[rca_tables[\"period\"]==3].loc[:,[\"iso2_code\", \"language\", \"rca01\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_df[\"rca01\"] = full_df[\"rca01\"].fillna(0)\n",
    "\n",
    "# drop languages with no complexity value\n",
    "full_df.dropna(subset=[\"pci\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join to full_df with entries and PCI\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rel_dens,\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for entry models -- only consider 00, 01 patterns\n",
    "full_df[\"entry01\"] = full_df[\"entry01\"].astype(int)\n",
    "full_df[\"consider00\"] = full_df[\"consider00\"].astype(int)\n",
    "export_df = full_df[(full_df[\"entry01\"]==1) | (full_df[\"consider00\"]==1)]\n",
    "export_df.to_csv(\"../outputs/data_entry_regressions_0011.csv\", index=False, sep=\";\")\n",
    "#export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold05.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 4 - for exit regressions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relatedness density -- as in Hidalgo et al. (2007) Science\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\")\n",
    "rel_dens = cdf[cdf[\"year\"] == 2020][[\"iso2_code\", \"language\", \"density\"]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88775, 7)\n"
     ]
    }
   ],
   "source": [
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")\n",
    "selected_period = \"year\"\n",
    "\n",
    "# steps to prep dataframe of ecomplexity\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# threshold for RCA : 1\n",
    "ps = [1, 2, 3, 4]\n",
    "rca_tables = []\n",
    "for p in ps:\n",
    "    print(p)\n",
    "    temp = bundle_data(df, periods=[p])\n",
    "    temp[\"period\"] = p\n",
    "    rca_tables.append(rca_calculation(temp, c_column=\"iso2_code\", p_column=\"language\", value_column=\"num_pushers\", threshold=1))\n",
    "\n",
    "#    dfbs.append(temp)\n",
    "rca_tables = pd.concat(rca_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify entry following the given patterns\n",
    "exit_pattern = [1,1,0,0]\n",
    "consider_pattern = [1,1,1,1]\n",
    "ext = rca_tables.sort_values([\"period\"], ascending=True).groupby([\"iso2_code\",\"language\"])[\"rca01\"].agg(list).reset_index()\n",
    "ext[\"entry01\"] = ext[\"rca01\"].apply(lambda x: x == exit_pattern).astype(int)\n",
    "ext[\"consider00\"] = ext[\"rca01\"].apply(lambda x: x == consider_pattern).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full combination\n",
    "all_countries = ext[\"iso2_code\"].unique()\n",
    "all_languages = ext[\"language\"].unique()\n",
    "\n",
    "all_combinations = list(product(all_countries, all_languages))\n",
    "full_df = pd.DataFrame(all_combinations, columns=[\"iso2_code\", \"language\"])\\\n",
    "    .sort_values([\"iso2_code\", \"language\"])\n",
    "\n",
    "# join entries\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    ext[[\"iso2_code\", \"language\", \"entry01\", \"consider00\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# join complexity\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\")\n",
    "cdf = cdf[cdf[\"year\"]==2020]\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    cdf[[\"iso2_code\", \"language\", \"pci\", \"ubiquity\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# join RCA from the baseline period\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rca_tables[rca_tables[\"period\"]==3].loc[:,[\"iso2_code\", \"language\", \"rca01\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_df[\"rca01\"] = full_df[\"rca01\"].fillna(0)\n",
    "\n",
    "# drop languages with no complexity value\n",
    "full_df.dropna(subset=[\"pci\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join to full_df with entries and PCI\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rel_dens,\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for entry models -- only consider 00, 01 patterns\n",
    "full_df[\"entry01\"] = full_df[\"entry01\"].astype(int)\n",
    "full_df.rename(columns={\"entry01\":\"exit01\"}, inplace=True)\n",
    "full_df[\"consider00\"] = full_df[\"consider00\"].astype(int)\n",
    "export_df = full_df[(full_df[\"exit01\"]==1) | (full_df[\"consider00\"]==1)]\n",
    "export_df.to_csv(\"../outputs/data_exit_regressions_1100.csv\", index=False, sep=\";\")\n",
    "#export_df.to_csv(\"../outputs/data_entry_regressions_0011_threshold05.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IV -- 3 most similar non-neighboring countries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECI_software table\n",
    "cdf = pd.read_csv(\"../outputs/eci_software_2020_2023.csv\", sep=\";\")\n",
    "\n",
    "# neighboring countries from https://github.com/geodatasource/country-borders\n",
    "nc = pd.read_csv(\"../data/geodatasource_country_borders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = list(set(cdf[\"iso2_code\"].to_list()))\n",
    "full_prod_countries = pd.DataFrame(itertools.product(locations, repeat=2), columns=[\"iso2_code1\", \"iso2_code2\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_prod_countries = pd.merge(\n",
    "    full_prod_countries,\n",
    "    nc,\n",
    "    left_on=[\"iso2_code1\", \"iso2_code2\"],\n",
    "    right_on=[\"country_code\", \"country_border_code\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_prod_countries[\"neighbor01\"] = full_prod_countries[\"country_border_code\"].notna().astype(int)\n",
    "full_prod_countries = full_prod_countries[[\"iso2_code1\", \"iso2_code2\", \"neighbor01\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_26217/3383011093.py:83: FutureWarning: The provided callable <function mean at 0x105feb1a0> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  .agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_26217/3383011093.py:83: FutureWarning: The provided callable <function mean at 0x105feb1a0> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  .agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_26217/3383011093.py:83: FutureWarning: The provided callable <function mean at 0x105feb1a0> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  .agg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_26217/3383011093.py:83: FutureWarning: The provided callable <function mean at 0x105feb1a0> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  .agg(\n"
     ]
    }
   ],
   "source": [
    "# select year\n",
    "year_list = [2020, 2021, 2022, 2023]\n",
    "cdf2 = []\n",
    "for y in year_list:\n",
    "    print(y)\n",
    "\n",
    "    tcdf = cdf[cdf[\"year\"] == y]\n",
    "    tcdf.year.isna().sum()\n",
    "\n",
    "    # generate full product dataframe\n",
    "    locations = list(set(tcdf[\"iso2_code\"].to_list()))\n",
    "    full_prod_countries = pd.DataFrame(itertools.product(locations, repeat=2), columns=[\"iso2_code1\", \"iso2_code2\"])\n",
    "    full_prod_countries = pd.merge(\n",
    "        full_prod_countries,\n",
    "        nc,\n",
    "        left_on=[\"iso2_code1\", \"iso2_code2\"],\n",
    "        right_on=[\"country_code\", \"country_border_code\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    full_prod_countries[\"neighbor01\"] = full_prod_countries[\"country_border_code\"].notna().astype(int)\n",
    "    full_prod_countries = full_prod_countries[[\"iso2_code1\", \"iso2_code2\", \"neighbor01\"]]\n",
    "\n",
    "    # add location - mcp array to location pairs\n",
    "    mcp_temp = tcdf.groupby(\"iso2_code\")[\"mcp\"].apply(np.array).reset_index()\n",
    "    full_prod_countries = pd.merge(\n",
    "        full_prod_countries,\n",
    "        mcp_temp,\n",
    "        left_on=\"iso2_code1\",\n",
    "        right_on=\"iso2_code\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    full_prod_countries = pd.merge(\n",
    "        full_prod_countries,\n",
    "        mcp_temp,\n",
    "        left_on=\"iso2_code2\",\n",
    "        right_on=\"iso2_code\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    full_prod_countries = full_prod_countries\\\n",
    "        .drop(columns=[\"iso2_code_x\", \"iso2_code_y\"])\\\n",
    "        .rename(columns={\"mcp_x\":\"mcp_array1\", \"mcp_y\":\"mcp_array2\"})\n",
    "\n",
    "    # minimum conditional probability -- to measure similarity between locations\n",
    "    full_prod_countries[\"spec_similarity\"] = full_prod_countries.apply(lambda r: round(sum(r[\"mcp_array1\"] * r[\"mcp_array2\"]) / max(sum(r[\"mcp_array1\"]), sum(r[\"mcp_array2\"])), 3), axis=1)\n",
    "\n",
    "    # drop iso2_code1 == iso2_code2 cases and neighbors\n",
    "    sim_spec_df = full_prod_countries[(full_prod_countries[\"iso2_code1\"] != full_prod_countries[\"iso2_code2\"]) & (full_prod_countries[\"neighbor01\"] == 0)]\n",
    "    \n",
    "    # keep the top3 most similar countries\n",
    "    sim_spec_df = sim_spec_df.groupby([\"iso2_code1\"])[\"spec_similarity\"]\\\n",
    "        .nlargest(3)\\\n",
    "        .reset_index()\\\n",
    "        .rename(columns={\"level_1\":\"iso2_code2_index\"})\n",
    "\n",
    "    # merge similar location names by index\n",
    "    sim_spec_df = pd.merge(\n",
    "        sim_spec_df,\n",
    "        full_prod_countries[[\"iso2_code2\"]].reset_index(),\n",
    "        left_on=\"iso2_code2_index\",\n",
    "        right_on=\"index\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # merge ECI values by location name\n",
    "    sim_spec_df = pd.merge(\n",
    "        sim_spec_df,\n",
    "        tcdf[[\"iso2_code\", \"eci\"]].drop_duplicates(),\n",
    "        left_on=\"iso2_code2\",\n",
    "        right_on=\"iso2_code\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # merge distance values by location name\n",
    "    sim_spec_df = pd.merge(\n",
    "        sim_spec_df,\n",
    "        full_prod_countries,\n",
    "        on=[\"iso2_code1\", \"iso2_code2\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # average ECI of the top 3 most similar location \n",
    "    avg_comp_sim_spec = sim_spec_df.groupby([\"iso2_code1\"])\\\n",
    "        .agg(\n",
    "            avg_eci_similar_spec = pd.NamedAgg(\"eci\", np.mean))\\\n",
    "        .reset_index()\\\n",
    "        .rename(columns={\"iso2_code1\" : \"iso2_code\"})\n",
    "\n",
    "    # join to full comb table\n",
    "    tcdf = pd.merge(\n",
    "        tcdf,\n",
    "        avg_comp_sim_spec,\n",
    "        on=\"iso2_code\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    cdf2.append(tcdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join and save\n",
    "cdf2 = pd.concat(cdf2)\n",
    "cdf2.to_csv(f\"../outputs/eci_software_2020_2023_iv_v2.csv\", index=False, sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
