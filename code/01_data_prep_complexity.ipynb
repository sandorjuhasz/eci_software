{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from itertools import product\n",
    "from ecomplexity import ecomplexity\n",
    "from ecomplexity import proximity\n",
    "import country_converter as coco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 0 - general data preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter to choose year / semester / quarter to construct period IDs\n",
    "selected_period = \"year\"\n",
    "\n",
    "# for ecomplexity calculcation\n",
    "key_cols = {\n",
    "    \"time\": \"period\",\n",
    "    \"loc\": \"iso2_code\",\n",
    "    \"prod\": \"language\",\n",
    "    \"val\": \"num_pushers\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data IN\n",
    "data = pd.read_csv(\"../data/languages.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter functions\n",
    "def drop_specifics_from_list(data, filter_list):\n",
    "    \"\"\"filter specific languages from list -- motivated by RM del Rio-Chanona et al 2023\"\"\"\n",
    "    data = data[~data[\"language\"].str.contains(filter_list, case=False, regex=True)]\n",
    "    return data\n",
    "\n",
    "def top_languages_filter(data, nr_languages):\n",
    "    \"\"\"keep top x number of languages ONLY\"\"\"\n",
    "    top_languages = data.groupby([\"language\"])[\"num_pushers\"].agg(\"sum\").reset_index().sort_values(by=\"num_pushers\", ascending=False)\n",
    "    top_languages = list(top_languages[\"language\"])[:nr_languages]\n",
    "    data = data[data[\"language\"].isin(top_languages)]\n",
    "    return data\n",
    "    \n",
    "def drop_country_codes_from_list(data, country_list):\n",
    "    data = data[~data[\"iso2_code\"].isin(country_list)]\n",
    "    data = data.dropna(subset=\"iso2_code\")\n",
    "    return data\n",
    "\n",
    "def add_period_ids(data, period):\n",
    "    \"\"\"create missing semester ID and construct different period IDs\"\"\"\n",
    "    if period==\"year\":\n",
    "        year_to_period = dict(zip(data[\"year\"].unique(), list(range(1, len(data[\"year\"].unique()) + 1))))\n",
    "        data[\"period\"] = data[\"year\"].map(year_to_period)\n",
    "    if period==\"semester\":\n",
    "        data[\"semester\"] = np.where(data[\"quarter\"] <= 2, 1, 2)\n",
    "        data[\"semester_id\"] = data[\"year\"].astype(str).str.cat(data[\"semester\"].astype(str), sep=\"s\")\n",
    "        semester_to_period = dict(zip(data[\"semester_id\"].unique(), list(range(1, len(data[\"semester_id\"].unique()) + 1))))\n",
    "        data[\"period\"] = data[\"semester_id\"].map(semester_to_period)\n",
    "    if period==\"quarter\":\n",
    "        data[\"quarter_id\"] = data[\"year\"].astype(str).str.cat(data[\"quarter\"].astype(str), sep=\"q\")\n",
    "        quarter_to_period = dict(zip(data[\"quarter_id\"].unique(), list(range(1, len(data[\"quarter_id\"].unique()) + 1))))\n",
    "        data[\"period\"] = data[\"quarter_id\"].map(quarter_to_period)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84934, 7)\n"
     ]
    }
   ],
   "source": [
    "# steps to prep dataframe of ecomplexity\n",
    "prev_filter = \"|\".join([\"yaml\", \"json\", \"text\", \"svg\", \"Markdown\", \"xml\"])\n",
    "df = drop_specifics_from_list(data, filter_list=prev_filter)\n",
    "df = top_languages_filter(df, nr_languages=150)\n",
    "df = drop_country_codes_from_list(df, country_list=[\"EU\"])\n",
    "df = add_period_ids(df, period=selected_period)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1 - from M_cl to complexity and relatedness - based on 2020,2021 data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bundle data for M_cl\n",
    "def bundle_data(data, periods):\n",
    "    data = data[data[\"period\"].isin(periods)]\\\n",
    "        .groupby([\"iso2_code\", \"language\"])[\"num_pushers\"]\\\n",
    "        .agg(\"sum\")\\\n",
    "        .reset_index()\n",
    "    data[\"period\"] = 1\n",
    "    return data\n",
    "\n",
    "# period IDs -- 1,2 means 2020, 2021 on yearly basis\n",
    "dfb = bundle_data(df, periods=[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# software complexity calculation\n",
    "cdf = ecomplexity(dfb, key_cols)\n",
    "cdf.to_csv(\"../outputs/software_complexity_2020_2021_based.csv\", index=False, sep=\";\")\n",
    "\n",
    "# software relatedness calculation\n",
    "rel_df = proximity(dfb, key_cols)\n",
    "rel_df.to_csv(\"../outputs/software_relatedness_2020_2021_based.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2 - comparing ECI(software, trade, technology, research)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in developers (for filtering option)\n",
    "df = pd.read_csv(\"../data/developers.csv\")\n",
    "df = df.groupby([\"iso2_code\", \"year\"])[\"developers\"].agg(\"mean\").reset_index()\n",
    "df[\"developers\"] = df[\"developers\"].astype(int)\n",
    "df = df[df[\"year\"]==2020]\n",
    "\n",
    "# add software ECI\n",
    "cdf = pd.read_csv(\"../outputs/software_complexity_2020_2021_based.csv\", sep=\";\")\n",
    "country_to_seci_2020 = dict(cdf.groupby([\"iso2_code\"])[\"eci\"].mean())\n",
    "df[\"software_eci_2020\"] = df[\"iso2_code\"].map(country_to_seci_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in 3 other ECI measures\n",
    "trade_eci = pd.read_csv(\"../data/eci_hs6_hs96_trade.csv\")\n",
    "trade_eci = trade_eci[[\"Country\", \"2020\"]]\n",
    "tech_eci = pd.read_csv(\"../data/eci_tech.csv\")\n",
    "tech_eci = tech_eci[[\"Country\", \"2020\"]]\n",
    "research_eci = pd.read_csv(\"../data/Data-ECI-Research.csv\")\n",
    "research_eci = research_eci[[\"Country\", \"2020\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map - using the country_converter package to \n",
    "c_to_iso = dict(\n",
    "    zip(trade_eci.Country.unique(), coco.convert(names=trade_eci.Country.unique(), to=\"ISO2\")))\n",
    "trade_eci[\"Country\"] = trade_eci[\"Country\"].map(c_to_iso)\n",
    "iso_to_trade_eci = dict(trade_eci.values)\n",
    "df[\"trade_eci_2020\"] = df[\"iso2_code\"].map(iso_to_trade_eci)\n",
    "\n",
    "tech_eci[\"Country\"] = tech_eci[\"Country\"].map(c_to_iso)\n",
    "iso_to_tech_eci = dict(tech_eci.values)\n",
    "df[\"tech_eci_2020\"] = df[\"iso2_code\"].map(iso_to_tech_eci)\n",
    "\n",
    "research_eci[\"Country\"] = research_eci[\"Country\"].map(c_to_iso)\n",
    "iso_to_research_eci = dict(research_eci.values)\n",
    "df[\"research_eci_2020\"] = df[\"iso2_code\"].map(iso_to_research_eci)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_36792/1367150806.py:2: DtypeWarning: Columns (40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  trade_df = pd.read_csv(\"../data/Gravity_V202211.csv\")\n"
     ]
    }
   ],
   "source": [
    "# data from CEPII -- http://www.cepii.fr/CEPII/en/bdd_modele/bdd_modele_item.asp?id=8\n",
    "trade_df = pd.read_csv(\"../data/Gravity_V202211.csv\")\n",
    "trade_df = trade_df[trade_df[\"year\"] == 2020]\n",
    "trade_df['country_id_o'] = trade_df['country_id_o'].map(lambda x: x.replace('.2',''))\n",
    "trade_df['country_id_d'] = trade_df['country_id_d'].map(lambda x: x.replace('.2',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ANT not found in ISO3\n",
      "CSK not found in ISO3\n",
      "DDR not found in ISO3\n",
      "SCG not found in ISO3\n",
      "SUN not found in ISO3\n",
      "VDR not found in ISO3\n",
      "YMD not found in ISO3\n",
      "YUG not found in ISO3\n"
     ]
    }
   ],
   "source": [
    "# code transformation\n",
    "iso3_codes = pd.concat([trade_df[\"iso3_o\"], trade_df[\"iso3_d\"]]).unique().tolist()\n",
    "iso2_codes = coco.convert(names=iso3_codes, to=\"ISO2\")\n",
    "codes = pd.DataFrame(iso3_codes, iso2_codes).reset_index()\n",
    "codes.columns = [\"iso2_code\", \"iso3_code\"]\n",
    "\n",
    "trade_df = pd.merge(\n",
    "    trade_df,\n",
    "    codes,\n",
    "    left_on=\"iso3_o\",\n",
    "    right_on=\"iso3_code\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# keep useful country data and join to \n",
    "key_country_info = [\n",
    "    \"year\",\n",
    "    \"iso3_code\",\n",
    "    \"iso2_code\",\n",
    "    \"pop_o\",\n",
    "    \"gdp_o\",\n",
    "    \"gdpcap_o\",\n",
    "    \"gdp_ppp_o\",\n",
    "    \"gdpcap_ppp_o\"\n",
    "]\n",
    "country_info = trade_df[key_country_info].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine country info the ECI collector dataframe\n",
    "df = pd.merge(\n",
    "    df,\n",
    "    country_info,\n",
    "    on=[\"iso2_code\", \"year\"],\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add GINI -- https://data.worldbank.org/indicator/SI.POV.GINI\n",
    "gini_df = pd.read_excel(\"../data/gini_worldbank_data.xls\")\n",
    "\n",
    "# too many NAs -- take the average across 10+ years\n",
    "years_list = [str(year) for year in range(2010, 2023)]\n",
    "gini_df[\"gini_mean\"] = gini_df[years_list].mean(axis=1)\n",
    "gini_df = gini_df[[\"Country Code\", \"gini_mean\"]]\n",
    "gini_df.columns = [\"iso3_code\", \"gini_mean\"]\n",
    "gini_df.dropna(subset=\"gini_mean\", inplace=True)\n",
    "\n",
    "# join iso2_codes -- create above\n",
    "gini_df = pd.merge(\n",
    "    gini_df,\n",
    "    codes,\n",
    "    on=\"iso3_code\",\n",
    "    how=\"left\"\n",
    ")\n",
    "gini_df.dropna(subset=\"iso2_code\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine GINI with ECI collector dataframe\n",
    "df = pd.merge(\n",
    "    df,\n",
    "    gini_df,\n",
    "    on=[\"iso2_code\", \"iso3_code\"],\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add emissions\n",
    "emdf = pd.read_csv(\"../data/data_embodied_emissions.csv\")\n",
    "emdf = emdf[emdf[\"year\"]==2019].loc[:,[\"country\", \"embodied_emissions\"]].drop_duplicates()\n",
    "\n",
    "df = pd.merge(\n",
    "    df,\n",
    "    emdf,\n",
    "    left_on=\"iso3_code\",\n",
    "    right_on=\"country\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "df.to_csv(\"../outputs/eci_comparisons_2020.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 3 - software space from relatedness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edgelist_cleaning_for_software_space(data, key_columns):\n",
    "    \"\"\"get software space network from raw proximity values\"\"\"\n",
    "    data = data[key_columns]\n",
    "\n",
    "    # drop zero -- non-existing edges\n",
    "    data = data[data[key_columns[2]] > 0]\n",
    "\n",
    "    # drop self loops\n",
    "    data = data[data[key_columns[0]] != data[key_columns[1]]]\n",
    "    return data\n",
    "\n",
    "def maximum_spanning_tree(data, key_columns):\n",
    "    \"\"\"get the maximum spanning tree of the full relatedness based network\"\"\"\n",
    "    table = data.copy()\n",
    "    table[\"distance\"] = 1.0 / table[key_columns[2]]\n",
    "    G = nx.from_pandas_edgelist(table, source = key_columns[0], target = key_columns[1], edge_attr = [\"distance\", key_columns[2]])\n",
    "    T = nx.minimum_spanning_tree(G, weight = \"distance\")\n",
    "    table2 = nx.to_pandas_edgelist(T)\n",
    "    table2 = table2[table2[key_columns[2]] > 0]\n",
    "    table2.rename(columns = {\"source\": key_columns[0], \"target\": key_columns[1], key_columns[2]: \"score\"}, inplace = True)\n",
    "    table = pd.merge(\n",
    "        table,\n",
    "        table2,\n",
    "        on=key_columns[0:2]\n",
    "    )  \n",
    "    table[\"edge\"] = table.apply(lambda x: \"%s-%s\" % (min(x[key_columns[0]], x[key_columns[1]]), max(x[key_columns[0]], x[key_columns[1]])), axis = 1)\n",
    "    table = table.drop_duplicates(subset = [\"edge\"])\n",
    "    table = table.drop(\"edge\", 1)\n",
    "    return table[key_columns]\n",
    "\n",
    "def add_edges(mst_edges, all_edges, nr_edges_to_add):\n",
    "    \"\"\"add edges to the maximum spanning tree to have a 1/3 nodes/edges ratio\"\"\"\n",
    "    # drop mst edges from the full edgelist\n",
    "    mst_edges[\"drop\"] = 1\n",
    "    all_edges = pd.merge(\n",
    "        all_edges,\n",
    "        mst_edges,\n",
    "        on = [\"language_1\", \"language_2\", \"proximity\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "    all_edges = all_edges[all_edges[\"drop\"] != 1].drop(columns=\"drop\")\n",
    "\n",
    "    # sort and select\n",
    "    all_edges = all_edges.sort_values(by=\"proximity\", ascending=False).iloc[:nr_edges_to_add]\n",
    "\n",
    "    # add to mst edgelist\n",
    "    software_space_el = pd.concat([mst_edges, all_edges])\n",
    "    software_space_el.drop(columns=[\"drop\"], inplace=True)\n",
    "    return software_space_el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_763/4209101596.py:28: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  table = table.drop(\"edge\", 1)\n"
     ]
    }
   ],
   "source": [
    "# from relatedness table to software space (MST w/ additional edges)\n",
    "space_table = edgelist_cleaning_for_software_space(rel_df, key_columns=[\"language_1\", \"language_2\", \"proximity\"])\n",
    "mst_el = maximum_spanning_tree(space_table, key_columns=[\"language_1\", \"language_2\", \"proximity\"])\n",
    "mst_graph = nx.from_pandas_edgelist(mst_el, source=\"language_1\", target=\"language_2\")\n",
    "n_nodes = mst_graph.number_of_nodes()\n",
    "n_edges = n_nodes * 2\n",
    "software_space_el = add_edges(mst_el, space_table, nr_edges_to_add=n_edges)\n",
    "\n",
    "# export final software space edgelist\n",
    "software_space_el.to_csv(\"../outputs/software_space_edgelist_2020_2021_based.csv\", index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 4 - regression data for cross-sectional entry models (2022-2023)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bundle cleaned data for M_cl -- 3 : 2022 / 4 : 2023\n",
    "dfb3 = bundle_data(df, periods=[3])\n",
    "dfb4 = bundle_data(df, periods=[4])\n",
    "dfb3[\"period\"] = 3\n",
    "dfb4[\"period\"] = 4\n",
    "dfbs = pd.concat([dfb3, dfb4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rca_calculation(table, c_column, p_column, value_column):\n",
    "    \"\"\"calculate RCA from an M_cp dataframe\"\"\"\n",
    "    table[\"e_p\"] = table.groupby(p_column)[value_column].transform(\"sum\")\n",
    "    table[\"e_c\"] = table.groupby(c_column)[value_column].transform(\"sum\")\n",
    "    table[\"e\"] = table[value_column].sum()\n",
    "\n",
    "    table[\"rca\"] = (table[value_column] / table[\"e_p\"]) / (table[\"e_c\"] / table[\"e\"])\n",
    "    table[\"rca01\"] = np.where(table[\"rca\"] >= 1, 1, 0)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_92093/172238313.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  table[\"e_p\"] = table.groupby(p_column)[value_column].transform(\"sum\")\n",
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_92093/172238313.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  table[\"e_c\"] = table.groupby(c_column)[value_column].transform(\"sum\")\n",
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_92093/172238313.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  table[\"e\"] = table[value_column].sum()\n",
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_92093/172238313.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  table[\"rca\"] = (table[value_column] / table[\"e_p\"]) / (table[\"e_c\"] / table[\"e\"])\n",
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_92093/172238313.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  table[\"rca01\"] = np.where(table[\"rca\"] >= 1, 1, 0)\n",
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_92093/172238313.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  table[\"e_p\"] = table.groupby(p_column)[value_column].transform(\"sum\")\n",
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_92093/172238313.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  table[\"e_c\"] = table.groupby(c_column)[value_column].transform(\"sum\")\n",
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_92093/172238313.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  table[\"e\"] = table[value_column].sum()\n",
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_92093/172238313.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  table[\"rca\"] = (table[value_column] / table[\"e_p\"]) / (table[\"e_c\"] / table[\"e\"])\n",
      "/var/folders/9d/8j37_fks51x11mk0_zwqsd940000gn/T/ipykernel_92093/172238313.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  table[\"rca01\"] = np.where(table[\"rca\"] >= 1, 1, 0)\n"
     ]
    }
   ],
   "source": [
    "# calculate RCA for each period\n",
    "rca_tables = list()\n",
    "for p in dfbs[\"period\"].unique():\n",
    "    rca_df = dfbs[dfbs[\"period\"]==p]\n",
    "    rca_tables.append(rca_calculation(rca_df, c_column=\"iso2_code\", p_column=\"language\", value_column=\"num_pushers\"))\n",
    "rca_tables = pd.concat(rca_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify entry following the given patterns\n",
    "entry_pattern = [0,1]\n",
    "consider_pattern = [0,0]\n",
    "ent = rca_tables.sort_values([\"period\"], ascending=True).groupby([\"iso2_code\",\"language\"])[\"rca01\"].agg(list).reset_index()\n",
    "ent[\"entry01\"] = ent[\"rca01\"].apply(lambda x: x == entry_pattern).astype(int)\n",
    "ent[\"consider00\"] = ent[\"rca01\"].apply(lambda x: x == consider_pattern).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full combination\n",
    "all_countries = ent[\"iso2_code\"].unique()\n",
    "all_languages = ent[\"language\"].unique()\n",
    "\n",
    "all_combinations = list(product(all_countries, all_languages))\n",
    "full_df = pd.DataFrame(all_combinations, columns=[\"iso2_code\", \"language\"])\\\n",
    "    .sort_values([\"iso2_code\", \"language\"])\n",
    "\n",
    "# join entries\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    ent[[\"iso2_code\", \"language\", \"entry01\", \"consider00\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ").fillna(0)\n",
    "\n",
    "# join complexity\n",
    "# cdf = pd.read_csv(\"../outputs/complexity_table2020.csv\", sep=\";\")\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    cdf[[\"iso2_code\", \"language\", \"pci\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# join RCA from the baseline period\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    rca_tables[rca_tables[\"period\"]==3].loc[:,[\"iso2_code\", \"language\", \"rca01\"]],\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "full_df[\"rca01\"] = full_df[\"rca01\"].fillna(0)\n",
    "\n",
    "# drop languages with no complexity value\n",
    "full_df.dropna(subset=[\"pci\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate relatedness ddensity \n",
    "# software_space_el = pd.read_csv(\"../outputs/software_space_edgelist_2020_2021_based.csv\", sep=\";\")\n",
    "software_space_el[\"proximity\"] = 1\n",
    "\n",
    "# symmetric relatedness matrix\n",
    "relatedness = pd.pivot_table(\n",
    "    software_space_el,\n",
    "    values=\"proximity\",\n",
    "    index=[\"language_1\"],\n",
    "    columns=[\"language_2\"],\n",
    "    aggfunc=np.sum,\n",
    "    margins=False\n",
    ")\n",
    "relatedness = relatedness.combine_first(relatedness.T).fillna(0).astype(int)\n",
    "\n",
    "# matrix from RCA values in the baseline period\n",
    "rca_tables = rca_tables[rca_tables[\"language\"].isin(relatedness.columns)]\n",
    "mat = pd.pivot_table(\n",
    "    rca_tables[rca_tables[\"period\"]==3].loc[:,[\"iso2_code\", \"language\", \"rca01\"]],\n",
    "    values=\"rca01\",\n",
    "    index=[\"iso2_code\"],\n",
    "    columns=[\"language\"],\n",
    "    aggfunc=np.sum,\n",
    "    margins=False\n",
    ").fillna(0).astype(int)\n",
    "\n",
    "# relatedness density\n",
    "rel = np.dot(mat, relatedness)\n",
    "reltot = np.sum(relatedness, axis=0)\n",
    "reltot = reltot.values.flatten()\n",
    "reldens = rel / reltot\n",
    "reldens_df = pd.DataFrame(reldens)\n",
    "reldens_df.index = mat.index\n",
    "reldens_df\n",
    "reldens_df.columns = mat.columns\n",
    "reldens_df = reldens_df.rename_axis(\"iso2_code\")\\\n",
    "  .reset_index()\\\n",
    "  .melt(\"iso2_code\", value_name=\"rel_density\", var_name=\"language\")\\\n",
    "  .reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join to full_df with entries and PCI\n",
    "full_df = pd.merge(\n",
    "    full_df,\n",
    "    reldens_df,\n",
    "    on=[\"iso2_code\", \"language\"],\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for entry models -- only consider 00, 01 patterns\n",
    "full_df[\"entry01\"] = full_df[\"entry01\"].astype(int)\n",
    "full_df[\"consider00\"] = full_df[\"consider00\"].astype(int)\n",
    "export_df = full_df[(full_df[\"entry01\"]==1) | (full_df[\"consider00\"]==1)]\n",
    "export_df.to_csv(\"../outputs/data_entry_regressions_2020_2021_based.csv\", index=False, sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
